{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "d6n47ueeqbXb"
   },
   "source": [
    "## Personalized Learning (Localized Learning?)\n",
    "\n",
    "#### This notebook includes the following online models;\n",
    "1. A single global model with all data\n",
    "2. Multiple local models (starting from a single global model)\n",
    "   1. that are updated with new data\n",
    "   2. that exchanges data in clusters\n",
    "   3. that exchanges parameters in clusters\n",
    "\n",
    "  \n",
    "#### The dataset that is used for this project is [CIFAR-100 dataset][1]\n",
    "* Has 100 classes containing 600 images each\n",
    "\n",
    "#### New data are fed by the following rules;\n",
    "1. Distributed, according to superclasses\n",
    "  * Clusters will only be updated with data that belongs to a specific superclass\n",
    "  * We update the NN by\n",
    "    1. Changing all parameters of the NN\n",
    "    2. Only changing the last few layers, as in many MTL models\n",
    "2. Randomly (why?)\n",
    "\n",
    "#### We expect to find an answer to the following research questions with this project;\n",
    "1. If models are updated with data (or parameters) that are shared within a cluster, can the model perform good enough with the labels that count?\n",
    "  * For example, the performance of the cluster that are updated with \"Vehicles\" superclass is only assessed with the labels that corresponds to the superclass.\n",
    "  \n",
    "[1]: https://www.cs.toronto.edu/~kriz/cifar.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Oji0BTfoqbXc"
   },
   "source": [
    "#### Questions\n",
    "\n",
    "Retraining: how does it work <br>\n",
    "How do we compare these models?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "mr4-uY0LqbXd"
   },
   "source": [
    "### Implementation with Custom Neural Network and EMNIST dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "tGoXLnOyqbXe",
    "outputId": "9ccd7215-80bf-4a0a-b852-8896b17c38f1",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "import tensorflow.keras as keras\n",
    "from tensorflow.keras.datasets import mnist\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout, Flatten\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D\n",
    "from tensorflow.keras import backend as K"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import matplotlib.lines as mlines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "log_dir = \"logs/fit/\" + datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=log_dir, histogram_freq=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1.15.0'"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "E2faBs1yqbXj"
   },
   "outputs": [],
   "source": [
    "# Hyperparameters\n",
    "batch_size = 50\n",
    "epochs = 20\n",
    "\n",
    "# input image dimensions\n",
    "img_rows, img_cols = 28, 28"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "QXfylSWLqbXl"
   },
   "source": [
    "#### Load MNIST dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the data, split between train and test sets\n",
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "if K.image_data_format() == 'channels_first':\n",
    "    x_train = x_train.reshape(x_train.shape[0], 1, img_rows, img_cols)\n",
    "    x_test = x_test.reshape(x_test.shape[0], 1, img_rows, img_cols)\n",
    "    input_shape = (1, img_rows, img_cols)\n",
    "else:\n",
    "    x_train = x_train.reshape(x_train.shape[0], img_rows, img_cols, 1)\n",
    "    x_test = x_test.reshape(x_test.shape[0], img_rows, img_cols, 1)\n",
    "    input_shape = (img_rows, img_cols, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(60000, 28, 28, 1)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = x_train.astype('float32')\n",
    "x_test = x_test.astype('float32')\n",
    "x_train /= 255\n",
    "x_test /= 255"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "global_dataset_size = 30000\n",
    "local_dataset_size = 30000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_global = x_train[-global_dataset_size:]\n",
    "Y_global = y_train[-global_dataset_size:]\n",
    "X_local = x_train[:-global_dataset_size]\n",
    "Y_local = y_train[:-global_dataset_size]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(np.unique(y_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "import utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<module 'utils' from '/home/seth/projects/fed-learn-experiment/utils.py'>"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import importlib\n",
    "importlib.reload(utils)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_local_list, Y_local_list = utils.split_training_set(1500, 20, X_local, Y_local)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert class vectors to binary class matrices\n",
    "num_classes = 10\n",
    "Y_global = keras.utils.to_categorical(Y_global, num_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test = keras.utils.to_categorical(y_test, num_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10000, 28, 28, 1)"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define models and compile & fit function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def custom_model():\n",
    "    model = Sequential()\n",
    "    model.add(Flatten(input_shape=input_shape))\n",
    "    model.add(Dense(200, activation='relu'))\n",
    "    model.add(Dense(200, activation='relu'))\n",
    "    model.add(Dense(num_classes, activation='softmax'))\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compile_model(model):  \n",
    "    # initiate SGD optimizer\n",
    "    opt = keras.optimizers.SGD(lr=0.1)\n",
    "    model.compile(loss='mean_squared_error', optimizer=opt, metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compile_model_lr(model):  \n",
    "    # initiate SGD optimizer\n",
    "    opt = keras.optimizers.SGD(lr=lr, decay=1e-6, momentum=0.9, nesterov=True)\n",
    "    model.compile(loss='mean_squared_error', optimizer=opt, metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit_model_global(model, epochs):\n",
    "    now = datetime.datetime.now()\n",
    "    print (\"Training date and time : \")\n",
    "    print (now.strftime(\"%Y-%m-%d %H:%M:%S\"))\n",
    "    return model.fit(X_global, Y_global,\n",
    "                      batch_size=100,\n",
    "                      epochs=40,\n",
    "                      shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit_model_with_datasets(model, epochs, x_train, y_train):\n",
    "    now = datetime.datetime.now()\n",
    "    print (\"Training date and time : \")\n",
    "    print (now.strftime(\"%Y-%m-%d %H:%M:%S\"))\n",
    "    res =  model.fit(x_train, y_train,\n",
    "                      batch_size=batch_size,\n",
    "                      epochs=epochs,\n",
    "                      shuffle=True, validation_split=0.1)\n",
    "    print (\"Elasped Time: \" + str(datetime.datetime.now() - now))\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training date and time : \n",
      "2020-04-25 21:32:58\n",
      "Train on 27000 samples, validate on 3000 samples\n",
      "Elasped Time: 0:00:00.036891\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7efb55144c18>"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model1 = custom_model()\n",
    "compile_model(model1)\n",
    "fit_model_with_datasets(model1, 0, X_global, Y_global)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/seth/projects/fed-learn-experiment/flexp/lib/python3.6/site-packages/tensorflow_core/python/ops/init_ops.py:97: calling GlorotUniform.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n",
      "WARNING:tensorflow:From /home/seth/projects/fed-learn-experiment/flexp/lib/python3.6/site-packages/tensorflow_core/python/ops/init_ops.py:97: calling Zeros.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n"
     ]
    }
   ],
   "source": [
    "model_list = list()\n",
    "for _ in range(20):\n",
    "    model_list.append(tf.keras.models.clone_model(model1)) \n",
    "    model_list[_].set_weights(model1.get_weights())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sort models according to similarity. We arbitrarily take the model1 as a \"standard\"\n",
    "standard_model = tf.keras.models.clone_model(model1)\n",
    "standard_model.set_weights(model_list[0].get_weights())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "import semantic_drift"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<module 'semantic_drift' from '/home/seth/projects/fed-learn-experiment/semantic_drift.py'>"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "importlib.reload(semantic_drift)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conduct transfer learning in local models using different datasets & epochs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training date and time : \n",
      "2020-04-25 21:07:47\n",
      "Train on 1350 samples, validate on 150 samples\n",
      "Epoch 1/10\n",
      "1350/1350 [==============================] - 0s 101us/sample - loss: 0.0689 - acc: 0.5333 - val_loss: 0.0667 - val_acc: 0.5267\n",
      "Epoch 2/10\n",
      "1350/1350 [==============================] - 0s 32us/sample - loss: 0.0667 - acc: 0.5407 - val_loss: 0.0647 - val_acc: 0.5400\n",
      "Epoch 3/10\n",
      "1350/1350 [==============================] - 0s 29us/sample - loss: 0.0645 - acc: 0.5533 - val_loss: 0.0628 - val_acc: 0.5667\n",
      "Epoch 4/10\n",
      "1350/1350 [==============================] - 0s 31us/sample - loss: 0.0624 - acc: 0.5674 - val_loss: 0.0609 - val_acc: 0.5800\n",
      "Epoch 5/10\n",
      "1350/1350 [==============================] - 0s 29us/sample - loss: 0.0603 - acc: 0.5785 - val_loss: 0.0590 - val_acc: 0.5867\n",
      "Epoch 6/10\n",
      "1350/1350 [==============================] - 0s 32us/sample - loss: 0.0582 - acc: 0.5970 - val_loss: 0.0571 - val_acc: 0.6200\n",
      "Epoch 7/10\n",
      "1350/1350 [==============================] - 0s 31us/sample - loss: 0.0561 - acc: 0.6193 - val_loss: 0.0552 - val_acc: 0.6200\n",
      "Epoch 8/10\n",
      "1350/1350 [==============================] - 0s 30us/sample - loss: 0.0541 - acc: 0.6415 - val_loss: 0.0534 - val_acc: 0.6467\n",
      "Epoch 9/10\n",
      "1350/1350 [==============================] - 0s 30us/sample - loss: 0.0520 - acc: 0.6830 - val_loss: 0.0516 - val_acc: 0.6733\n",
      "Epoch 10/10\n",
      "1350/1350 [==============================] - 0s 30us/sample - loss: 0.0500 - acc: 0.7081 - val_loss: 0.0497 - val_acc: 0.7067\n",
      "Elasped Time: 0:00:00.662702\n",
      "Training date and time : \n",
      "2020-04-25 21:07:48\n",
      "Train on 1350 samples, validate on 150 samples\n",
      "Epoch 1/20\n",
      "1350/1350 [==============================] - 0s 100us/sample - loss: 0.0671 - acc: 0.5407 - val_loss: 0.0622 - val_acc: 0.6000\n",
      "Epoch 2/20\n",
      "1350/1350 [==============================] - 0s 29us/sample - loss: 0.0647 - acc: 0.5541 - val_loss: 0.0597 - val_acc: 0.6133\n",
      "Epoch 3/20\n",
      "1350/1350 [==============================] - 0s 29us/sample - loss: 0.0623 - acc: 0.5659 - val_loss: 0.0572 - val_acc: 0.6333\n",
      "Epoch 4/20\n",
      "1350/1350 [==============================] - 0s 29us/sample - loss: 0.0600 - acc: 0.5815 - val_loss: 0.0547 - val_acc: 0.6467\n",
      "Epoch 5/20\n",
      "1350/1350 [==============================] - 0s 31us/sample - loss: 0.0576 - acc: 0.6030 - val_loss: 0.0524 - val_acc: 0.6533\n",
      "Epoch 6/20\n",
      "1350/1350 [==============================] - 0s 30us/sample - loss: 0.0554 - acc: 0.6281 - val_loss: 0.0501 - val_acc: 0.6667\n",
      "Epoch 7/20\n",
      "1350/1350 [==============================] - 0s 31us/sample - loss: 0.0531 - acc: 0.6585 - val_loss: 0.0480 - val_acc: 0.6867\n",
      "Epoch 8/20\n",
      "1350/1350 [==============================] - 0s 30us/sample - loss: 0.0508 - acc: 0.6933 - val_loss: 0.0459 - val_acc: 0.7267\n",
      "Epoch 9/20\n",
      "1350/1350 [==============================] - 0s 29us/sample - loss: 0.0486 - acc: 0.7274 - val_loss: 0.0438 - val_acc: 0.7467\n",
      "Epoch 10/20\n",
      "1350/1350 [==============================] - 0s 30us/sample - loss: 0.0464 - acc: 0.7652 - val_loss: 0.0418 - val_acc: 0.7867\n",
      "Epoch 11/20\n",
      "1350/1350 [==============================] - 0s 31us/sample - loss: 0.0442 - acc: 0.7881 - val_loss: 0.0399 - val_acc: 0.8000\n",
      "Epoch 12/20\n",
      "1350/1350 [==============================] - 0s 30us/sample - loss: 0.0422 - acc: 0.7970 - val_loss: 0.0381 - val_acc: 0.8133\n",
      "Epoch 13/20\n",
      "1350/1350 [==============================] - 0s 31us/sample - loss: 0.0402 - acc: 0.8111 - val_loss: 0.0364 - val_acc: 0.8267\n",
      "Epoch 14/20\n",
      "1350/1350 [==============================] - 0s 29us/sample - loss: 0.0383 - acc: 0.8222 - val_loss: 0.0349 - val_acc: 0.8400\n",
      "Epoch 15/20\n",
      "1350/1350 [==============================] - 0s 30us/sample - loss: 0.0366 - acc: 0.8319 - val_loss: 0.0336 - val_acc: 0.8400\n",
      "Epoch 16/20\n",
      "1350/1350 [==============================] - 0s 30us/sample - loss: 0.0350 - acc: 0.8437 - val_loss: 0.0324 - val_acc: 0.8600\n",
      "Epoch 17/20\n",
      "1350/1350 [==============================] - 0s 31us/sample - loss: 0.0335 - acc: 0.8481 - val_loss: 0.0313 - val_acc: 0.8600\n",
      "Epoch 18/20\n",
      "1350/1350 [==============================] - 0s 30us/sample - loss: 0.0322 - acc: 0.8556 - val_loss: 0.0302 - val_acc: 0.8600\n",
      "Epoch 19/20\n",
      "1350/1350 [==============================] - 0s 29us/sample - loss: 0.0309 - acc: 0.8593 - val_loss: 0.0293 - val_acc: 0.8600\n",
      "Epoch 20/20\n",
      "1350/1350 [==============================] - 0s 33us/sample - loss: 0.0298 - acc: 0.8667 - val_loss: 0.0284 - val_acc: 0.8667\n",
      "Elasped Time: 0:00:01.083046\n",
      "Training date and time : \n",
      "2020-04-25 21:07:49\n",
      "Train on 1350 samples, validate on 150 samples\n",
      "Epoch 1/30\n",
      "1350/1350 [==============================] - 0s 102us/sample - loss: 0.0685 - acc: 0.5296 - val_loss: 0.0696 - val_acc: 0.5533\n",
      "Epoch 2/30\n",
      "1350/1350 [==============================] - 0s 31us/sample - loss: 0.0664 - acc: 0.5481 - val_loss: 0.0677 - val_acc: 0.5667\n",
      "Epoch 3/30\n",
      "1350/1350 [==============================] - 0s 30us/sample - loss: 0.0642 - acc: 0.5652 - val_loss: 0.0657 - val_acc: 0.5800\n",
      "Epoch 4/30\n",
      "1350/1350 [==============================] - 0s 34us/sample - loss: 0.0620 - acc: 0.5800 - val_loss: 0.0638 - val_acc: 0.5933\n",
      "Epoch 5/30\n",
      "1350/1350 [==============================] - 0s 30us/sample - loss: 0.0598 - acc: 0.6030 - val_loss: 0.0618 - val_acc: 0.6200\n",
      "Epoch 6/30\n",
      "1350/1350 [==============================] - 0s 33us/sample - loss: 0.0576 - acc: 0.6215 - val_loss: 0.0598 - val_acc: 0.6267\n",
      "Epoch 7/30\n",
      "1350/1350 [==============================] - 0s 32us/sample - loss: 0.0554 - acc: 0.6474 - val_loss: 0.0579 - val_acc: 0.6533\n",
      "Epoch 8/30\n",
      "1350/1350 [==============================] - 0s 29us/sample - loss: 0.0533 - acc: 0.6748 - val_loss: 0.0560 - val_acc: 0.6867\n",
      "Epoch 9/30\n",
      "1350/1350 [==============================] - 0s 30us/sample - loss: 0.0511 - acc: 0.6985 - val_loss: 0.0541 - val_acc: 0.7133\n",
      "Epoch 10/30\n",
      "1350/1350 [==============================] - 0s 31us/sample - loss: 0.0490 - acc: 0.7296 - val_loss: 0.0521 - val_acc: 0.7467\n",
      "Epoch 11/30\n",
      "1350/1350 [==============================] - 0s 29us/sample - loss: 0.0469 - acc: 0.7637 - val_loss: 0.0503 - val_acc: 0.7733\n",
      "Epoch 12/30\n",
      "1350/1350 [==============================] - 0s 30us/sample - loss: 0.0448 - acc: 0.7822 - val_loss: 0.0483 - val_acc: 0.7933\n",
      "Epoch 13/30\n",
      "1350/1350 [==============================] - 0s 29us/sample - loss: 0.0428 - acc: 0.7948 - val_loss: 0.0466 - val_acc: 0.8267\n",
      "Epoch 14/30\n",
      "1350/1350 [==============================] - 0s 30us/sample - loss: 0.0409 - acc: 0.8096 - val_loss: 0.0447 - val_acc: 0.8200\n",
      "Epoch 15/30\n",
      "1350/1350 [==============================] - 0s 30us/sample - loss: 0.0390 - acc: 0.8237 - val_loss: 0.0429 - val_acc: 0.8267\n",
      "Epoch 16/30\n",
      "1350/1350 [==============================] - 0s 29us/sample - loss: 0.0372 - acc: 0.8274 - val_loss: 0.0412 - val_acc: 0.8333\n",
      "Epoch 17/30\n",
      "1350/1350 [==============================] - 0s 31us/sample - loss: 0.0355 - acc: 0.8378 - val_loss: 0.0399 - val_acc: 0.8467\n",
      "Epoch 18/30\n",
      "1350/1350 [==============================] - 0s 30us/sample - loss: 0.0340 - acc: 0.8496 - val_loss: 0.0383 - val_acc: 0.8400\n",
      "Epoch 19/30\n",
      "1350/1350 [==============================] - 0s 29us/sample - loss: 0.0326 - acc: 0.8519 - val_loss: 0.0369 - val_acc: 0.8467\n",
      "Epoch 20/30\n",
      "1350/1350 [==============================] - 0s 30us/sample - loss: 0.0313 - acc: 0.8622 - val_loss: 0.0357 - val_acc: 0.8467\n",
      "Epoch 21/30\n",
      "1350/1350 [==============================] - 0s 30us/sample - loss: 0.0300 - acc: 0.8689 - val_loss: 0.0344 - val_acc: 0.8600\n",
      "Epoch 22/30\n",
      "1350/1350 [==============================] - 0s 31us/sample - loss: 0.0289 - acc: 0.8667 - val_loss: 0.0336 - val_acc: 0.8400\n",
      "Epoch 23/30\n",
      "1350/1350 [==============================] - 0s 31us/sample - loss: 0.0278 - acc: 0.8778 - val_loss: 0.0323 - val_acc: 0.8600\n",
      "Epoch 24/30\n",
      "1350/1350 [==============================] - 0s 30us/sample - loss: 0.0269 - acc: 0.8785 - val_loss: 0.0315 - val_acc: 0.8467\n",
      "Epoch 25/30\n",
      "1350/1350 [==============================] - 0s 28us/sample - loss: 0.0260 - acc: 0.8830 - val_loss: 0.0307 - val_acc: 0.8333\n",
      "Epoch 26/30\n",
      "1350/1350 [==============================] - 0s 29us/sample - loss: 0.0251 - acc: 0.8867 - val_loss: 0.0298 - val_acc: 0.8467\n",
      "Epoch 27/30\n",
      "1350/1350 [==============================] - 0s 28us/sample - loss: 0.0244 - acc: 0.8881 - val_loss: 0.0292 - val_acc: 0.8333\n",
      "Epoch 28/30\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1350/1350 [==============================] - 0s 30us/sample - loss: 0.0236 - acc: 0.8896 - val_loss: 0.0284 - val_acc: 0.8467\n",
      "Epoch 29/30\n",
      "1350/1350 [==============================] - 0s 29us/sample - loss: 0.0229 - acc: 0.8941 - val_loss: 0.0278 - val_acc: 0.8533\n",
      "Epoch 30/30\n",
      "1350/1350 [==============================] - 0s 31us/sample - loss: 0.0223 - acc: 0.8956 - val_loss: 0.0270 - val_acc: 0.8600\n",
      "Elasped Time: 0:00:01.524528\n",
      "Training date and time : \n",
      "2020-04-25 21:07:50\n",
      "Train on 1350 samples, validate on 150 samples\n",
      "Epoch 1/40\n",
      "1350/1350 [==============================] - 0s 114us/sample - loss: 0.0662 - acc: 0.5600 - val_loss: 0.0666 - val_acc: 0.5667\n",
      "Epoch 2/40\n",
      "1350/1350 [==============================] - 0s 32us/sample - loss: 0.0638 - acc: 0.5667 - val_loss: 0.0643 - val_acc: 0.5933\n",
      "Epoch 3/40\n",
      "1350/1350 [==============================] - 0s 30us/sample - loss: 0.0614 - acc: 0.5822 - val_loss: 0.0619 - val_acc: 0.6133\n",
      "Epoch 4/40\n",
      "1350/1350 [==============================] - 0s 35us/sample - loss: 0.0591 - acc: 0.5970 - val_loss: 0.0596 - val_acc: 0.6267\n",
      "Epoch 5/40\n",
      "1350/1350 [==============================] - 0s 32us/sample - loss: 0.0569 - acc: 0.6185 - val_loss: 0.0573 - val_acc: 0.6400\n",
      "Epoch 6/40\n",
      "1350/1350 [==============================] - 0s 31us/sample - loss: 0.0547 - acc: 0.6348 - val_loss: 0.0552 - val_acc: 0.6600\n",
      "Epoch 7/40\n",
      "1350/1350 [==============================] - 0s 30us/sample - loss: 0.0526 - acc: 0.6585 - val_loss: 0.0531 - val_acc: 0.6667\n",
      "Epoch 8/40\n",
      "1350/1350 [==============================] - 0s 30us/sample - loss: 0.0506 - acc: 0.6756 - val_loss: 0.0511 - val_acc: 0.6867\n",
      "Epoch 9/40\n",
      "1350/1350 [==============================] - 0s 31us/sample - loss: 0.0486 - acc: 0.6933 - val_loss: 0.0492 - val_acc: 0.7333\n",
      "Epoch 10/40\n",
      "1350/1350 [==============================] - 0s 30us/sample - loss: 0.0467 - acc: 0.7148 - val_loss: 0.0473 - val_acc: 0.7467\n",
      "Epoch 11/40\n",
      "1350/1350 [==============================] - 0s 29us/sample - loss: 0.0449 - acc: 0.7333 - val_loss: 0.0457 - val_acc: 0.7600\n",
      "Epoch 12/40\n",
      "1350/1350 [==============================] - 0s 30us/sample - loss: 0.0431 - acc: 0.7496 - val_loss: 0.0440 - val_acc: 0.7733\n",
      "Epoch 13/40\n",
      "1350/1350 [==============================] - 0s 31us/sample - loss: 0.0413 - acc: 0.7733 - val_loss: 0.0423 - val_acc: 0.7867\n",
      "Epoch 14/40\n",
      "1350/1350 [==============================] - 0s 30us/sample - loss: 0.0396 - acc: 0.7904 - val_loss: 0.0409 - val_acc: 0.7800\n",
      "Epoch 15/40\n",
      "1350/1350 [==============================] - 0s 30us/sample - loss: 0.0380 - acc: 0.8044 - val_loss: 0.0395 - val_acc: 0.7733\n",
      "Epoch 16/40\n",
      "1350/1350 [==============================] - 0s 32us/sample - loss: 0.0364 - acc: 0.8126 - val_loss: 0.0380 - val_acc: 0.7933\n",
      "Epoch 17/40\n",
      "1350/1350 [==============================] - 0s 29us/sample - loss: 0.0349 - acc: 0.8267 - val_loss: 0.0368 - val_acc: 0.7933\n",
      "Epoch 18/40\n",
      "1350/1350 [==============================] - 0s 31us/sample - loss: 0.0335 - acc: 0.8370 - val_loss: 0.0357 - val_acc: 0.7933\n",
      "Epoch 19/40\n",
      "1350/1350 [==============================] - 0s 30us/sample - loss: 0.0323 - acc: 0.8496 - val_loss: 0.0345 - val_acc: 0.8000\n",
      "Epoch 20/40\n",
      "1350/1350 [==============================] - 0s 30us/sample - loss: 0.0311 - acc: 0.8467 - val_loss: 0.0334 - val_acc: 0.8067\n",
      "Epoch 21/40\n",
      "1350/1350 [==============================] - 0s 30us/sample - loss: 0.0300 - acc: 0.8541 - val_loss: 0.0326 - val_acc: 0.8133\n",
      "Epoch 22/40\n",
      "1350/1350 [==============================] - 0s 31us/sample - loss: 0.0290 - acc: 0.8630 - val_loss: 0.0315 - val_acc: 0.8333\n",
      "Epoch 23/40\n",
      "1350/1350 [==============================] - 0s 29us/sample - loss: 0.0280 - acc: 0.8674 - val_loss: 0.0309 - val_acc: 0.8267\n",
      "Epoch 24/40\n",
      "1350/1350 [==============================] - 0s 30us/sample - loss: 0.0272 - acc: 0.8681 - val_loss: 0.0301 - val_acc: 0.8400\n",
      "Epoch 25/40\n",
      "1350/1350 [==============================] - 0s 29us/sample - loss: 0.0264 - acc: 0.8726 - val_loss: 0.0294 - val_acc: 0.8333\n",
      "Epoch 26/40\n",
      "1350/1350 [==============================] - 0s 29us/sample - loss: 0.0256 - acc: 0.8711 - val_loss: 0.0288 - val_acc: 0.8400\n",
      "Epoch 27/40\n",
      "1350/1350 [==============================] - 0s 32us/sample - loss: 0.0250 - acc: 0.8748 - val_loss: 0.0283 - val_acc: 0.8333\n",
      "Epoch 28/40\n",
      "1350/1350 [==============================] - 0s 29us/sample - loss: 0.0243 - acc: 0.8793 - val_loss: 0.0275 - val_acc: 0.8467\n",
      "Epoch 29/40\n",
      "1350/1350 [==============================] - 0s 30us/sample - loss: 0.0237 - acc: 0.8800 - val_loss: 0.0271 - val_acc: 0.8333\n",
      "Epoch 30/40\n",
      "1350/1350 [==============================] - 0s 29us/sample - loss: 0.0231 - acc: 0.8844 - val_loss: 0.0267 - val_acc: 0.8600\n",
      "Epoch 31/40\n",
      "1350/1350 [==============================] - 0s 30us/sample - loss: 0.0225 - acc: 0.8874 - val_loss: 0.0262 - val_acc: 0.8467\n",
      "Epoch 32/40\n",
      "1350/1350 [==============================] - 0s 29us/sample - loss: 0.0220 - acc: 0.8867 - val_loss: 0.0259 - val_acc: 0.8533\n",
      "Epoch 33/40\n",
      "1350/1350 [==============================] - 0s 30us/sample - loss: 0.0216 - acc: 0.8933 - val_loss: 0.0254 - val_acc: 0.8467\n",
      "Epoch 34/40\n",
      "1350/1350 [==============================] - 0s 29us/sample - loss: 0.0211 - acc: 0.8933 - val_loss: 0.0249 - val_acc: 0.8467\n",
      "Epoch 35/40\n",
      "1350/1350 [==============================] - 0s 29us/sample - loss: 0.0207 - acc: 0.8978 - val_loss: 0.0246 - val_acc: 0.8533\n",
      "Epoch 36/40\n",
      "1350/1350 [==============================] - 0s 31us/sample - loss: 0.0203 - acc: 0.8970 - val_loss: 0.0243 - val_acc: 0.8600\n",
      "Epoch 37/40\n",
      "1350/1350 [==============================] - 0s 30us/sample - loss: 0.0199 - acc: 0.9000 - val_loss: 0.0240 - val_acc: 0.8533\n",
      "Epoch 38/40\n",
      "1350/1350 [==============================] - 0s 30us/sample - loss: 0.0195 - acc: 0.9044 - val_loss: 0.0238 - val_acc: 0.8467\n",
      "Epoch 39/40\n",
      "1350/1350 [==============================] - 0s 31us/sample - loss: 0.0192 - acc: 0.9044 - val_loss: 0.0235 - val_acc: 0.8467\n",
      "Epoch 40/40\n",
      "1350/1350 [==============================] - 0s 32us/sample - loss: 0.0188 - acc: 0.9037 - val_loss: 0.0232 - val_acc: 0.8733\n",
      "Elasped Time: 0:00:01.998393\n",
      "Training date and time : \n",
      "2020-04-25 21:07:52\n",
      "Train on 1350 samples, validate on 150 samples\n",
      "Epoch 1/50\n",
      "1350/1350 [==============================] - 0s 109us/sample - loss: 0.0676 - acc: 0.5319 - val_loss: 0.0688 - val_acc: 0.5867\n",
      "Epoch 2/50\n",
      "1350/1350 [==============================] - 0s 30us/sample - loss: 0.0653 - acc: 0.5541 - val_loss: 0.0669 - val_acc: 0.5933\n",
      "Epoch 3/50\n",
      "1350/1350 [==============================] - 0s 29us/sample - loss: 0.0631 - acc: 0.5785 - val_loss: 0.0650 - val_acc: 0.6200\n",
      "Epoch 4/50\n",
      "1350/1350 [==============================] - 0s 30us/sample - loss: 0.0608 - acc: 0.6074 - val_loss: 0.0630 - val_acc: 0.6467\n",
      "Epoch 5/50\n",
      "1350/1350 [==============================] - 0s 31us/sample - loss: 0.0586 - acc: 0.6267 - val_loss: 0.0611 - val_acc: 0.6533\n",
      "Epoch 6/50\n",
      "1350/1350 [==============================] - 0s 30us/sample - loss: 0.0564 - acc: 0.6444 - val_loss: 0.0592 - val_acc: 0.6800\n",
      "Epoch 7/50\n",
      "1350/1350 [==============================] - 0s 30us/sample - loss: 0.0542 - acc: 0.6719 - val_loss: 0.0572 - val_acc: 0.6867\n",
      "Epoch 8/50\n",
      "1350/1350 [==============================] - 0s 30us/sample - loss: 0.0521 - acc: 0.6881 - val_loss: 0.0554 - val_acc: 0.6867\n",
      "Epoch 9/50\n",
      "1350/1350 [==============================] - 0s 30us/sample - loss: 0.0500 - acc: 0.7081 - val_loss: 0.0536 - val_acc: 0.6867\n",
      "Epoch 10/50\n",
      "1350/1350 [==============================] - 0s 30us/sample - loss: 0.0481 - acc: 0.7304 - val_loss: 0.0519 - val_acc: 0.6867\n",
      "Epoch 11/50\n",
      "1350/1350 [==============================] - 0s 30us/sample - loss: 0.0461 - acc: 0.7556 - val_loss: 0.0500 - val_acc: 0.6933\n",
      "Epoch 12/50\n",
      "1350/1350 [==============================] - 0s 29us/sample - loss: 0.0442 - acc: 0.7674 - val_loss: 0.0482 - val_acc: 0.7267\n",
      "Epoch 13/50\n",
      "1350/1350 [==============================] - 0s 30us/sample - loss: 0.0424 - acc: 0.7867 - val_loss: 0.0466 - val_acc: 0.7667\n",
      "Epoch 14/50\n",
      "1350/1350 [==============================] - 0s 30us/sample - loss: 0.0406 - acc: 0.7948 - val_loss: 0.0451 - val_acc: 0.7800\n",
      "Epoch 15/50\n",
      "1350/1350 [==============================] - 0s 33us/sample - loss: 0.0390 - acc: 0.8081 - val_loss: 0.0436 - val_acc: 0.7933\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 16/50\n",
      "1350/1350 [==============================] - 0s 30us/sample - loss: 0.0373 - acc: 0.8178 - val_loss: 0.0421 - val_acc: 0.8067\n",
      "Epoch 17/50\n",
      "1350/1350 [==============================] - 0s 30us/sample - loss: 0.0359 - acc: 0.8304 - val_loss: 0.0406 - val_acc: 0.8267\n",
      "Epoch 18/50\n",
      "1350/1350 [==============================] - 0s 29us/sample - loss: 0.0345 - acc: 0.8326 - val_loss: 0.0393 - val_acc: 0.8267\n",
      "Epoch 19/50\n",
      "1350/1350 [==============================] - 0s 29us/sample - loss: 0.0332 - acc: 0.8452 - val_loss: 0.0382 - val_acc: 0.8267\n",
      "Epoch 20/50\n",
      "1350/1350 [==============================] - 0s 31us/sample - loss: 0.0320 - acc: 0.8459 - val_loss: 0.0372 - val_acc: 0.8200\n",
      "Epoch 21/50\n",
      "1350/1350 [==============================] - 0s 30us/sample - loss: 0.0309 - acc: 0.8533 - val_loss: 0.0362 - val_acc: 0.8200\n",
      "Epoch 22/50\n",
      "1350/1350 [==============================] - 0s 30us/sample - loss: 0.0299 - acc: 0.8615 - val_loss: 0.0352 - val_acc: 0.8133\n",
      "Epoch 23/50\n",
      "1350/1350 [==============================] - 0s 28us/sample - loss: 0.0289 - acc: 0.8615 - val_loss: 0.0343 - val_acc: 0.8200\n",
      "Epoch 24/50\n",
      "1350/1350 [==============================] - 0s 29us/sample - loss: 0.0280 - acc: 0.8644 - val_loss: 0.0333 - val_acc: 0.8200\n",
      "Epoch 25/50\n",
      "1350/1350 [==============================] - 0s 29us/sample - loss: 0.0271 - acc: 0.8637 - val_loss: 0.0326 - val_acc: 0.8267\n",
      "Epoch 26/50\n",
      "1350/1350 [==============================] - 0s 29us/sample - loss: 0.0264 - acc: 0.8681 - val_loss: 0.0318 - val_acc: 0.8267\n",
      "Epoch 27/50\n",
      "1350/1350 [==============================] - 0s 30us/sample - loss: 0.0256 - acc: 0.8689 - val_loss: 0.0311 - val_acc: 0.8267\n",
      "Epoch 28/50\n",
      "1350/1350 [==============================] - 0s 32us/sample - loss: 0.0250 - acc: 0.8726 - val_loss: 0.0305 - val_acc: 0.8333\n",
      "Epoch 29/50\n",
      "1350/1350 [==============================] - 0s 31us/sample - loss: 0.0242 - acc: 0.8800 - val_loss: 0.0298 - val_acc: 0.8333\n",
      "Epoch 30/50\n",
      "1350/1350 [==============================] - 0s 30us/sample - loss: 0.0237 - acc: 0.8815 - val_loss: 0.0293 - val_acc: 0.8400\n",
      "Epoch 31/50\n",
      "1350/1350 [==============================] - 0s 32us/sample - loss: 0.0231 - acc: 0.8800 - val_loss: 0.0287 - val_acc: 0.8400\n",
      "Epoch 32/50\n",
      "1350/1350 [==============================] - 0s 29us/sample - loss: 0.0225 - acc: 0.8837 - val_loss: 0.0282 - val_acc: 0.8400\n",
      "Epoch 33/50\n",
      "1350/1350 [==============================] - 0s 29us/sample - loss: 0.0220 - acc: 0.8852 - val_loss: 0.0277 - val_acc: 0.8400\n",
      "Epoch 34/50\n",
      "1350/1350 [==============================] - 0s 29us/sample - loss: 0.0215 - acc: 0.8919 - val_loss: 0.0273 - val_acc: 0.8400\n",
      "Epoch 35/50\n",
      "1350/1350 [==============================] - 0s 31us/sample - loss: 0.0211 - acc: 0.8933 - val_loss: 0.0269 - val_acc: 0.8400\n",
      "Epoch 36/50\n",
      "1350/1350 [==============================] - 0s 32us/sample - loss: 0.0206 - acc: 0.8956 - val_loss: 0.0268 - val_acc: 0.8400\n",
      "Epoch 37/50\n",
      "1350/1350 [==============================] - 0s 30us/sample - loss: 0.0202 - acc: 0.9000 - val_loss: 0.0261 - val_acc: 0.8333\n",
      "Epoch 38/50\n",
      "1350/1350 [==============================] - 0s 30us/sample - loss: 0.0198 - acc: 0.8985 - val_loss: 0.0256 - val_acc: 0.8400\n",
      "Epoch 39/50\n",
      "1350/1350 [==============================] - 0s 31us/sample - loss: 0.0194 - acc: 0.9007 - val_loss: 0.0250 - val_acc: 0.8533\n",
      "Epoch 40/50\n",
      "1350/1350 [==============================] - 0s 30us/sample - loss: 0.0191 - acc: 0.9052 - val_loss: 0.0245 - val_acc: 0.8600\n",
      "Epoch 41/50\n",
      "1350/1350 [==============================] - 0s 32us/sample - loss: 0.0187 - acc: 0.9037 - val_loss: 0.0244 - val_acc: 0.8533\n",
      "Epoch 42/50\n",
      "1350/1350 [==============================] - 0s 29us/sample - loss: 0.0184 - acc: 0.9089 - val_loss: 0.0244 - val_acc: 0.8467\n",
      "Epoch 43/50\n",
      "1350/1350 [==============================] - 0s 30us/sample - loss: 0.0180 - acc: 0.9111 - val_loss: 0.0241 - val_acc: 0.8600\n",
      "Epoch 44/50\n",
      "1350/1350 [==============================] - 0s 29us/sample - loss: 0.0177 - acc: 0.9126 - val_loss: 0.0236 - val_acc: 0.8600\n",
      "Epoch 45/50\n",
      "1350/1350 [==============================] - 0s 29us/sample - loss: 0.0174 - acc: 0.9156 - val_loss: 0.0237 - val_acc: 0.8600\n",
      "Epoch 46/50\n",
      "1350/1350 [==============================] - 0s 29us/sample - loss: 0.0171 - acc: 0.9156 - val_loss: 0.0233 - val_acc: 0.8600\n",
      "Epoch 47/50\n",
      "1350/1350 [==============================] - 0s 29us/sample - loss: 0.0168 - acc: 0.9170 - val_loss: 0.0230 - val_acc: 0.8600\n",
      "Epoch 48/50\n",
      "1350/1350 [==============================] - 0s 30us/sample - loss: 0.0166 - acc: 0.9141 - val_loss: 0.0226 - val_acc: 0.8600\n",
      "Epoch 49/50\n",
      "1350/1350 [==============================] - 0s 30us/sample - loss: 0.0163 - acc: 0.9170 - val_loss: 0.0223 - val_acc: 0.8600\n",
      "Epoch 50/50\n",
      "1350/1350 [==============================] - 0s 28us/sample - loss: 0.0161 - acc: 0.9215 - val_loss: 0.0221 - val_acc: 0.8600\n",
      "Elasped Time: 0:00:02.370549\n",
      "Training date and time : \n",
      "2020-04-25 21:07:55\n",
      "Train on 1350 samples, validate on 150 samples\n",
      "Epoch 1/60\n",
      "1350/1350 [==============================] - 0s 111us/sample - loss: 0.0683 - acc: 0.5296 - val_loss: 0.0663 - val_acc: 0.5133\n",
      "Epoch 2/60\n",
      "1350/1350 [==============================] - 0s 32us/sample - loss: 0.0663 - acc: 0.5511 - val_loss: 0.0645 - val_acc: 0.5133\n",
      "Epoch 3/60\n",
      "1350/1350 [==============================] - 0s 31us/sample - loss: 0.0642 - acc: 0.5659 - val_loss: 0.0627 - val_acc: 0.5467\n",
      "Epoch 4/60\n",
      "1350/1350 [==============================] - 0s 30us/sample - loss: 0.0621 - acc: 0.5689 - val_loss: 0.0608 - val_acc: 0.5467\n",
      "Epoch 5/60\n",
      "1350/1350 [==============================] - 0s 29us/sample - loss: 0.0601 - acc: 0.5881 - val_loss: 0.0592 - val_acc: 0.5467\n",
      "Epoch 6/60\n",
      "1350/1350 [==============================] - 0s 30us/sample - loss: 0.0581 - acc: 0.6037 - val_loss: 0.0575 - val_acc: 0.5533\n",
      "Epoch 7/60\n",
      "1350/1350 [==============================] - 0s 32us/sample - loss: 0.0562 - acc: 0.6185 - val_loss: 0.0559 - val_acc: 0.5667\n",
      "Epoch 8/60\n",
      "1350/1350 [==============================] - 0s 31us/sample - loss: 0.0543 - acc: 0.6356 - val_loss: 0.0544 - val_acc: 0.5733\n",
      "Epoch 9/60\n",
      "1350/1350 [==============================] - 0s 33us/sample - loss: 0.0525 - acc: 0.6630 - val_loss: 0.0530 - val_acc: 0.5867\n",
      "Epoch 10/60\n",
      "1350/1350 [==============================] - 0s 30us/sample - loss: 0.0507 - acc: 0.6807 - val_loss: 0.0517 - val_acc: 0.5867\n",
      "Epoch 11/60\n",
      "1350/1350 [==============================] - 0s 32us/sample - loss: 0.0491 - acc: 0.6985 - val_loss: 0.0503 - val_acc: 0.6067\n",
      "Epoch 12/60\n",
      "1350/1350 [==============================] - 0s 29us/sample - loss: 0.0474 - acc: 0.7207 - val_loss: 0.0489 - val_acc: 0.6133\n",
      "Epoch 13/60\n",
      "1350/1350 [==============================] - 0s 30us/sample - loss: 0.0457 - acc: 0.7430 - val_loss: 0.0476 - val_acc: 0.6467\n",
      "Epoch 14/60\n",
      "1350/1350 [==============================] - 0s 31us/sample - loss: 0.0442 - acc: 0.7585 - val_loss: 0.0464 - val_acc: 0.6667\n",
      "Epoch 15/60\n",
      "1350/1350 [==============================] - 0s 28us/sample - loss: 0.0427 - acc: 0.7704 - val_loss: 0.0452 - val_acc: 0.6867\n",
      "Epoch 16/60\n",
      "1350/1350 [==============================] - 0s 28us/sample - loss: 0.0412 - acc: 0.7896 - val_loss: 0.0442 - val_acc: 0.6867\n",
      "Epoch 17/60\n",
      "1350/1350 [==============================] - 0s 29us/sample - loss: 0.0398 - acc: 0.7896 - val_loss: 0.0429 - val_acc: 0.7133\n",
      "Epoch 18/60\n",
      "1350/1350 [==============================] - 0s 30us/sample - loss: 0.0385 - acc: 0.7948 - val_loss: 0.0418 - val_acc: 0.7200\n",
      "Epoch 19/60\n",
      "1350/1350 [==============================] - 0s 30us/sample - loss: 0.0372 - acc: 0.8030 - val_loss: 0.0410 - val_acc: 0.7200\n",
      "Epoch 20/60\n",
      "1350/1350 [==============================] - 0s 29us/sample - loss: 0.0360 - acc: 0.8111 - val_loss: 0.0403 - val_acc: 0.7267\n",
      "Epoch 21/60\n",
      "1350/1350 [==============================] - 0s 30us/sample - loss: 0.0349 - acc: 0.8170 - val_loss: 0.0390 - val_acc: 0.7400\n",
      "Epoch 22/60\n",
      "1350/1350 [==============================] - 0s 29us/sample - loss: 0.0339 - acc: 0.8193 - val_loss: 0.0384 - val_acc: 0.7533\n",
      "Epoch 23/60\n",
      "1350/1350 [==============================] - 0s 30us/sample - loss: 0.0329 - acc: 0.8252 - val_loss: 0.0379 - val_acc: 0.7533\n",
      "Epoch 24/60\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1350/1350 [==============================] - 0s 30us/sample - loss: 0.0319 - acc: 0.8244 - val_loss: 0.0372 - val_acc: 0.7600\n",
      "Epoch 25/60\n",
      "1350/1350 [==============================] - 0s 30us/sample - loss: 0.0310 - acc: 0.8304 - val_loss: 0.0366 - val_acc: 0.7533\n",
      "Epoch 26/60\n",
      "1350/1350 [==============================] - 0s 31us/sample - loss: 0.0302 - acc: 0.8370 - val_loss: 0.0361 - val_acc: 0.7667\n",
      "Epoch 27/60\n",
      "1350/1350 [==============================] - 0s 31us/sample - loss: 0.0294 - acc: 0.8407 - val_loss: 0.0357 - val_acc: 0.7600\n",
      "Epoch 28/60\n",
      "1350/1350 [==============================] - 0s 32us/sample - loss: 0.0287 - acc: 0.8526 - val_loss: 0.0352 - val_acc: 0.7667\n",
      "Epoch 29/60\n",
      "1350/1350 [==============================] - 0s 30us/sample - loss: 0.0279 - acc: 0.8600 - val_loss: 0.0347 - val_acc: 0.7533\n",
      "Epoch 30/60\n",
      "1350/1350 [==============================] - 0s 31us/sample - loss: 0.0272 - acc: 0.8630 - val_loss: 0.0341 - val_acc: 0.7600\n",
      "Epoch 31/60\n",
      "1350/1350 [==============================] - 0s 32us/sample - loss: 0.0266 - acc: 0.8652 - val_loss: 0.0336 - val_acc: 0.7667\n",
      "Epoch 32/60\n",
      "1350/1350 [==============================] - 0s 30us/sample - loss: 0.0260 - acc: 0.8704 - val_loss: 0.0334 - val_acc: 0.7733\n",
      "Epoch 33/60\n",
      "1350/1350 [==============================] - 0s 32us/sample - loss: 0.0254 - acc: 0.8756 - val_loss: 0.0329 - val_acc: 0.7667\n",
      "Epoch 34/60\n",
      "1350/1350 [==============================] - 0s 32us/sample - loss: 0.0248 - acc: 0.8763 - val_loss: 0.0327 - val_acc: 0.7600\n",
      "Epoch 35/60\n",
      "1350/1350 [==============================] - 0s 32us/sample - loss: 0.0242 - acc: 0.8830 - val_loss: 0.0329 - val_acc: 0.7533\n",
      "Epoch 36/60\n",
      "1350/1350 [==============================] - 0s 31us/sample - loss: 0.0238 - acc: 0.8837 - val_loss: 0.0320 - val_acc: 0.7600\n",
      "Epoch 37/60\n",
      "1350/1350 [==============================] - 0s 30us/sample - loss: 0.0232 - acc: 0.8889 - val_loss: 0.0317 - val_acc: 0.7667\n",
      "Epoch 38/60\n",
      "1350/1350 [==============================] - 0s 29us/sample - loss: 0.0228 - acc: 0.8926 - val_loss: 0.0318 - val_acc: 0.7667\n",
      "Epoch 39/60\n",
      "1350/1350 [==============================] - 0s 29us/sample - loss: 0.0223 - acc: 0.8933 - val_loss: 0.0314 - val_acc: 0.7667\n",
      "Epoch 40/60\n",
      "1350/1350 [==============================] - 0s 31us/sample - loss: 0.0219 - acc: 0.8956 - val_loss: 0.0311 - val_acc: 0.7733\n",
      "Epoch 41/60\n",
      "1350/1350 [==============================] - 0s 32us/sample - loss: 0.0214 - acc: 0.9022 - val_loss: 0.0308 - val_acc: 0.7667\n",
      "Epoch 42/60\n",
      "1350/1350 [==============================] - 0s 31us/sample - loss: 0.0210 - acc: 0.9037 - val_loss: 0.0306 - val_acc: 0.7733\n",
      "Epoch 43/60\n",
      "1350/1350 [==============================] - 0s 30us/sample - loss: 0.0207 - acc: 0.9015 - val_loss: 0.0303 - val_acc: 0.7867\n",
      "Epoch 44/60\n",
      "1350/1350 [==============================] - 0s 31us/sample - loss: 0.0202 - acc: 0.9037 - val_loss: 0.0302 - val_acc: 0.7667\n",
      "Epoch 45/60\n",
      "1350/1350 [==============================] - 0s 30us/sample - loss: 0.0199 - acc: 0.9044 - val_loss: 0.0304 - val_acc: 0.7933\n",
      "Epoch 46/60\n",
      "1350/1350 [==============================] - 0s 30us/sample - loss: 0.0196 - acc: 0.9081 - val_loss: 0.0296 - val_acc: 0.7933\n",
      "Epoch 47/60\n",
      "1350/1350 [==============================] - 0s 30us/sample - loss: 0.0192 - acc: 0.9081 - val_loss: 0.0295 - val_acc: 0.8000\n",
      "Epoch 48/60\n",
      "1350/1350 [==============================] - 0s 29us/sample - loss: 0.0189 - acc: 0.9119 - val_loss: 0.0297 - val_acc: 0.8000\n",
      "Epoch 49/60\n",
      "1350/1350 [==============================] - 0s 29us/sample - loss: 0.0185 - acc: 0.9148 - val_loss: 0.0291 - val_acc: 0.7933\n",
      "Epoch 50/60\n",
      "1350/1350 [==============================] - 0s 31us/sample - loss: 0.0183 - acc: 0.9156 - val_loss: 0.0290 - val_acc: 0.8133\n",
      "Epoch 51/60\n",
      "1350/1350 [==============================] - 0s 33us/sample - loss: 0.0180 - acc: 0.9141 - val_loss: 0.0285 - val_acc: 0.7933\n",
      "Epoch 52/60\n",
      "1350/1350 [==============================] - 0s 32us/sample - loss: 0.0177 - acc: 0.9163 - val_loss: 0.0286 - val_acc: 0.7867\n",
      "Epoch 53/60\n",
      "1350/1350 [==============================] - 0s 32us/sample - loss: 0.0174 - acc: 0.9207 - val_loss: 0.0282 - val_acc: 0.8067\n",
      "Epoch 54/60\n",
      "1350/1350 [==============================] - 0s 31us/sample - loss: 0.0171 - acc: 0.9193 - val_loss: 0.0283 - val_acc: 0.8067\n",
      "Epoch 55/60\n",
      "1350/1350 [==============================] - 0s 31us/sample - loss: 0.0169 - acc: 0.9207 - val_loss: 0.0282 - val_acc: 0.8000\n",
      "Epoch 56/60\n",
      "1350/1350 [==============================] - 0s 34us/sample - loss: 0.0166 - acc: 0.9200 - val_loss: 0.0277 - val_acc: 0.8133\n",
      "Epoch 57/60\n",
      "1350/1350 [==============================] - 0s 33us/sample - loss: 0.0164 - acc: 0.9237 - val_loss: 0.0278 - val_acc: 0.8067\n",
      "Epoch 58/60\n",
      "1350/1350 [==============================] - 0s 31us/sample - loss: 0.0162 - acc: 0.9244 - val_loss: 0.0279 - val_acc: 0.8133\n",
      "Epoch 59/60\n",
      "1350/1350 [==============================] - 0s 31us/sample - loss: 0.0159 - acc: 0.9252 - val_loss: 0.0277 - val_acc: 0.8133\n",
      "Epoch 60/60\n",
      "1350/1350 [==============================] - 0s 31us/sample - loss: 0.0157 - acc: 0.9244 - val_loss: 0.0274 - val_acc: 0.8067\n",
      "Elasped Time: 0:00:02.854478\n",
      "Training date and time : \n",
      "2020-04-25 21:07:58\n",
      "Train on 1350 samples, validate on 150 samples\n",
      "Epoch 1/70\n",
      "1350/1350 [==============================] - 0s 118us/sample - loss: 0.0662 - acc: 0.5704 - val_loss: 0.0637 - val_acc: 0.5733\n",
      "Epoch 2/70\n",
      "1350/1350 [==============================] - 0s 30us/sample - loss: 0.0637 - acc: 0.5807 - val_loss: 0.0611 - val_acc: 0.5800\n",
      "Epoch 3/70\n",
      "1350/1350 [==============================] - 0s 32us/sample - loss: 0.0612 - acc: 0.5874 - val_loss: 0.0587 - val_acc: 0.5800\n",
      "Epoch 4/70\n",
      "1350/1350 [==============================] - 0s 34us/sample - loss: 0.0588 - acc: 0.5985 - val_loss: 0.0563 - val_acc: 0.5800\n",
      "Epoch 5/70\n",
      "1350/1350 [==============================] - 0s 30us/sample - loss: 0.0564 - acc: 0.6044 - val_loss: 0.0540 - val_acc: 0.5867\n",
      "Epoch 6/70\n",
      "1350/1350 [==============================] - 0s 31us/sample - loss: 0.0542 - acc: 0.6200 - val_loss: 0.0519 - val_acc: 0.6067\n",
      "Epoch 7/70\n",
      "1350/1350 [==============================] - 0s 31us/sample - loss: 0.0520 - acc: 0.6356 - val_loss: 0.0498 - val_acc: 0.6133\n",
      "Epoch 8/70\n",
      "1350/1350 [==============================] - 0s 32us/sample - loss: 0.0499 - acc: 0.6593 - val_loss: 0.0476 - val_acc: 0.6333\n",
      "Epoch 9/70\n",
      "1350/1350 [==============================] - 0s 28us/sample - loss: 0.0479 - acc: 0.6896 - val_loss: 0.0457 - val_acc: 0.6533\n",
      "Epoch 10/70\n",
      "1350/1350 [==============================] - 0s 32us/sample - loss: 0.0460 - acc: 0.7207 - val_loss: 0.0437 - val_acc: 0.6867\n",
      "Epoch 11/70\n",
      "1350/1350 [==============================] - 0s 30us/sample - loss: 0.0441 - acc: 0.7511 - val_loss: 0.0417 - val_acc: 0.7133\n",
      "Epoch 12/70\n",
      "1350/1350 [==============================] - 0s 30us/sample - loss: 0.0423 - acc: 0.7711 - val_loss: 0.0398 - val_acc: 0.7667\n",
      "Epoch 13/70\n",
      "1350/1350 [==============================] - 0s 30us/sample - loss: 0.0406 - acc: 0.7941 - val_loss: 0.0380 - val_acc: 0.8000\n",
      "Epoch 14/70\n",
      "1350/1350 [==============================] - 0s 30us/sample - loss: 0.0389 - acc: 0.8104 - val_loss: 0.0361 - val_acc: 0.8400\n",
      "Epoch 15/70\n",
      "1350/1350 [==============================] - 0s 30us/sample - loss: 0.0373 - acc: 0.8230 - val_loss: 0.0344 - val_acc: 0.8467\n",
      "Epoch 16/70\n",
      "1350/1350 [==============================] - 0s 29us/sample - loss: 0.0357 - acc: 0.8385 - val_loss: 0.0331 - val_acc: 0.8533\n",
      "Epoch 17/70\n",
      "1350/1350 [==============================] - 0s 30us/sample - loss: 0.0343 - acc: 0.8348 - val_loss: 0.0314 - val_acc: 0.8667\n",
      "Epoch 18/70\n",
      "1350/1350 [==============================] - 0s 31us/sample - loss: 0.0329 - acc: 0.8430 - val_loss: 0.0301 - val_acc: 0.8733\n",
      "Epoch 19/70\n",
      "1350/1350 [==============================] - 0s 31us/sample - loss: 0.0316 - acc: 0.8533 - val_loss: 0.0290 - val_acc: 0.8800\n",
      "Epoch 20/70\n",
      "1350/1350 [==============================] - 0s 30us/sample - loss: 0.0304 - acc: 0.8548 - val_loss: 0.0277 - val_acc: 0.8733\n",
      "Epoch 21/70\n",
      "1350/1350 [==============================] - 0s 28us/sample - loss: 0.0293 - acc: 0.8622 - val_loss: 0.0267 - val_acc: 0.8867\n",
      "Epoch 22/70\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1350/1350 [==============================] - 0s 30us/sample - loss: 0.0283 - acc: 0.8659 - val_loss: 0.0259 - val_acc: 0.8800\n",
      "Epoch 23/70\n",
      "1350/1350 [==============================] - 0s 29us/sample - loss: 0.0274 - acc: 0.8696 - val_loss: 0.0250 - val_acc: 0.8800\n",
      "Epoch 24/70\n",
      "1350/1350 [==============================] - 0s 30us/sample - loss: 0.0265 - acc: 0.8719 - val_loss: 0.0242 - val_acc: 0.8933\n",
      "Epoch 25/70\n",
      "1350/1350 [==============================] - 0s 32us/sample - loss: 0.0257 - acc: 0.8770 - val_loss: 0.0235 - val_acc: 0.8867\n",
      "Epoch 26/70\n",
      "1350/1350 [==============================] - 0s 31us/sample - loss: 0.0250 - acc: 0.8763 - val_loss: 0.0228 - val_acc: 0.9000\n",
      "Epoch 27/70\n",
      "1350/1350 [==============================] - 0s 29us/sample - loss: 0.0242 - acc: 0.8800 - val_loss: 0.0223 - val_acc: 0.8933\n",
      "Epoch 28/70\n",
      "1350/1350 [==============================] - 0s 31us/sample - loss: 0.0236 - acc: 0.8807 - val_loss: 0.0217 - val_acc: 0.9000\n",
      "Epoch 29/70\n",
      "1350/1350 [==============================] - 0s 30us/sample - loss: 0.0230 - acc: 0.8837 - val_loss: 0.0211 - val_acc: 0.9000\n",
      "Epoch 30/70\n",
      "1350/1350 [==============================] - 0s 32us/sample - loss: 0.0224 - acc: 0.8837 - val_loss: 0.0208 - val_acc: 0.9000\n",
      "Epoch 31/70\n",
      "1350/1350 [==============================] - 0s 30us/sample - loss: 0.0219 - acc: 0.8852 - val_loss: 0.0202 - val_acc: 0.8933\n",
      "Epoch 32/70\n",
      "1350/1350 [==============================] - 0s 30us/sample - loss: 0.0214 - acc: 0.8859 - val_loss: 0.0198 - val_acc: 0.9067\n",
      "Epoch 33/70\n",
      "1350/1350 [==============================] - 0s 29us/sample - loss: 0.0209 - acc: 0.8919 - val_loss: 0.0194 - val_acc: 0.9067\n",
      "Epoch 34/70\n",
      "1350/1350 [==============================] - 0s 29us/sample - loss: 0.0204 - acc: 0.8896 - val_loss: 0.0190 - val_acc: 0.9133\n",
      "Epoch 35/70\n",
      "1350/1350 [==============================] - 0s 29us/sample - loss: 0.0200 - acc: 0.8941 - val_loss: 0.0185 - val_acc: 0.9133\n",
      "Epoch 36/70\n",
      "1350/1350 [==============================] - 0s 30us/sample - loss: 0.0196 - acc: 0.8956 - val_loss: 0.0182 - val_acc: 0.9133\n",
      "Epoch 37/70\n",
      "1350/1350 [==============================] - 0s 30us/sample - loss: 0.0192 - acc: 0.9044 - val_loss: 0.0179 - val_acc: 0.9133\n",
      "Epoch 38/70\n",
      "1350/1350 [==============================] - 0s 32us/sample - loss: 0.0189 - acc: 0.8993 - val_loss: 0.0175 - val_acc: 0.9200\n",
      "Epoch 39/70\n",
      "1350/1350 [==============================] - 0s 29us/sample - loss: 0.0185 - acc: 0.9096 - val_loss: 0.0173 - val_acc: 0.9200\n",
      "Epoch 40/70\n",
      "1350/1350 [==============================] - 0s 32us/sample - loss: 0.0182 - acc: 0.9074 - val_loss: 0.0171 - val_acc: 0.9200\n",
      "Epoch 41/70\n",
      "1350/1350 [==============================] - 0s 33us/sample - loss: 0.0179 - acc: 0.9089 - val_loss: 0.0167 - val_acc: 0.9133\n",
      "Epoch 42/70\n",
      "1350/1350 [==============================] - 0s 31us/sample - loss: 0.0176 - acc: 0.9111 - val_loss: 0.0165 - val_acc: 0.9200\n",
      "Epoch 43/70\n",
      "1350/1350 [==============================] - 0s 30us/sample - loss: 0.0173 - acc: 0.9111 - val_loss: 0.0162 - val_acc: 0.9200\n",
      "Epoch 44/70\n",
      "1350/1350 [==============================] - 0s 31us/sample - loss: 0.0170 - acc: 0.9141 - val_loss: 0.0159 - val_acc: 0.9200\n",
      "Epoch 45/70\n",
      "1350/1350 [==============================] - 0s 30us/sample - loss: 0.0167 - acc: 0.9170 - val_loss: 0.0157 - val_acc: 0.9200\n",
      "Epoch 46/70\n",
      "1350/1350 [==============================] - 0s 32us/sample - loss: 0.0165 - acc: 0.9170 - val_loss: 0.0155 - val_acc: 0.9200\n",
      "Epoch 47/70\n",
      "1350/1350 [==============================] - 0s 34us/sample - loss: 0.0162 - acc: 0.9193 - val_loss: 0.0153 - val_acc: 0.9200\n",
      "Epoch 48/70\n",
      "1350/1350 [==============================] - 0s 31us/sample - loss: 0.0160 - acc: 0.9193 - val_loss: 0.0152 - val_acc: 0.9200\n",
      "Epoch 49/70\n",
      "1350/1350 [==============================] - 0s 31us/sample - loss: 0.0157 - acc: 0.9215 - val_loss: 0.0151 - val_acc: 0.9267\n",
      "Epoch 50/70\n",
      "1350/1350 [==============================] - 0s 32us/sample - loss: 0.0155 - acc: 0.9252 - val_loss: 0.0149 - val_acc: 0.9200\n",
      "Epoch 51/70\n",
      "1350/1350 [==============================] - 0s 31us/sample - loss: 0.0153 - acc: 0.9274 - val_loss: 0.0145 - val_acc: 0.9200\n",
      "Epoch 52/70\n",
      "1350/1350 [==============================] - 0s 29us/sample - loss: 0.0151 - acc: 0.9244 - val_loss: 0.0144 - val_acc: 0.9200\n",
      "Epoch 53/70\n",
      "1350/1350 [==============================] - 0s 31us/sample - loss: 0.0149 - acc: 0.9267 - val_loss: 0.0143 - val_acc: 0.9200\n",
      "Epoch 54/70\n",
      "1350/1350 [==============================] - 0s 32us/sample - loss: 0.0147 - acc: 0.9296 - val_loss: 0.0141 - val_acc: 0.9267\n",
      "Epoch 55/70\n",
      "1350/1350 [==============================] - 0s 32us/sample - loss: 0.0145 - acc: 0.9304 - val_loss: 0.0140 - val_acc: 0.9200\n",
      "Epoch 56/70\n",
      "1350/1350 [==============================] - 0s 31us/sample - loss: 0.0143 - acc: 0.9296 - val_loss: 0.0138 - val_acc: 0.9267\n",
      "Epoch 57/70\n",
      "1350/1350 [==============================] - 0s 32us/sample - loss: 0.0141 - acc: 0.9326 - val_loss: 0.0137 - val_acc: 0.9267\n",
      "Epoch 58/70\n",
      "1350/1350 [==============================] - 0s 32us/sample - loss: 0.0139 - acc: 0.9296 - val_loss: 0.0136 - val_acc: 0.9267\n",
      "Epoch 59/70\n",
      "1350/1350 [==============================] - 0s 30us/sample - loss: 0.0137 - acc: 0.9319 - val_loss: 0.0135 - val_acc: 0.9267\n",
      "Epoch 60/70\n",
      "1350/1350 [==============================] - 0s 29us/sample - loss: 0.0136 - acc: 0.9326 - val_loss: 0.0134 - val_acc: 0.9267\n",
      "Epoch 61/70\n",
      "1350/1350 [==============================] - 0s 29us/sample - loss: 0.0134 - acc: 0.9348 - val_loss: 0.0132 - val_acc: 0.9267\n",
      "Epoch 62/70\n",
      "1350/1350 [==============================] - 0s 32us/sample - loss: 0.0133 - acc: 0.9333 - val_loss: 0.0131 - val_acc: 0.9200\n",
      "Epoch 63/70\n",
      "1350/1350 [==============================] - 0s 29us/sample - loss: 0.0131 - acc: 0.9348 - val_loss: 0.0131 - val_acc: 0.9200\n",
      "Epoch 64/70\n",
      "1350/1350 [==============================] - 0s 30us/sample - loss: 0.0129 - acc: 0.9356 - val_loss: 0.0128 - val_acc: 0.9267\n",
      "Epoch 65/70\n",
      "1350/1350 [==============================] - 0s 30us/sample - loss: 0.0128 - acc: 0.9363 - val_loss: 0.0128 - val_acc: 0.9267\n",
      "Epoch 66/70\n",
      "1350/1350 [==============================] - 0s 31us/sample - loss: 0.0127 - acc: 0.9348 - val_loss: 0.0127 - val_acc: 0.9200\n",
      "Epoch 67/70\n",
      "1350/1350 [==============================] - 0s 31us/sample - loss: 0.0125 - acc: 0.9356 - val_loss: 0.0126 - val_acc: 0.9267\n",
      "Epoch 68/70\n",
      "1350/1350 [==============================] - 0s 30us/sample - loss: 0.0124 - acc: 0.9378 - val_loss: 0.0125 - val_acc: 0.9267\n",
      "Epoch 69/70\n",
      "1350/1350 [==============================] - 0s 31us/sample - loss: 0.0123 - acc: 0.9356 - val_loss: 0.0124 - val_acc: 0.9267\n",
      "Epoch 70/70\n",
      "1350/1350 [==============================] - 0s 30us/sample - loss: 0.0121 - acc: 0.9385 - val_loss: 0.0122 - val_acc: 0.9267\n",
      "Elasped Time: 0:00:03.352115\n",
      "Training date and time : \n",
      "2020-04-25 21:08:01\n",
      "Train on 1350 samples, validate on 150 samples\n",
      "Epoch 1/80\n",
      "1350/1350 [==============================] - 0s 119us/sample - loss: 0.0670 - acc: 0.5400 - val_loss: 0.0697 - val_acc: 0.4467\n",
      "Epoch 2/80\n",
      "1350/1350 [==============================] - 0s 28us/sample - loss: 0.0647 - acc: 0.5563 - val_loss: 0.0680 - val_acc: 0.4467\n",
      "Epoch 3/80\n",
      "1350/1350 [==============================] - 0s 31us/sample - loss: 0.0623 - acc: 0.5689 - val_loss: 0.0662 - val_acc: 0.4667\n",
      "Epoch 4/80\n",
      "1350/1350 [==============================] - 0s 32us/sample - loss: 0.0601 - acc: 0.5867 - val_loss: 0.0644 - val_acc: 0.4933\n",
      "Epoch 5/80\n",
      "1350/1350 [==============================] - 0s 32us/sample - loss: 0.0578 - acc: 0.6030 - val_loss: 0.0625 - val_acc: 0.5133\n",
      "Epoch 6/80\n",
      "1350/1350 [==============================] - 0s 31us/sample - loss: 0.0556 - acc: 0.6200 - val_loss: 0.0606 - val_acc: 0.5533\n",
      "Epoch 7/80\n",
      "1350/1350 [==============================] - 0s 32us/sample - loss: 0.0535 - acc: 0.6481 - val_loss: 0.0586 - val_acc: 0.5733\n",
      "Epoch 8/80\n",
      "1350/1350 [==============================] - 0s 32us/sample - loss: 0.0514 - acc: 0.6741 - val_loss: 0.0566 - val_acc: 0.5867\n",
      "Epoch 9/80\n",
      "1350/1350 [==============================] - 0s 32us/sample - loss: 0.0493 - acc: 0.7193 - val_loss: 0.0547 - val_acc: 0.6333\n",
      "Epoch 10/80\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1350/1350 [==============================] - 0s 31us/sample - loss: 0.0473 - acc: 0.7481 - val_loss: 0.0528 - val_acc: 0.6733\n",
      "Epoch 11/80\n",
      "1350/1350 [==============================] - 0s 30us/sample - loss: 0.0453 - acc: 0.7704 - val_loss: 0.0508 - val_acc: 0.7000\n",
      "Epoch 12/80\n",
      "1350/1350 [==============================] - 0s 31us/sample - loss: 0.0433 - acc: 0.7941 - val_loss: 0.0489 - val_acc: 0.7200\n",
      "Epoch 13/80\n",
      "1350/1350 [==============================] - 0s 31us/sample - loss: 0.0415 - acc: 0.8104 - val_loss: 0.0472 - val_acc: 0.7333\n",
      "Epoch 14/80\n",
      "1350/1350 [==============================] - 0s 32us/sample - loss: 0.0396 - acc: 0.8215 - val_loss: 0.0454 - val_acc: 0.7467\n",
      "Epoch 15/80\n",
      "1350/1350 [==============================] - 0s 30us/sample - loss: 0.0379 - acc: 0.8356 - val_loss: 0.0437 - val_acc: 0.7533\n",
      "Epoch 16/80\n",
      "1350/1350 [==============================] - 0s 32us/sample - loss: 0.0363 - acc: 0.8378 - val_loss: 0.0422 - val_acc: 0.7667\n",
      "Epoch 17/80\n",
      "1350/1350 [==============================] - 0s 30us/sample - loss: 0.0348 - acc: 0.8415 - val_loss: 0.0405 - val_acc: 0.7933\n",
      "Epoch 18/80\n",
      "1350/1350 [==============================] - 0s 32us/sample - loss: 0.0334 - acc: 0.8481 - val_loss: 0.0391 - val_acc: 0.8000\n",
      "Epoch 19/80\n",
      "1350/1350 [==============================] - 0s 31us/sample - loss: 0.0321 - acc: 0.8548 - val_loss: 0.0381 - val_acc: 0.8067\n",
      "Epoch 20/80\n",
      "1350/1350 [==============================] - 0s 32us/sample - loss: 0.0309 - acc: 0.8563 - val_loss: 0.0367 - val_acc: 0.8000\n",
      "Epoch 21/80\n",
      "1350/1350 [==============================] - 0s 30us/sample - loss: 0.0298 - acc: 0.8593 - val_loss: 0.0355 - val_acc: 0.8133\n",
      "Epoch 22/80\n",
      "1350/1350 [==============================] - 0s 31us/sample - loss: 0.0288 - acc: 0.8652 - val_loss: 0.0345 - val_acc: 0.8133\n",
      "Epoch 23/80\n",
      "1350/1350 [==============================] - 0s 34us/sample - loss: 0.0278 - acc: 0.8726 - val_loss: 0.0337 - val_acc: 0.8267\n",
      "Epoch 24/80\n",
      "1350/1350 [==============================] - 0s 31us/sample - loss: 0.0269 - acc: 0.8741 - val_loss: 0.0327 - val_acc: 0.8200\n",
      "Epoch 25/80\n",
      "1350/1350 [==============================] - 0s 31us/sample - loss: 0.0261 - acc: 0.8741 - val_loss: 0.0318 - val_acc: 0.8400\n",
      "Epoch 26/80\n",
      "1350/1350 [==============================] - 0s 31us/sample - loss: 0.0253 - acc: 0.8807 - val_loss: 0.0310 - val_acc: 0.8267\n",
      "Epoch 27/80\n",
      "1350/1350 [==============================] - 0s 31us/sample - loss: 0.0246 - acc: 0.8793 - val_loss: 0.0303 - val_acc: 0.8333\n",
      "Epoch 28/80\n",
      "1350/1350 [==============================] - 0s 30us/sample - loss: 0.0240 - acc: 0.8800 - val_loss: 0.0295 - val_acc: 0.8333\n",
      "Epoch 29/80\n",
      "1350/1350 [==============================] - 0s 30us/sample - loss: 0.0233 - acc: 0.8830 - val_loss: 0.0290 - val_acc: 0.8533\n",
      "Epoch 30/80\n",
      "1350/1350 [==============================] - 0s 32us/sample - loss: 0.0227 - acc: 0.8852 - val_loss: 0.0284 - val_acc: 0.8600\n",
      "Epoch 31/80\n",
      "1350/1350 [==============================] - 0s 33us/sample - loss: 0.0221 - acc: 0.8874 - val_loss: 0.0279 - val_acc: 0.8533\n",
      "Epoch 32/80\n",
      "1350/1350 [==============================] - 0s 32us/sample - loss: 0.0217 - acc: 0.8911 - val_loss: 0.0273 - val_acc: 0.8533\n",
      "Epoch 33/80\n",
      "1350/1350 [==============================] - 0s 30us/sample - loss: 0.0212 - acc: 0.8896 - val_loss: 0.0269 - val_acc: 0.8600\n",
      "Epoch 34/80\n",
      "1350/1350 [==============================] - 0s 30us/sample - loss: 0.0207 - acc: 0.8948 - val_loss: 0.0263 - val_acc: 0.8600\n",
      "Epoch 35/80\n",
      "1350/1350 [==============================] - 0s 30us/sample - loss: 0.0202 - acc: 0.8963 - val_loss: 0.0261 - val_acc: 0.8667\n",
      "Epoch 36/80\n",
      "1350/1350 [==============================] - 0s 32us/sample - loss: 0.0198 - acc: 0.8970 - val_loss: 0.0255 - val_acc: 0.8667\n",
      "Epoch 37/80\n",
      "1350/1350 [==============================] - 0s 31us/sample - loss: 0.0194 - acc: 0.9000 - val_loss: 0.0251 - val_acc: 0.8600\n",
      "Epoch 38/80\n",
      "1350/1350 [==============================] - 0s 33us/sample - loss: 0.0190 - acc: 0.9074 - val_loss: 0.0247 - val_acc: 0.8667\n",
      "Epoch 39/80\n",
      "1350/1350 [==============================] - 0s 31us/sample - loss: 0.0186 - acc: 0.9044 - val_loss: 0.0244 - val_acc: 0.8667\n",
      "Epoch 40/80\n",
      "1350/1350 [==============================] - 0s 30us/sample - loss: 0.0183 - acc: 0.9074 - val_loss: 0.0241 - val_acc: 0.8733\n",
      "Epoch 41/80\n",
      "1350/1350 [==============================] - 0s 30us/sample - loss: 0.0180 - acc: 0.9074 - val_loss: 0.0237 - val_acc: 0.8667\n",
      "Epoch 42/80\n",
      "1350/1350 [==============================] - 0s 31us/sample - loss: 0.0176 - acc: 0.9096 - val_loss: 0.0235 - val_acc: 0.8733\n",
      "Epoch 43/80\n",
      "1350/1350 [==============================] - 0s 31us/sample - loss: 0.0172 - acc: 0.9111 - val_loss: 0.0231 - val_acc: 0.8733\n",
      "Epoch 44/80\n",
      "1350/1350 [==============================] - 0s 31us/sample - loss: 0.0170 - acc: 0.9141 - val_loss: 0.0227 - val_acc: 0.8733\n",
      "Epoch 45/80\n",
      "1350/1350 [==============================] - 0s 30us/sample - loss: 0.0167 - acc: 0.9133 - val_loss: 0.0225 - val_acc: 0.8733\n",
      "Epoch 46/80\n",
      "1350/1350 [==============================] - 0s 30us/sample - loss: 0.0163 - acc: 0.9193 - val_loss: 0.0225 - val_acc: 0.8733\n",
      "Epoch 47/80\n",
      "1350/1350 [==============================] - 0s 30us/sample - loss: 0.0160 - acc: 0.9215 - val_loss: 0.0220 - val_acc: 0.8733\n",
      "Epoch 48/80\n",
      "1350/1350 [==============================] - 0s 32us/sample - loss: 0.0158 - acc: 0.9222 - val_loss: 0.0218 - val_acc: 0.8800\n",
      "Epoch 49/80\n",
      "1350/1350 [==============================] - 0s 32us/sample - loss: 0.0156 - acc: 0.9230 - val_loss: 0.0216 - val_acc: 0.8733\n",
      "Epoch 50/80\n",
      "1350/1350 [==============================] - 0s 31us/sample - loss: 0.0153 - acc: 0.9252 - val_loss: 0.0214 - val_acc: 0.8733\n",
      "Epoch 51/80\n",
      "1350/1350 [==============================] - 0s 31us/sample - loss: 0.0151 - acc: 0.9259 - val_loss: 0.0211 - val_acc: 0.8867\n",
      "Epoch 52/80\n",
      "1350/1350 [==============================] - 0s 29us/sample - loss: 0.0148 - acc: 0.9296 - val_loss: 0.0208 - val_acc: 0.8867\n",
      "Epoch 53/80\n",
      "1350/1350 [==============================] - 0s 31us/sample - loss: 0.0146 - acc: 0.9289 - val_loss: 0.0208 - val_acc: 0.8867\n",
      "Epoch 54/80\n",
      "1350/1350 [==============================] - 0s 31us/sample - loss: 0.0144 - acc: 0.9319 - val_loss: 0.0205 - val_acc: 0.8800\n",
      "Epoch 55/80\n",
      "1350/1350 [==============================] - 0s 33us/sample - loss: 0.0142 - acc: 0.9311 - val_loss: 0.0205 - val_acc: 0.8800\n",
      "Epoch 56/80\n",
      "1350/1350 [==============================] - 0s 31us/sample - loss: 0.0140 - acc: 0.9304 - val_loss: 0.0202 - val_acc: 0.8933\n",
      "Epoch 57/80\n",
      "1350/1350 [==============================] - 0s 30us/sample - loss: 0.0138 - acc: 0.9341 - val_loss: 0.0201 - val_acc: 0.8933\n",
      "Epoch 58/80\n",
      "1350/1350 [==============================] - 0s 32us/sample - loss: 0.0135 - acc: 0.9363 - val_loss: 0.0200 - val_acc: 0.8867\n",
      "Epoch 59/80\n",
      "1350/1350 [==============================] - 0s 30us/sample - loss: 0.0134 - acc: 0.9356 - val_loss: 0.0198 - val_acc: 0.8933\n",
      "Epoch 60/80\n",
      "1350/1350 [==============================] - 0s 29us/sample - loss: 0.0132 - acc: 0.9385 - val_loss: 0.0195 - val_acc: 0.8933\n",
      "Epoch 61/80\n",
      "1350/1350 [==============================] - 0s 29us/sample - loss: 0.0130 - acc: 0.9370 - val_loss: 0.0194 - val_acc: 0.8933\n",
      "Epoch 62/80\n",
      "1350/1350 [==============================] - 0s 31us/sample - loss: 0.0128 - acc: 0.9385 - val_loss: 0.0194 - val_acc: 0.8933\n",
      "Epoch 63/80\n",
      "1350/1350 [==============================] - 0s 31us/sample - loss: 0.0127 - acc: 0.9400 - val_loss: 0.0193 - val_acc: 0.8933\n",
      "Epoch 64/80\n",
      "1350/1350 [==============================] - 0s 30us/sample - loss: 0.0125 - acc: 0.9407 - val_loss: 0.0192 - val_acc: 0.8933\n",
      "Epoch 65/80\n",
      "1350/1350 [==============================] - 0s 31us/sample - loss: 0.0123 - acc: 0.9422 - val_loss: 0.0190 - val_acc: 0.9000\n",
      "Epoch 66/80\n",
      "1350/1350 [==============================] - 0s 32us/sample - loss: 0.0122 - acc: 0.9422 - val_loss: 0.0189 - val_acc: 0.8933\n",
      "Epoch 67/80\n",
      "1350/1350 [==============================] - 0s 30us/sample - loss: 0.0120 - acc: 0.9444 - val_loss: 0.0188 - val_acc: 0.8933\n",
      "Epoch 68/80\n",
      "1350/1350 [==============================] - 0s 30us/sample - loss: 0.0119 - acc: 0.9459 - val_loss: 0.0185 - val_acc: 0.9000\n",
      "Epoch 69/80\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1350/1350 [==============================] - 0s 31us/sample - loss: 0.0117 - acc: 0.9459 - val_loss: 0.0186 - val_acc: 0.8933\n",
      "Epoch 70/80\n",
      "1350/1350 [==============================] - 0s 32us/sample - loss: 0.0116 - acc: 0.9481 - val_loss: 0.0183 - val_acc: 0.9000\n",
      "Epoch 71/80\n",
      "1350/1350 [==============================] - 0s 34us/sample - loss: 0.0114 - acc: 0.9467 - val_loss: 0.0182 - val_acc: 0.8933\n",
      "Epoch 72/80\n",
      "1350/1350 [==============================] - 0s 30us/sample - loss: 0.0113 - acc: 0.9504 - val_loss: 0.0183 - val_acc: 0.8933\n",
      "Epoch 73/80\n",
      "1350/1350 [==============================] - 0s 32us/sample - loss: 0.0112 - acc: 0.9467 - val_loss: 0.0181 - val_acc: 0.9000\n",
      "Epoch 74/80\n",
      "1350/1350 [==============================] - 0s 32us/sample - loss: 0.0110 - acc: 0.9481 - val_loss: 0.0181 - val_acc: 0.8933\n",
      "Epoch 75/80\n",
      "1350/1350 [==============================] - 0s 30us/sample - loss: 0.0109 - acc: 0.9511 - val_loss: 0.0179 - val_acc: 0.8933\n",
      "Epoch 76/80\n",
      "1350/1350 [==============================] - 0s 31us/sample - loss: 0.0108 - acc: 0.9496 - val_loss: 0.0178 - val_acc: 0.8933\n",
      "Epoch 77/80\n",
      "1350/1350 [==============================] - 0s 33us/sample - loss: 0.0107 - acc: 0.9519 - val_loss: 0.0179 - val_acc: 0.8867\n",
      "Epoch 78/80\n",
      "1350/1350 [==============================] - 0s 30us/sample - loss: 0.0105 - acc: 0.9526 - val_loss: 0.0177 - val_acc: 0.9000\n",
      "Epoch 79/80\n",
      "1350/1350 [==============================] - 0s 33us/sample - loss: 0.0104 - acc: 0.9548 - val_loss: 0.0176 - val_acc: 0.9000\n",
      "Epoch 80/80\n",
      "1350/1350 [==============================] - 0s 31us/sample - loss: 0.0103 - acc: 0.9548 - val_loss: 0.0175 - val_acc: 0.9000\n",
      "Elasped Time: 0:00:03.773297\n",
      "Training date and time : \n",
      "2020-04-25 21:08:05\n",
      "Train on 1350 samples, validate on 150 samples\n",
      "Epoch 1/90\n",
      "1350/1350 [==============================] - 0s 123us/sample - loss: 0.0690 - acc: 0.5400 - val_loss: 0.0651 - val_acc: 0.5267\n",
      "Epoch 2/90\n",
      "1350/1350 [==============================] - 0s 31us/sample - loss: 0.0668 - acc: 0.5474 - val_loss: 0.0631 - val_acc: 0.5400\n",
      "Epoch 3/90\n",
      "1350/1350 [==============================] - 0s 31us/sample - loss: 0.0646 - acc: 0.5630 - val_loss: 0.0611 - val_acc: 0.5600\n",
      "Epoch 4/90\n",
      "1350/1350 [==============================] - 0s 30us/sample - loss: 0.0625 - acc: 0.5748 - val_loss: 0.0589 - val_acc: 0.5867\n",
      "Epoch 5/90\n",
      "1350/1350 [==============================] - 0s 32us/sample - loss: 0.0603 - acc: 0.5963 - val_loss: 0.0568 - val_acc: 0.6067\n",
      "Epoch 6/90\n",
      "1350/1350 [==============================] - 0s 31us/sample - loss: 0.0582 - acc: 0.6170 - val_loss: 0.0548 - val_acc: 0.6333\n",
      "Epoch 7/90\n",
      "1350/1350 [==============================] - 0s 33us/sample - loss: 0.0562 - acc: 0.6326 - val_loss: 0.0526 - val_acc: 0.6600\n",
      "Epoch 8/90\n",
      "1350/1350 [==============================] - 0s 31us/sample - loss: 0.0542 - acc: 0.6556 - val_loss: 0.0505 - val_acc: 0.6933\n",
      "Epoch 9/90\n",
      "1350/1350 [==============================] - 0s 30us/sample - loss: 0.0522 - acc: 0.6793 - val_loss: 0.0485 - val_acc: 0.7067\n",
      "Epoch 10/90\n",
      "1350/1350 [==============================] - 0s 31us/sample - loss: 0.0503 - acc: 0.7044 - val_loss: 0.0465 - val_acc: 0.7533\n",
      "Epoch 11/90\n",
      "1350/1350 [==============================] - 0s 33us/sample - loss: 0.0484 - acc: 0.7281 - val_loss: 0.0448 - val_acc: 0.7800\n",
      "Epoch 12/90\n",
      "1350/1350 [==============================] - 0s 29us/sample - loss: 0.0465 - acc: 0.7496 - val_loss: 0.0429 - val_acc: 0.7933\n",
      "Epoch 13/90\n",
      "1350/1350 [==============================] - 0s 32us/sample - loss: 0.0447 - acc: 0.7689 - val_loss: 0.0411 - val_acc: 0.8067\n",
      "Epoch 14/90\n",
      "1350/1350 [==============================] - 0s 32us/sample - loss: 0.0430 - acc: 0.7822 - val_loss: 0.0394 - val_acc: 0.8333\n",
      "Epoch 15/90\n",
      "1350/1350 [==============================] - 0s 30us/sample - loss: 0.0413 - acc: 0.7941 - val_loss: 0.0376 - val_acc: 0.8333\n",
      "Epoch 16/90\n",
      "1350/1350 [==============================] - 0s 29us/sample - loss: 0.0397 - acc: 0.8044 - val_loss: 0.0361 - val_acc: 0.8467\n",
      "Epoch 17/90\n",
      "1350/1350 [==============================] - 0s 28us/sample - loss: 0.0381 - acc: 0.8089 - val_loss: 0.0344 - val_acc: 0.8867\n",
      "Epoch 18/90\n",
      "1350/1350 [==============================] - 0s 32us/sample - loss: 0.0367 - acc: 0.8244 - val_loss: 0.0332 - val_acc: 0.8933\n",
      "Epoch 19/90\n",
      "1350/1350 [==============================] - 0s 31us/sample - loss: 0.0353 - acc: 0.8252 - val_loss: 0.0321 - val_acc: 0.8933\n",
      "Epoch 20/90\n",
      "1350/1350 [==============================] - 0s 32us/sample - loss: 0.0341 - acc: 0.8378 - val_loss: 0.0309 - val_acc: 0.9000\n",
      "Epoch 21/90\n",
      "1350/1350 [==============================] - 0s 29us/sample - loss: 0.0330 - acc: 0.8467 - val_loss: 0.0297 - val_acc: 0.9000\n",
      "Epoch 22/90\n",
      "1350/1350 [==============================] - 0s 31us/sample - loss: 0.0319 - acc: 0.8496 - val_loss: 0.0287 - val_acc: 0.9067\n",
      "Epoch 23/90\n",
      "1350/1350 [==============================] - 0s 29us/sample - loss: 0.0309 - acc: 0.8504 - val_loss: 0.0279 - val_acc: 0.9000\n",
      "Epoch 24/90\n",
      "1350/1350 [==============================] - 0s 30us/sample - loss: 0.0300 - acc: 0.8548 - val_loss: 0.0271 - val_acc: 0.9000\n",
      "Epoch 25/90\n",
      "1350/1350 [==============================] - 0s 30us/sample - loss: 0.0291 - acc: 0.8570 - val_loss: 0.0262 - val_acc: 0.9000\n",
      "Epoch 26/90\n",
      "1350/1350 [==============================] - 0s 30us/sample - loss: 0.0283 - acc: 0.8585 - val_loss: 0.0259 - val_acc: 0.9200\n",
      "Epoch 27/90\n",
      "1350/1350 [==============================] - 0s 29us/sample - loss: 0.0276 - acc: 0.8622 - val_loss: 0.0249 - val_acc: 0.9267\n",
      "Epoch 28/90\n",
      "1350/1350 [==============================] - 0s 31us/sample - loss: 0.0269 - acc: 0.8644 - val_loss: 0.0244 - val_acc: 0.9267\n",
      "Epoch 29/90\n",
      "1350/1350 [==============================] - 0s 31us/sample - loss: 0.0263 - acc: 0.8667 - val_loss: 0.0238 - val_acc: 0.9200\n",
      "Epoch 30/90\n",
      "1350/1350 [==============================] - 0s 30us/sample - loss: 0.0257 - acc: 0.8681 - val_loss: 0.0235 - val_acc: 0.9200\n",
      "Epoch 31/90\n",
      "1350/1350 [==============================] - 0s 30us/sample - loss: 0.0251 - acc: 0.8689 - val_loss: 0.0233 - val_acc: 0.9133\n",
      "Epoch 32/90\n",
      "1350/1350 [==============================] - 0s 31us/sample - loss: 0.0245 - acc: 0.8711 - val_loss: 0.0224 - val_acc: 0.9133\n",
      "Epoch 33/90\n",
      "1350/1350 [==============================] - 0s 31us/sample - loss: 0.0240 - acc: 0.8733 - val_loss: 0.0222 - val_acc: 0.9133\n",
      "Epoch 34/90\n",
      "1350/1350 [==============================] - 0s 31us/sample - loss: 0.0235 - acc: 0.8748 - val_loss: 0.0217 - val_acc: 0.9133\n",
      "Epoch 35/90\n",
      "1350/1350 [==============================] - 0s 31us/sample - loss: 0.0231 - acc: 0.8770 - val_loss: 0.0215 - val_acc: 0.9133\n",
      "Epoch 36/90\n",
      "1350/1350 [==============================] - 0s 30us/sample - loss: 0.0226 - acc: 0.8800 - val_loss: 0.0214 - val_acc: 0.9067\n",
      "Epoch 37/90\n",
      "1350/1350 [==============================] - 0s 31us/sample - loss: 0.0222 - acc: 0.8807 - val_loss: 0.0209 - val_acc: 0.9133\n",
      "Epoch 38/90\n",
      "1350/1350 [==============================] - 0s 29us/sample - loss: 0.0218 - acc: 0.8815 - val_loss: 0.0206 - val_acc: 0.9133\n",
      "Epoch 39/90\n",
      "1350/1350 [==============================] - 0s 30us/sample - loss: 0.0214 - acc: 0.8815 - val_loss: 0.0202 - val_acc: 0.9133\n",
      "Epoch 40/90\n",
      "1350/1350 [==============================] - 0s 34us/sample - loss: 0.0211 - acc: 0.8852 - val_loss: 0.0200 - val_acc: 0.9067\n",
      "Epoch 41/90\n",
      "1350/1350 [==============================] - 0s 30us/sample - loss: 0.0207 - acc: 0.8874 - val_loss: 0.0199 - val_acc: 0.9067\n",
      "Epoch 42/90\n",
      "1350/1350 [==============================] - 0s 30us/sample - loss: 0.0204 - acc: 0.8867 - val_loss: 0.0195 - val_acc: 0.9067\n",
      "Epoch 43/90\n",
      "1350/1350 [==============================] - 0s 30us/sample - loss: 0.0201 - acc: 0.8904 - val_loss: 0.0196 - val_acc: 0.9067\n",
      "Epoch 44/90\n",
      "1350/1350 [==============================] - 0s 29us/sample - loss: 0.0197 - acc: 0.8889 - val_loss: 0.0192 - val_acc: 0.9067\n",
      "Epoch 45/90\n",
      "1350/1350 [==============================] - 0s 29us/sample - loss: 0.0194 - acc: 0.8896 - val_loss: 0.0191 - val_acc: 0.9067\n",
      "Epoch 46/90\n",
      "1350/1350 [==============================] - 0s 31us/sample - loss: 0.0192 - acc: 0.8911 - val_loss: 0.0191 - val_acc: 0.9067\n",
      "Epoch 47/90\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1350/1350 [==============================] - 0s 29us/sample - loss: 0.0188 - acc: 0.8956 - val_loss: 0.0187 - val_acc: 0.9067\n",
      "Epoch 48/90\n",
      "1350/1350 [==============================] - 0s 31us/sample - loss: 0.0186 - acc: 0.8978 - val_loss: 0.0186 - val_acc: 0.9067\n",
      "Epoch 49/90\n",
      "1350/1350 [==============================] - 0s 29us/sample - loss: 0.0184 - acc: 0.9007 - val_loss: 0.0183 - val_acc: 0.9067\n",
      "Epoch 50/90\n",
      "1350/1350 [==============================] - 0s 31us/sample - loss: 0.0181 - acc: 0.9022 - val_loss: 0.0180 - val_acc: 0.9067\n",
      "Epoch 51/90\n",
      "1350/1350 [==============================] - 0s 30us/sample - loss: 0.0178 - acc: 0.9037 - val_loss: 0.0178 - val_acc: 0.9067\n",
      "Epoch 52/90\n",
      "1350/1350 [==============================] - 0s 30us/sample - loss: 0.0176 - acc: 0.9037 - val_loss: 0.0180 - val_acc: 0.9067\n",
      "Epoch 53/90\n",
      "1350/1350 [==============================] - 0s 29us/sample - loss: 0.0174 - acc: 0.9052 - val_loss: 0.0180 - val_acc: 0.9067\n",
      "Epoch 54/90\n",
      "1350/1350 [==============================] - 0s 29us/sample - loss: 0.0171 - acc: 0.9059 - val_loss: 0.0178 - val_acc: 0.9067\n",
      "Epoch 55/90\n",
      "1350/1350 [==============================] - 0s 29us/sample - loss: 0.0169 - acc: 0.9067 - val_loss: 0.0174 - val_acc: 0.9067\n",
      "Epoch 56/90\n",
      "1350/1350 [==============================] - 0s 30us/sample - loss: 0.0167 - acc: 0.9104 - val_loss: 0.0174 - val_acc: 0.9067\n",
      "Epoch 57/90\n",
      "1350/1350 [==============================] - 0s 30us/sample - loss: 0.0165 - acc: 0.9096 - val_loss: 0.0173 - val_acc: 0.9067\n",
      "Epoch 58/90\n",
      "1350/1350 [==============================] - 0s 30us/sample - loss: 0.0162 - acc: 0.9081 - val_loss: 0.0171 - val_acc: 0.9067\n",
      "Epoch 59/90\n",
      "1350/1350 [==============================] - 0s 29us/sample - loss: 0.0160 - acc: 0.9119 - val_loss: 0.0172 - val_acc: 0.9067\n",
      "Epoch 60/90\n",
      "1350/1350 [==============================] - 0s 31us/sample - loss: 0.0159 - acc: 0.9096 - val_loss: 0.0170 - val_acc: 0.9067\n",
      "Epoch 61/90\n",
      "1350/1350 [==============================] - 0s 31us/sample - loss: 0.0157 - acc: 0.9104 - val_loss: 0.0169 - val_acc: 0.9067\n",
      "Epoch 62/90\n",
      "1350/1350 [==============================] - 0s 30us/sample - loss: 0.0154 - acc: 0.9111 - val_loss: 0.0168 - val_acc: 0.9067\n",
      "Epoch 63/90\n",
      "1350/1350 [==============================] - 0s 30us/sample - loss: 0.0153 - acc: 0.9141 - val_loss: 0.0167 - val_acc: 0.9067\n",
      "Epoch 64/90\n",
      "1350/1350 [==============================] - 0s 31us/sample - loss: 0.0151 - acc: 0.9148 - val_loss: 0.0167 - val_acc: 0.9067\n",
      "Epoch 65/90\n",
      "1350/1350 [==============================] - 0s 31us/sample - loss: 0.0149 - acc: 0.9141 - val_loss: 0.0167 - val_acc: 0.9067\n",
      "Epoch 66/90\n",
      "1350/1350 [==============================] - 0s 31us/sample - loss: 0.0148 - acc: 0.9141 - val_loss: 0.0165 - val_acc: 0.9067\n",
      "Epoch 67/90\n",
      "1350/1350 [==============================] - 0s 32us/sample - loss: 0.0146 - acc: 0.9178 - val_loss: 0.0164 - val_acc: 0.9067\n",
      "Epoch 68/90\n",
      "1350/1350 [==============================] - 0s 31us/sample - loss: 0.0144 - acc: 0.9193 - val_loss: 0.0163 - val_acc: 0.9067\n",
      "Epoch 69/90\n",
      "1350/1350 [==============================] - 0s 30us/sample - loss: 0.0143 - acc: 0.9193 - val_loss: 0.0162 - val_acc: 0.9067\n",
      "Epoch 70/90\n",
      "1350/1350 [==============================] - 0s 30us/sample - loss: 0.0141 - acc: 0.9207 - val_loss: 0.0160 - val_acc: 0.9133\n",
      "Epoch 71/90\n",
      "1350/1350 [==============================] - 0s 32us/sample - loss: 0.0140 - acc: 0.9244 - val_loss: 0.0160 - val_acc: 0.9067\n",
      "Epoch 72/90\n",
      "1350/1350 [==============================] - 0s 29us/sample - loss: 0.0138 - acc: 0.9244 - val_loss: 0.0159 - val_acc: 0.9067\n",
      "Epoch 73/90\n",
      "1350/1350 [==============================] - 0s 31us/sample - loss: 0.0136 - acc: 0.9230 - val_loss: 0.0160 - val_acc: 0.9067\n",
      "Epoch 74/90\n",
      "1350/1350 [==============================] - 0s 30us/sample - loss: 0.0135 - acc: 0.9274 - val_loss: 0.0160 - val_acc: 0.9067\n",
      "Epoch 75/90\n",
      "1350/1350 [==============================] - 0s 30us/sample - loss: 0.0133 - acc: 0.9289 - val_loss: 0.0160 - val_acc: 0.9067\n",
      "Epoch 76/90\n",
      "1350/1350 [==============================] - 0s 31us/sample - loss: 0.0132 - acc: 0.9281 - val_loss: 0.0160 - val_acc: 0.9067\n",
      "Epoch 77/90\n",
      "1350/1350 [==============================] - 0s 30us/sample - loss: 0.0131 - acc: 0.9311 - val_loss: 0.0160 - val_acc: 0.9067\n",
      "Epoch 78/90\n",
      "1350/1350 [==============================] - 0s 31us/sample - loss: 0.0129 - acc: 0.9296 - val_loss: 0.0161 - val_acc: 0.9067\n",
      "Epoch 79/90\n",
      "1350/1350 [==============================] - 0s 30us/sample - loss: 0.0128 - acc: 0.9326 - val_loss: 0.0159 - val_acc: 0.9067\n",
      "Epoch 80/90\n",
      "1350/1350 [==============================] - 0s 30us/sample - loss: 0.0126 - acc: 0.9311 - val_loss: 0.0157 - val_acc: 0.9067\n",
      "Epoch 81/90\n",
      "1350/1350 [==============================] - 0s 29us/sample - loss: 0.0125 - acc: 0.9348 - val_loss: 0.0157 - val_acc: 0.9067\n",
      "Epoch 82/90\n",
      "1350/1350 [==============================] - 0s 30us/sample - loss: 0.0124 - acc: 0.9333 - val_loss: 0.0154 - val_acc: 0.9133\n",
      "Epoch 83/90\n",
      "1350/1350 [==============================] - 0s 30us/sample - loss: 0.0122 - acc: 0.9356 - val_loss: 0.0158 - val_acc: 0.9067\n",
      "Epoch 84/90\n",
      "1350/1350 [==============================] - 0s 31us/sample - loss: 0.0121 - acc: 0.9348 - val_loss: 0.0156 - val_acc: 0.9133\n",
      "Epoch 85/90\n",
      "1350/1350 [==============================] - 0s 30us/sample - loss: 0.0120 - acc: 0.9385 - val_loss: 0.0154 - val_acc: 0.9133\n",
      "Epoch 86/90\n",
      "1350/1350 [==============================] - 0s 29us/sample - loss: 0.0119 - acc: 0.9370 - val_loss: 0.0157 - val_acc: 0.9067\n",
      "Epoch 87/90\n",
      "1350/1350 [==============================] - 0s 29us/sample - loss: 0.0118 - acc: 0.9393 - val_loss: 0.0152 - val_acc: 0.9200\n",
      "Epoch 88/90\n",
      "1350/1350 [==============================] - 0s 30us/sample - loss: 0.0116 - acc: 0.9422 - val_loss: 0.0154 - val_acc: 0.9133\n",
      "Epoch 89/90\n",
      "1350/1350 [==============================] - 0s 31us/sample - loss: 0.0116 - acc: 0.9415 - val_loss: 0.0152 - val_acc: 0.9133\n",
      "Epoch 90/90\n",
      "1350/1350 [==============================] - 0s 33us/sample - loss: 0.0114 - acc: 0.9437 - val_loss: 0.0151 - val_acc: 0.9200\n",
      "Elasped Time: 0:00:04.156414\n",
      "Training date and time : \n",
      "2020-04-25 21:08:09\n",
      "Train on 1350 samples, validate on 150 samples\n",
      "Epoch 1/100\n",
      "1350/1350 [==============================] - 0s 131us/sample - loss: 0.0699 - acc: 0.5141 - val_loss: 0.0664 - val_acc: 0.5067\n",
      "Epoch 2/100\n",
      "1350/1350 [==============================] - 0s 30us/sample - loss: 0.0680 - acc: 0.5304 - val_loss: 0.0647 - val_acc: 0.5267\n",
      "Epoch 3/100\n",
      "1350/1350 [==============================] - 0s 29us/sample - loss: 0.0661 - acc: 0.5370 - val_loss: 0.0630 - val_acc: 0.5533\n",
      "Epoch 4/100\n",
      "1350/1350 [==============================] - 0s 31us/sample - loss: 0.0642 - acc: 0.5496 - val_loss: 0.0613 - val_acc: 0.5600\n",
      "Epoch 5/100\n",
      "1350/1350 [==============================] - 0s 30us/sample - loss: 0.0622 - acc: 0.5652 - val_loss: 0.0596 - val_acc: 0.5600\n",
      "Epoch 6/100\n",
      "1350/1350 [==============================] - 0s 31us/sample - loss: 0.0603 - acc: 0.5785 - val_loss: 0.0579 - val_acc: 0.5867\n",
      "Epoch 7/100\n",
      "1350/1350 [==============================] - 0s 32us/sample - loss: 0.0584 - acc: 0.6096 - val_loss: 0.0562 - val_acc: 0.6000\n",
      "Epoch 8/100\n",
      "1350/1350 [==============================] - 0s 31us/sample - loss: 0.0565 - acc: 0.6341 - val_loss: 0.0546 - val_acc: 0.6267\n",
      "Epoch 9/100\n",
      "1350/1350 [==============================] - 0s 31us/sample - loss: 0.0547 - acc: 0.6541 - val_loss: 0.0528 - val_acc: 0.6467\n",
      "Epoch 10/100\n",
      "1350/1350 [==============================] - 0s 30us/sample - loss: 0.0529 - acc: 0.6748 - val_loss: 0.0512 - val_acc: 0.6800\n",
      "Epoch 11/100\n",
      "1350/1350 [==============================] - 0s 29us/sample - loss: 0.0511 - acc: 0.6919 - val_loss: 0.0495 - val_acc: 0.7067\n",
      "Epoch 12/100\n",
      "1350/1350 [==============================] - 0s 31us/sample - loss: 0.0494 - acc: 0.7133 - val_loss: 0.0479 - val_acc: 0.7133\n",
      "Epoch 13/100\n",
      "1350/1350 [==============================] - 0s 30us/sample - loss: 0.0477 - acc: 0.7333 - val_loss: 0.0463 - val_acc: 0.7267\n",
      "Epoch 14/100\n",
      "1350/1350 [==============================] - 0s 31us/sample - loss: 0.0460 - acc: 0.7467 - val_loss: 0.0448 - val_acc: 0.7333\n",
      "Epoch 15/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1350/1350 [==============================] - 0s 31us/sample - loss: 0.0445 - acc: 0.7563 - val_loss: 0.0433 - val_acc: 0.7533\n",
      "Epoch 16/100\n",
      "1350/1350 [==============================] - 0s 30us/sample - loss: 0.0429 - acc: 0.7756 - val_loss: 0.0418 - val_acc: 0.7667\n",
      "Epoch 17/100\n",
      "1350/1350 [==============================] - 0s 31us/sample - loss: 0.0414 - acc: 0.7896 - val_loss: 0.0404 - val_acc: 0.7733\n",
      "Epoch 18/100\n",
      "1350/1350 [==============================] - 0s 32us/sample - loss: 0.0401 - acc: 0.7993 - val_loss: 0.0391 - val_acc: 0.7933\n",
      "Epoch 19/100\n",
      "1350/1350 [==============================] - 0s 30us/sample - loss: 0.0388 - acc: 0.8104 - val_loss: 0.0379 - val_acc: 0.8133\n",
      "Epoch 20/100\n",
      "1350/1350 [==============================] - 0s 29us/sample - loss: 0.0375 - acc: 0.8244 - val_loss: 0.0367 - val_acc: 0.8133\n",
      "Epoch 21/100\n",
      "1350/1350 [==============================] - 0s 30us/sample - loss: 0.0363 - acc: 0.8230 - val_loss: 0.0357 - val_acc: 0.8133\n",
      "Epoch 22/100\n",
      "1350/1350 [==============================] - 0s 32us/sample - loss: 0.0352 - acc: 0.8311 - val_loss: 0.0346 - val_acc: 0.8133\n",
      "Epoch 23/100\n",
      "1350/1350 [==============================] - 0s 31us/sample - loss: 0.0341 - acc: 0.8348 - val_loss: 0.0338 - val_acc: 0.8067\n",
      "Epoch 24/100\n",
      "1350/1350 [==============================] - 0s 29us/sample - loss: 0.0331 - acc: 0.8415 - val_loss: 0.0329 - val_acc: 0.8200\n",
      "Epoch 25/100\n",
      "1350/1350 [==============================] - 0s 29us/sample - loss: 0.0322 - acc: 0.8437 - val_loss: 0.0322 - val_acc: 0.8067\n",
      "Epoch 26/100\n",
      "1350/1350 [==============================] - 0s 32us/sample - loss: 0.0313 - acc: 0.8452 - val_loss: 0.0315 - val_acc: 0.8200\n",
      "Epoch 27/100\n",
      "1350/1350 [==============================] - 0s 30us/sample - loss: 0.0305 - acc: 0.8481 - val_loss: 0.0307 - val_acc: 0.8133\n",
      "Epoch 28/100\n",
      "1350/1350 [==============================] - 0s 33us/sample - loss: 0.0297 - acc: 0.8481 - val_loss: 0.0301 - val_acc: 0.8267\n",
      "Epoch 29/100\n",
      "1350/1350 [==============================] - 0s 29us/sample - loss: 0.0290 - acc: 0.8600 - val_loss: 0.0294 - val_acc: 0.8267\n",
      "Epoch 30/100\n",
      "1350/1350 [==============================] - 0s 32us/sample - loss: 0.0282 - acc: 0.8578 - val_loss: 0.0289 - val_acc: 0.8267\n",
      "Epoch 31/100\n",
      "1350/1350 [==============================] - 0s 30us/sample - loss: 0.0276 - acc: 0.8637 - val_loss: 0.0284 - val_acc: 0.8333\n",
      "Epoch 32/100\n",
      "1350/1350 [==============================] - 0s 30us/sample - loss: 0.0269 - acc: 0.8630 - val_loss: 0.0280 - val_acc: 0.8267\n",
      "Epoch 33/100\n",
      "1350/1350 [==============================] - 0s 30us/sample - loss: 0.0263 - acc: 0.8667 - val_loss: 0.0275 - val_acc: 0.8333\n",
      "Epoch 34/100\n",
      "1350/1350 [==============================] - 0s 29us/sample - loss: 0.0258 - acc: 0.8681 - val_loss: 0.0269 - val_acc: 0.8333\n",
      "Epoch 35/100\n",
      "1350/1350 [==============================] - 0s 32us/sample - loss: 0.0252 - acc: 0.8719 - val_loss: 0.0266 - val_acc: 0.8333\n",
      "Epoch 36/100\n",
      "1350/1350 [==============================] - 0s 29us/sample - loss: 0.0247 - acc: 0.8689 - val_loss: 0.0263 - val_acc: 0.8267\n",
      "Epoch 37/100\n",
      "1350/1350 [==============================] - 0s 31us/sample - loss: 0.0242 - acc: 0.8711 - val_loss: 0.0258 - val_acc: 0.8333\n",
      "Epoch 38/100\n",
      "1350/1350 [==============================] - 0s 30us/sample - loss: 0.0238 - acc: 0.8748 - val_loss: 0.0254 - val_acc: 0.8333\n",
      "Epoch 39/100\n",
      "1350/1350 [==============================] - 0s 30us/sample - loss: 0.0233 - acc: 0.8770 - val_loss: 0.0253 - val_acc: 0.8333\n",
      "Epoch 40/100\n",
      "1350/1350 [==============================] - 0s 31us/sample - loss: 0.0229 - acc: 0.8778 - val_loss: 0.0250 - val_acc: 0.8200\n",
      "Epoch 41/100\n",
      "1350/1350 [==============================] - 0s 30us/sample - loss: 0.0225 - acc: 0.8822 - val_loss: 0.0246 - val_acc: 0.8400\n",
      "Epoch 42/100\n",
      "1350/1350 [==============================] - 0s 32us/sample - loss: 0.0221 - acc: 0.8822 - val_loss: 0.0243 - val_acc: 0.8267\n",
      "Epoch 43/100\n",
      "1350/1350 [==============================] - 0s 29us/sample - loss: 0.0217 - acc: 0.8830 - val_loss: 0.0241 - val_acc: 0.8333\n",
      "Epoch 44/100\n",
      "1350/1350 [==============================] - 0s 30us/sample - loss: 0.0214 - acc: 0.8859 - val_loss: 0.0239 - val_acc: 0.8267\n",
      "Epoch 45/100\n",
      "1350/1350 [==============================] - 0s 29us/sample - loss: 0.0210 - acc: 0.8874 - val_loss: 0.0236 - val_acc: 0.8267\n",
      "Epoch 46/100\n",
      "1350/1350 [==============================] - 0s 31us/sample - loss: 0.0207 - acc: 0.8911 - val_loss: 0.0232 - val_acc: 0.8467\n",
      "Epoch 47/100\n",
      "1350/1350 [==============================] - 0s 30us/sample - loss: 0.0204 - acc: 0.8941 - val_loss: 0.0231 - val_acc: 0.8467\n",
      "Epoch 48/100\n",
      "1350/1350 [==============================] - 0s 33us/sample - loss: 0.0200 - acc: 0.8970 - val_loss: 0.0231 - val_acc: 0.8400\n",
      "Epoch 49/100\n",
      "1350/1350 [==============================] - 0s 30us/sample - loss: 0.0197 - acc: 0.8970 - val_loss: 0.0227 - val_acc: 0.8400\n",
      "Epoch 50/100\n",
      "1350/1350 [==============================] - 0s 30us/sample - loss: 0.0195 - acc: 0.8993 - val_loss: 0.0226 - val_acc: 0.8400\n",
      "Epoch 51/100\n",
      "1350/1350 [==============================] - 0s 30us/sample - loss: 0.0192 - acc: 0.9022 - val_loss: 0.0226 - val_acc: 0.8400\n",
      "Epoch 52/100\n",
      "1350/1350 [==============================] - 0s 29us/sample - loss: 0.0189 - acc: 0.9022 - val_loss: 0.0222 - val_acc: 0.8400\n",
      "Epoch 53/100\n",
      "1350/1350 [==============================] - 0s 30us/sample - loss: 0.0187 - acc: 0.9015 - val_loss: 0.0222 - val_acc: 0.8400\n",
      "Epoch 54/100\n",
      "1350/1350 [==============================] - 0s 31us/sample - loss: 0.0184 - acc: 0.9037 - val_loss: 0.0219 - val_acc: 0.8467\n",
      "Epoch 55/100\n",
      "1350/1350 [==============================] - 0s 32us/sample - loss: 0.0182 - acc: 0.9030 - val_loss: 0.0218 - val_acc: 0.8400\n",
      "Epoch 56/100\n",
      "1350/1350 [==============================] - 0s 30us/sample - loss: 0.0179 - acc: 0.9074 - val_loss: 0.0217 - val_acc: 0.8400\n",
      "Epoch 57/100\n",
      "1350/1350 [==============================] - 0s 29us/sample - loss: 0.0177 - acc: 0.9089 - val_loss: 0.0215 - val_acc: 0.8400\n",
      "Epoch 58/100\n",
      "1350/1350 [==============================] - 0s 30us/sample - loss: 0.0174 - acc: 0.9133 - val_loss: 0.0212 - val_acc: 0.8600\n",
      "Epoch 59/100\n",
      "1350/1350 [==============================] - 0s 30us/sample - loss: 0.0173 - acc: 0.9141 - val_loss: 0.0213 - val_acc: 0.8400\n",
      "Epoch 60/100\n",
      "1350/1350 [==============================] - 0s 31us/sample - loss: 0.0170 - acc: 0.9141 - val_loss: 0.0212 - val_acc: 0.8467\n",
      "Epoch 61/100\n",
      "1350/1350 [==============================] - 0s 29us/sample - loss: 0.0168 - acc: 0.9119 - val_loss: 0.0211 - val_acc: 0.8333\n",
      "Epoch 62/100\n",
      "1350/1350 [==============================] - 0s 30us/sample - loss: 0.0166 - acc: 0.9148 - val_loss: 0.0211 - val_acc: 0.8467\n",
      "Epoch 63/100\n",
      "1350/1350 [==============================] - 0s 30us/sample - loss: 0.0164 - acc: 0.9156 - val_loss: 0.0208 - val_acc: 0.8467\n",
      "Epoch 64/100\n",
      "1350/1350 [==============================] - 0s 29us/sample - loss: 0.0163 - acc: 0.9163 - val_loss: 0.0208 - val_acc: 0.8333\n",
      "Epoch 65/100\n",
      "1350/1350 [==============================] - 0s 30us/sample - loss: 0.0161 - acc: 0.9178 - val_loss: 0.0206 - val_acc: 0.8600\n",
      "Epoch 66/100\n",
      "1350/1350 [==============================] - 0s 30us/sample - loss: 0.0159 - acc: 0.9185 - val_loss: 0.0204 - val_acc: 0.8600\n",
      "Epoch 67/100\n",
      "1350/1350 [==============================] - 0s 31us/sample - loss: 0.0157 - acc: 0.9200 - val_loss: 0.0203 - val_acc: 0.8533\n",
      "Epoch 68/100\n",
      "1350/1350 [==============================] - 0s 31us/sample - loss: 0.0156 - acc: 0.9200 - val_loss: 0.0203 - val_acc: 0.8600\n",
      "Epoch 69/100\n",
      "1350/1350 [==============================] - 0s 30us/sample - loss: 0.0154 - acc: 0.9230 - val_loss: 0.0203 - val_acc: 0.8467\n",
      "Epoch 70/100\n",
      "1350/1350 [==============================] - 0s 30us/sample - loss: 0.0152 - acc: 0.9222 - val_loss: 0.0200 - val_acc: 0.8600\n",
      "Epoch 71/100\n",
      "1350/1350 [==============================] - 0s 30us/sample - loss: 0.0151 - acc: 0.9252 - val_loss: 0.0201 - val_acc: 0.8533\n",
      "Epoch 72/100\n",
      "1350/1350 [==============================] - 0s 29us/sample - loss: 0.0149 - acc: 0.9252 - val_loss: 0.0199 - val_acc: 0.8533\n",
      "Epoch 73/100\n",
      "1350/1350 [==============================] - 0s 30us/sample - loss: 0.0147 - acc: 0.9252 - val_loss: 0.0200 - val_acc: 0.8533\n",
      "Epoch 74/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1350/1350 [==============================] - 0s 31us/sample - loss: 0.0146 - acc: 0.9281 - val_loss: 0.0198 - val_acc: 0.8600\n",
      "Epoch 75/100\n",
      "1350/1350 [==============================] - 0s 29us/sample - loss: 0.0144 - acc: 0.9267 - val_loss: 0.0197 - val_acc: 0.8600\n",
      "Epoch 76/100\n",
      "1350/1350 [==============================] - 0s 29us/sample - loss: 0.0143 - acc: 0.9267 - val_loss: 0.0195 - val_acc: 0.8667\n",
      "Epoch 77/100\n",
      "1350/1350 [==============================] - 0s 30us/sample - loss: 0.0142 - acc: 0.9274 - val_loss: 0.0197 - val_acc: 0.8600\n",
      "Epoch 78/100\n",
      "1350/1350 [==============================] - 0s 31us/sample - loss: 0.0140 - acc: 0.9296 - val_loss: 0.0196 - val_acc: 0.8667\n",
      "Epoch 79/100\n",
      "1350/1350 [==============================] - 0s 30us/sample - loss: 0.0139 - acc: 0.9289 - val_loss: 0.0193 - val_acc: 0.8667\n",
      "Epoch 80/100\n",
      "1350/1350 [==============================] - 0s 30us/sample - loss: 0.0138 - acc: 0.9319 - val_loss: 0.0193 - val_acc: 0.8667\n",
      "Epoch 81/100\n",
      "1350/1350 [==============================] - 0s 29us/sample - loss: 0.0136 - acc: 0.9311 - val_loss: 0.0191 - val_acc: 0.8667\n",
      "Epoch 82/100\n",
      "1350/1350 [==============================] - 0s 30us/sample - loss: 0.0135 - acc: 0.9326 - val_loss: 0.0191 - val_acc: 0.8600\n",
      "Epoch 83/100\n",
      "1350/1350 [==============================] - 0s 30us/sample - loss: 0.0134 - acc: 0.9326 - val_loss: 0.0192 - val_acc: 0.8667\n",
      "Epoch 84/100\n",
      "1350/1350 [==============================] - 0s 32us/sample - loss: 0.0133 - acc: 0.9326 - val_loss: 0.0191 - val_acc: 0.8667\n",
      "Epoch 85/100\n",
      "1350/1350 [==============================] - 0s 31us/sample - loss: 0.0131 - acc: 0.9356 - val_loss: 0.0189 - val_acc: 0.8667\n",
      "Epoch 86/100\n",
      "1350/1350 [==============================] - 0s 29us/sample - loss: 0.0130 - acc: 0.9341 - val_loss: 0.0190 - val_acc: 0.8667\n",
      "Epoch 87/100\n",
      "1350/1350 [==============================] - 0s 30us/sample - loss: 0.0129 - acc: 0.9326 - val_loss: 0.0188 - val_acc: 0.8667\n",
      "Epoch 88/100\n",
      "1350/1350 [==============================] - 0s 29us/sample - loss: 0.0128 - acc: 0.9348 - val_loss: 0.0188 - val_acc: 0.8667\n",
      "Epoch 89/100\n",
      "1350/1350 [==============================] - 0s 30us/sample - loss: 0.0127 - acc: 0.9363 - val_loss: 0.0188 - val_acc: 0.8667\n",
      "Epoch 90/100\n",
      "1350/1350 [==============================] - 0s 28us/sample - loss: 0.0126 - acc: 0.9378 - val_loss: 0.0187 - val_acc: 0.8600\n",
      "Epoch 91/100\n",
      "1350/1350 [==============================] - 0s 32us/sample - loss: 0.0125 - acc: 0.9363 - val_loss: 0.0189 - val_acc: 0.8667\n",
      "Epoch 92/100\n",
      "1350/1350 [==============================] - 0s 30us/sample - loss: 0.0124 - acc: 0.9370 - val_loss: 0.0185 - val_acc: 0.8733\n",
      "Epoch 93/100\n",
      "1350/1350 [==============================] - 0s 30us/sample - loss: 0.0123 - acc: 0.9370 - val_loss: 0.0185 - val_acc: 0.8667\n",
      "Epoch 94/100\n",
      "1350/1350 [==============================] - 0s 32us/sample - loss: 0.0122 - acc: 0.9407 - val_loss: 0.0184 - val_acc: 0.8667\n",
      "Epoch 95/100\n",
      "1350/1350 [==============================] - 0s 30us/sample - loss: 0.0121 - acc: 0.9385 - val_loss: 0.0184 - val_acc: 0.8667\n",
      "Epoch 96/100\n",
      "1350/1350 [==============================] - 0s 31us/sample - loss: 0.0120 - acc: 0.9393 - val_loss: 0.0185 - val_acc: 0.8667\n",
      "Epoch 97/100\n",
      "1350/1350 [==============================] - 0s 31us/sample - loss: 0.0119 - acc: 0.9393 - val_loss: 0.0183 - val_acc: 0.8667\n",
      "Epoch 98/100\n",
      "1350/1350 [==============================] - 0s 31us/sample - loss: 0.0117 - acc: 0.9400 - val_loss: 0.0182 - val_acc: 0.8667\n",
      "Epoch 99/100\n",
      "1350/1350 [==============================] - 0s 30us/sample - loss: 0.0117 - acc: 0.9422 - val_loss: 0.0182 - val_acc: 0.8667\n",
      "Epoch 100/100\n",
      "1350/1350 [==============================] - 0s 31us/sample - loss: 0.0116 - acc: 0.9407 - val_loss: 0.0182 - val_acc: 0.8600\n",
      "Elasped Time: 0:00:04.574255\n",
      "Training date and time : \n",
      "2020-04-25 21:08:13\n",
      "Train on 1350 samples, validate on 150 samples\n",
      "Epoch 1/110\n",
      "1350/1350 [==============================] - 0s 127us/sample - loss: 0.0680 - acc: 0.5481 - val_loss: 0.0665 - val_acc: 0.5600\n",
      "Epoch 2/110\n",
      "1350/1350 [==============================] - 0s 30us/sample - loss: 0.0658 - acc: 0.5681 - val_loss: 0.0646 - val_acc: 0.5600\n",
      "Epoch 3/110\n",
      "1350/1350 [==============================] - 0s 30us/sample - loss: 0.0637 - acc: 0.5815 - val_loss: 0.0626 - val_acc: 0.5667\n",
      "Epoch 4/110\n",
      "1350/1350 [==============================] - 0s 30us/sample - loss: 0.0615 - acc: 0.5904 - val_loss: 0.0607 - val_acc: 0.5733\n",
      "Epoch 5/110\n",
      "1350/1350 [==============================] - 0s 33us/sample - loss: 0.0594 - acc: 0.6081 - val_loss: 0.0587 - val_acc: 0.5867\n",
      "Epoch 6/110\n",
      "1350/1350 [==============================] - 0s 30us/sample - loss: 0.0573 - acc: 0.6289 - val_loss: 0.0567 - val_acc: 0.5933\n",
      "Epoch 7/110\n",
      "1350/1350 [==============================] - 0s 32us/sample - loss: 0.0553 - acc: 0.6496 - val_loss: 0.0547 - val_acc: 0.6200\n",
      "Epoch 8/110\n",
      "1350/1350 [==============================] - 0s 31us/sample - loss: 0.0533 - acc: 0.6681 - val_loss: 0.0529 - val_acc: 0.6467\n",
      "Epoch 9/110\n",
      "1350/1350 [==============================] - 0s 32us/sample - loss: 0.0514 - acc: 0.6785 - val_loss: 0.0509 - val_acc: 0.6467\n",
      "Epoch 10/110\n",
      "1350/1350 [==============================] - 0s 31us/sample - loss: 0.0495 - acc: 0.6926 - val_loss: 0.0490 - val_acc: 0.6600\n",
      "Epoch 11/110\n",
      "1350/1350 [==============================] - 0s 29us/sample - loss: 0.0477 - acc: 0.7089 - val_loss: 0.0471 - val_acc: 0.7267\n",
      "Epoch 12/110\n",
      "1350/1350 [==============================] - 0s 31us/sample - loss: 0.0459 - acc: 0.7281 - val_loss: 0.0452 - val_acc: 0.7333\n",
      "Epoch 13/110\n",
      "1350/1350 [==============================] - 0s 32us/sample - loss: 0.0442 - acc: 0.7496 - val_loss: 0.0434 - val_acc: 0.7400\n",
      "Epoch 14/110\n",
      "1350/1350 [==============================] - 0s 33us/sample - loss: 0.0425 - acc: 0.7637 - val_loss: 0.0415 - val_acc: 0.7867\n",
      "Epoch 15/110\n",
      "1350/1350 [==============================] - 0s 30us/sample - loss: 0.0409 - acc: 0.7800 - val_loss: 0.0400 - val_acc: 0.7933\n",
      "Epoch 16/110\n",
      "1350/1350 [==============================] - 0s 30us/sample - loss: 0.0394 - acc: 0.7933 - val_loss: 0.0384 - val_acc: 0.8133\n",
      "Epoch 17/110\n",
      "1350/1350 [==============================] - 0s 32us/sample - loss: 0.0379 - acc: 0.8074 - val_loss: 0.0367 - val_acc: 0.8333\n",
      "Epoch 18/110\n",
      "1350/1350 [==============================] - 0s 31us/sample - loss: 0.0365 - acc: 0.8178 - val_loss: 0.0351 - val_acc: 0.8400\n",
      "Epoch 19/110\n",
      "1350/1350 [==============================] - 0s 32us/sample - loss: 0.0352 - acc: 0.8281 - val_loss: 0.0339 - val_acc: 0.8400\n",
      "Epoch 20/110\n",
      "1350/1350 [==============================] - 0s 30us/sample - loss: 0.0339 - acc: 0.8333 - val_loss: 0.0326 - val_acc: 0.8467\n",
      "Epoch 21/110\n",
      "1350/1350 [==============================] - 0s 30us/sample - loss: 0.0327 - acc: 0.8407 - val_loss: 0.0315 - val_acc: 0.8600\n",
      "Epoch 22/110\n",
      "1350/1350 [==============================] - 0s 31us/sample - loss: 0.0316 - acc: 0.8481 - val_loss: 0.0307 - val_acc: 0.8600\n",
      "Epoch 23/110\n",
      "1350/1350 [==============================] - 0s 29us/sample - loss: 0.0306 - acc: 0.8496 - val_loss: 0.0295 - val_acc: 0.8733\n",
      "Epoch 24/110\n",
      "1350/1350 [==============================] - 0s 30us/sample - loss: 0.0296 - acc: 0.8548 - val_loss: 0.0285 - val_acc: 0.8733\n",
      "Epoch 25/110\n",
      "1350/1350 [==============================] - 0s 31us/sample - loss: 0.0287 - acc: 0.8607 - val_loss: 0.0278 - val_acc: 0.8800\n",
      "Epoch 26/110\n",
      "1350/1350 [==============================] - 0s 30us/sample - loss: 0.0278 - acc: 0.8667 - val_loss: 0.0272 - val_acc: 0.8800\n",
      "Epoch 27/110\n",
      "1350/1350 [==============================] - 0s 32us/sample - loss: 0.0270 - acc: 0.8763 - val_loss: 0.0266 - val_acc: 0.8867\n",
      "Epoch 28/110\n",
      "1350/1350 [==============================] - 0s 30us/sample - loss: 0.0262 - acc: 0.8756 - val_loss: 0.0259 - val_acc: 0.8800\n",
      "Epoch 29/110\n",
      "1350/1350 [==============================] - 0s 32us/sample - loss: 0.0256 - acc: 0.8778 - val_loss: 0.0252 - val_acc: 0.8800\n",
      "Epoch 30/110\n",
      "1350/1350 [==============================] - 0s 32us/sample - loss: 0.0249 - acc: 0.8822 - val_loss: 0.0246 - val_acc: 0.8800\n",
      "Epoch 31/110\n",
      "1350/1350 [==============================] - 0s 31us/sample - loss: 0.0242 - acc: 0.8844 - val_loss: 0.0243 - val_acc: 0.8800\n",
      "Epoch 32/110\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1350/1350 [==============================] - 0s 31us/sample - loss: 0.0236 - acc: 0.8874 - val_loss: 0.0238 - val_acc: 0.8800\n",
      "Epoch 33/110\n",
      "1350/1350 [==============================] - 0s 30us/sample - loss: 0.0230 - acc: 0.8896 - val_loss: 0.0233 - val_acc: 0.8867\n",
      "Epoch 34/110\n",
      "1350/1350 [==============================] - 0s 29us/sample - loss: 0.0225 - acc: 0.8896 - val_loss: 0.0230 - val_acc: 0.8933\n",
      "Epoch 35/110\n",
      "1350/1350 [==============================] - 0s 34us/sample - loss: 0.0220 - acc: 0.8919 - val_loss: 0.0226 - val_acc: 0.8867\n",
      "Epoch 36/110\n",
      "1350/1350 [==============================] - 0s 30us/sample - loss: 0.0215 - acc: 0.8956 - val_loss: 0.0223 - val_acc: 0.8867\n",
      "Epoch 37/110\n",
      "1350/1350 [==============================] - 0s 30us/sample - loss: 0.0210 - acc: 0.8948 - val_loss: 0.0218 - val_acc: 0.8800\n",
      "Epoch 38/110\n",
      "1350/1350 [==============================] - 0s 30us/sample - loss: 0.0206 - acc: 0.8956 - val_loss: 0.0217 - val_acc: 0.8867\n",
      "Epoch 39/110\n",
      "1350/1350 [==============================] - 0s 31us/sample - loss: 0.0202 - acc: 0.8970 - val_loss: 0.0213 - val_acc: 0.8800\n",
      "Epoch 40/110\n",
      "1350/1350 [==============================] - 0s 30us/sample - loss: 0.0198 - acc: 0.9030 - val_loss: 0.0208 - val_acc: 0.8800\n",
      "Epoch 41/110\n",
      "1350/1350 [==============================] - 0s 30us/sample - loss: 0.0194 - acc: 0.9000 - val_loss: 0.0209 - val_acc: 0.8933\n",
      "Epoch 42/110\n",
      "1350/1350 [==============================] - 0s 32us/sample - loss: 0.0190 - acc: 0.8993 - val_loss: 0.0204 - val_acc: 0.8867\n",
      "Epoch 43/110\n",
      "1350/1350 [==============================] - 0s 31us/sample - loss: 0.0187 - acc: 0.9074 - val_loss: 0.0202 - val_acc: 0.8867\n",
      "Epoch 44/110\n",
      "1350/1350 [==============================] - 0s 31us/sample - loss: 0.0183 - acc: 0.9059 - val_loss: 0.0202 - val_acc: 0.8867\n",
      "Epoch 45/110\n",
      "1350/1350 [==============================] - 0s 32us/sample - loss: 0.0180 - acc: 0.9074 - val_loss: 0.0198 - val_acc: 0.8933\n",
      "Epoch 46/110\n",
      "1350/1350 [==============================] - 0s 30us/sample - loss: 0.0177 - acc: 0.9126 - val_loss: 0.0198 - val_acc: 0.8933\n",
      "Epoch 47/110\n",
      "1350/1350 [==============================] - 0s 31us/sample - loss: 0.0173 - acc: 0.9104 - val_loss: 0.0192 - val_acc: 0.8933\n",
      "Epoch 48/110\n",
      "1350/1350 [==============================] - 0s 31us/sample - loss: 0.0171 - acc: 0.9119 - val_loss: 0.0192 - val_acc: 0.8867\n",
      "Epoch 49/110\n",
      "1350/1350 [==============================] - 0s 29us/sample - loss: 0.0168 - acc: 0.9148 - val_loss: 0.0190 - val_acc: 0.8933\n",
      "Epoch 50/110\n",
      "1350/1350 [==============================] - 0s 31us/sample - loss: 0.0165 - acc: 0.9163 - val_loss: 0.0188 - val_acc: 0.8933\n",
      "Epoch 51/110\n",
      "1350/1350 [==============================] - 0s 30us/sample - loss: 0.0163 - acc: 0.9156 - val_loss: 0.0188 - val_acc: 0.8933\n",
      "Epoch 52/110\n",
      "1350/1350 [==============================] - 0s 31us/sample - loss: 0.0160 - acc: 0.9222 - val_loss: 0.0188 - val_acc: 0.9000\n",
      "Epoch 53/110\n",
      "1350/1350 [==============================] - 0s 30us/sample - loss: 0.0158 - acc: 0.9237 - val_loss: 0.0185 - val_acc: 0.8867\n",
      "Epoch 54/110\n",
      "1350/1350 [==============================] - 0s 32us/sample - loss: 0.0155 - acc: 0.9230 - val_loss: 0.0183 - val_acc: 0.8867\n",
      "Epoch 55/110\n",
      "1350/1350 [==============================] - 0s 30us/sample - loss: 0.0153 - acc: 0.9259 - val_loss: 0.0182 - val_acc: 0.8933\n",
      "Epoch 56/110\n",
      "1350/1350 [==============================] - 0s 31us/sample - loss: 0.0151 - acc: 0.9274 - val_loss: 0.0183 - val_acc: 0.8933\n",
      "Epoch 57/110\n",
      "1350/1350 [==============================] - 0s 30us/sample - loss: 0.0148 - acc: 0.9274 - val_loss: 0.0182 - val_acc: 0.9000\n",
      "Epoch 58/110\n",
      "1350/1350 [==============================] - 0s 29us/sample - loss: 0.0146 - acc: 0.9252 - val_loss: 0.0180 - val_acc: 0.8933\n",
      "Epoch 59/110\n",
      "1350/1350 [==============================] - 0s 30us/sample - loss: 0.0144 - acc: 0.9289 - val_loss: 0.0177 - val_acc: 0.8867\n",
      "Epoch 60/110\n",
      "1350/1350 [==============================] - 0s 32us/sample - loss: 0.0142 - acc: 0.9304 - val_loss: 0.0177 - val_acc: 0.8933\n",
      "Epoch 61/110\n",
      "1350/1350 [==============================] - 0s 30us/sample - loss: 0.0140 - acc: 0.9289 - val_loss: 0.0177 - val_acc: 0.8933\n",
      "Epoch 62/110\n",
      "1350/1350 [==============================] - 0s 29us/sample - loss: 0.0138 - acc: 0.9319 - val_loss: 0.0173 - val_acc: 0.8800\n",
      "Epoch 63/110\n",
      "1350/1350 [==============================] - 0s 31us/sample - loss: 0.0136 - acc: 0.9341 - val_loss: 0.0173 - val_acc: 0.8867\n",
      "Epoch 64/110\n",
      "1350/1350 [==============================] - 0s 30us/sample - loss: 0.0135 - acc: 0.9341 - val_loss: 0.0171 - val_acc: 0.8867\n",
      "Epoch 65/110\n",
      "1350/1350 [==============================] - 0s 31us/sample - loss: 0.0133 - acc: 0.9341 - val_loss: 0.0171 - val_acc: 0.8867\n",
      "Epoch 66/110\n",
      "1350/1350 [==============================] - 0s 30us/sample - loss: 0.0132 - acc: 0.9385 - val_loss: 0.0172 - val_acc: 0.8933\n",
      "Epoch 67/110\n",
      "1350/1350 [==============================] - 0s 29us/sample - loss: 0.0130 - acc: 0.9393 - val_loss: 0.0172 - val_acc: 0.8933\n",
      "Epoch 68/110\n",
      "1350/1350 [==============================] - 0s 31us/sample - loss: 0.0128 - acc: 0.9370 - val_loss: 0.0170 - val_acc: 0.8933\n",
      "Epoch 69/110\n",
      "1350/1350 [==============================] - 0s 29us/sample - loss: 0.0127 - acc: 0.9393 - val_loss: 0.0170 - val_acc: 0.8933\n",
      "Epoch 70/110\n",
      "1350/1350 [==============================] - 0s 29us/sample - loss: 0.0125 - acc: 0.9407 - val_loss: 0.0167 - val_acc: 0.8867\n",
      "Epoch 71/110\n",
      "1350/1350 [==============================] - 0s 31us/sample - loss: 0.0124 - acc: 0.9407 - val_loss: 0.0167 - val_acc: 0.8933\n",
      "Epoch 72/110\n",
      "1350/1350 [==============================] - 0s 31us/sample - loss: 0.0122 - acc: 0.9400 - val_loss: 0.0166 - val_acc: 0.8867\n",
      "Epoch 73/110\n",
      "1350/1350 [==============================] - 0s 30us/sample - loss: 0.0121 - acc: 0.9430 - val_loss: 0.0166 - val_acc: 0.8867\n",
      "Epoch 74/110\n",
      "1350/1350 [==============================] - 0s 33us/sample - loss: 0.0119 - acc: 0.9422 - val_loss: 0.0168 - val_acc: 0.8933\n",
      "Epoch 75/110\n",
      "1350/1350 [==============================] - 0s 30us/sample - loss: 0.0117 - acc: 0.9444 - val_loss: 0.0165 - val_acc: 0.8867\n",
      "Epoch 76/110\n",
      "1350/1350 [==============================] - 0s 31us/sample - loss: 0.0117 - acc: 0.9437 - val_loss: 0.0164 - val_acc: 0.8867\n",
      "Epoch 77/110\n",
      "1350/1350 [==============================] - 0s 31us/sample - loss: 0.0115 - acc: 0.9444 - val_loss: 0.0164 - val_acc: 0.8933\n",
      "Epoch 78/110\n",
      "1350/1350 [==============================] - 0s 31us/sample - loss: 0.0114 - acc: 0.9452 - val_loss: 0.0164 - val_acc: 0.8933\n",
      "Epoch 79/110\n",
      "1350/1350 [==============================] - 0s 30us/sample - loss: 0.0113 - acc: 0.9467 - val_loss: 0.0163 - val_acc: 0.9000\n",
      "Epoch 80/110\n",
      "1350/1350 [==============================] - 0s 29us/sample - loss: 0.0111 - acc: 0.9474 - val_loss: 0.0162 - val_acc: 0.8933\n",
      "Epoch 81/110\n",
      "1350/1350 [==============================] - 0s 31us/sample - loss: 0.0110 - acc: 0.9496 - val_loss: 0.0164 - val_acc: 0.9000\n",
      "Epoch 82/110\n",
      "1350/1350 [==============================] - 0s 32us/sample - loss: 0.0109 - acc: 0.9481 - val_loss: 0.0161 - val_acc: 0.9000\n",
      "Epoch 83/110\n",
      "1350/1350 [==============================] - 0s 32us/sample - loss: 0.0108 - acc: 0.9496 - val_loss: 0.0158 - val_acc: 0.9000\n",
      "Epoch 84/110\n",
      "1350/1350 [==============================] - 0s 30us/sample - loss: 0.0107 - acc: 0.9489 - val_loss: 0.0161 - val_acc: 0.9000\n",
      "Epoch 85/110\n",
      "1350/1350 [==============================] - 0s 31us/sample - loss: 0.0105 - acc: 0.9504 - val_loss: 0.0159 - val_acc: 0.9000\n",
      "Epoch 86/110\n",
      "1350/1350 [==============================] - 0s 30us/sample - loss: 0.0105 - acc: 0.9541 - val_loss: 0.0159 - val_acc: 0.8933\n",
      "Epoch 87/110\n",
      "1350/1350 [==============================] - 0s 32us/sample - loss: 0.0103 - acc: 0.9548 - val_loss: 0.0157 - val_acc: 0.8933\n",
      "Epoch 88/110\n",
      "1350/1350 [==============================] - 0s 31us/sample - loss: 0.0102 - acc: 0.9541 - val_loss: 0.0157 - val_acc: 0.8933\n",
      "Epoch 89/110\n",
      "1350/1350 [==============================] - 0s 30us/sample - loss: 0.0102 - acc: 0.9548 - val_loss: 0.0156 - val_acc: 0.8933\n",
      "Epoch 90/110\n",
      "1350/1350 [==============================] - 0s 30us/sample - loss: 0.0100 - acc: 0.9556 - val_loss: 0.0156 - val_acc: 0.8933\n",
      "Epoch 91/110\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1350/1350 [==============================] - 0s 29us/sample - loss: 0.0100 - acc: 0.9548 - val_loss: 0.0157 - val_acc: 0.9000\n",
      "Epoch 92/110\n",
      "1350/1350 [==============================] - 0s 30us/sample - loss: 0.0098 - acc: 0.9548 - val_loss: 0.0157 - val_acc: 0.8933\n",
      "Epoch 93/110\n",
      "1350/1350 [==============================] - 0s 31us/sample - loss: 0.0098 - acc: 0.9578 - val_loss: 0.0155 - val_acc: 0.9000\n",
      "Epoch 94/110\n",
      "1350/1350 [==============================] - 0s 30us/sample - loss: 0.0097 - acc: 0.9570 - val_loss: 0.0156 - val_acc: 0.9000\n",
      "Epoch 95/110\n",
      "1350/1350 [==============================] - 0s 31us/sample - loss: 0.0096 - acc: 0.9570 - val_loss: 0.0155 - val_acc: 0.9000\n",
      "Epoch 96/110\n",
      "1350/1350 [==============================] - 0s 31us/sample - loss: 0.0095 - acc: 0.9578 - val_loss: 0.0155 - val_acc: 0.9000\n",
      "Epoch 97/110\n",
      "1350/1350 [==============================] - 0s 31us/sample - loss: 0.0094 - acc: 0.9585 - val_loss: 0.0153 - val_acc: 0.8933\n",
      "Epoch 98/110\n",
      "1350/1350 [==============================] - 0s 31us/sample - loss: 0.0093 - acc: 0.9578 - val_loss: 0.0152 - val_acc: 0.8933\n",
      "Epoch 99/110\n",
      "1350/1350 [==============================] - 0s 30us/sample - loss: 0.0092 - acc: 0.9585 - val_loss: 0.0155 - val_acc: 0.9000\n",
      "Epoch 100/110\n",
      "1350/1350 [==============================] - 0s 33us/sample - loss: 0.0091 - acc: 0.9585 - val_loss: 0.0153 - val_acc: 0.8933\n",
      "Epoch 101/110\n",
      "1350/1350 [==============================] - 0s 32us/sample - loss: 0.0091 - acc: 0.9585 - val_loss: 0.0153 - val_acc: 0.9000\n",
      "Epoch 102/110\n",
      "1350/1350 [==============================] - 0s 31us/sample - loss: 0.0090 - acc: 0.9600 - val_loss: 0.0154 - val_acc: 0.8867\n",
      "Epoch 103/110\n",
      "1350/1350 [==============================] - 0s 32us/sample - loss: 0.0089 - acc: 0.9600 - val_loss: 0.0153 - val_acc: 0.8933\n",
      "Epoch 104/110\n",
      "1350/1350 [==============================] - 0s 31us/sample - loss: 0.0088 - acc: 0.9600 - val_loss: 0.0152 - val_acc: 0.8933\n",
      "Epoch 105/110\n",
      "1350/1350 [==============================] - 0s 29us/sample - loss: 0.0087 - acc: 0.9615 - val_loss: 0.0152 - val_acc: 0.9000\n",
      "Epoch 106/110\n",
      "1350/1350 [==============================] - 0s 31us/sample - loss: 0.0087 - acc: 0.9607 - val_loss: 0.0151 - val_acc: 0.9000\n",
      "Epoch 107/110\n",
      "1350/1350 [==============================] - 0s 31us/sample - loss: 0.0086 - acc: 0.9600 - val_loss: 0.0151 - val_acc: 0.9000\n",
      "Epoch 108/110\n",
      "1350/1350 [==============================] - 0s 32us/sample - loss: 0.0085 - acc: 0.9607 - val_loss: 0.0150 - val_acc: 0.9000\n",
      "Epoch 109/110\n",
      "1350/1350 [==============================] - 0s 29us/sample - loss: 0.0084 - acc: 0.9615 - val_loss: 0.0151 - val_acc: 0.8933\n",
      "Epoch 110/110\n",
      "1350/1350 [==============================] - 0s 30us/sample - loss: 0.0084 - acc: 0.9607 - val_loss: 0.0151 - val_acc: 0.8933\n",
      "Elasped Time: 0:00:05.057109\n",
      "Training date and time : \n",
      "2020-04-25 21:08:19\n",
      "Train on 1350 samples, validate on 150 samples\n",
      "Epoch 1/120\n",
      "1350/1350 [==============================] - 0s 143us/sample - loss: 0.0708 - acc: 0.5096 - val_loss: 0.0694 - val_acc: 0.5000\n",
      "Epoch 2/120\n",
      "1350/1350 [==============================] - 0s 30us/sample - loss: 0.0688 - acc: 0.5304 - val_loss: 0.0675 - val_acc: 0.5067\n",
      "Epoch 3/120\n",
      "1350/1350 [==============================] - 0s 31us/sample - loss: 0.0667 - acc: 0.5415 - val_loss: 0.0657 - val_acc: 0.5333\n",
      "Epoch 4/120\n",
      "1350/1350 [==============================] - 0s 31us/sample - loss: 0.0647 - acc: 0.5593 - val_loss: 0.0638 - val_acc: 0.5400\n",
      "Epoch 5/120\n",
      "1350/1350 [==============================] - 0s 30us/sample - loss: 0.0628 - acc: 0.5756 - val_loss: 0.0619 - val_acc: 0.5600\n",
      "Epoch 6/120\n",
      "1350/1350 [==============================] - 0s 31us/sample - loss: 0.0608 - acc: 0.5978 - val_loss: 0.0601 - val_acc: 0.5533\n",
      "Epoch 7/120\n",
      "1350/1350 [==============================] - 0s 31us/sample - loss: 0.0589 - acc: 0.6222 - val_loss: 0.0582 - val_acc: 0.5667\n",
      "Epoch 8/120\n",
      "1350/1350 [==============================] - 0s 30us/sample - loss: 0.0570 - acc: 0.6385 - val_loss: 0.0564 - val_acc: 0.6133\n",
      "Epoch 9/120\n",
      "1350/1350 [==============================] - 0s 31us/sample - loss: 0.0551 - acc: 0.6607 - val_loss: 0.0547 - val_acc: 0.6133\n",
      "Epoch 10/120\n",
      "1350/1350 [==============================] - 0s 31us/sample - loss: 0.0533 - acc: 0.6748 - val_loss: 0.0529 - val_acc: 0.6267\n",
      "Epoch 11/120\n",
      "1350/1350 [==============================] - 0s 31us/sample - loss: 0.0515 - acc: 0.6956 - val_loss: 0.0512 - val_acc: 0.6467\n",
      "Epoch 12/120\n",
      "1350/1350 [==============================] - 0s 31us/sample - loss: 0.0497 - acc: 0.7133 - val_loss: 0.0495 - val_acc: 0.6733\n",
      "Epoch 13/120\n",
      "1350/1350 [==============================] - 0s 31us/sample - loss: 0.0480 - acc: 0.7319 - val_loss: 0.0478 - val_acc: 0.7000\n",
      "Epoch 14/120\n",
      "1350/1350 [==============================] - 0s 31us/sample - loss: 0.0462 - acc: 0.7444 - val_loss: 0.0460 - val_acc: 0.7133\n",
      "Epoch 15/120\n",
      "1350/1350 [==============================] - 0s 32us/sample - loss: 0.0445 - acc: 0.7607 - val_loss: 0.0441 - val_acc: 0.7600\n",
      "Epoch 16/120\n",
      "1350/1350 [==============================] - 0s 31us/sample - loss: 0.0428 - acc: 0.7770 - val_loss: 0.0423 - val_acc: 0.7800\n",
      "Epoch 17/120\n",
      "1350/1350 [==============================] - 0s 31us/sample - loss: 0.0412 - acc: 0.7933 - val_loss: 0.0408 - val_acc: 0.8000\n",
      "Epoch 18/120\n",
      "1350/1350 [==============================] - 0s 31us/sample - loss: 0.0397 - acc: 0.8007 - val_loss: 0.0393 - val_acc: 0.8000\n",
      "Epoch 19/120\n",
      "1350/1350 [==============================] - 0s 30us/sample - loss: 0.0381 - acc: 0.8037 - val_loss: 0.0377 - val_acc: 0.8000\n",
      "Epoch 20/120\n",
      "1350/1350 [==============================] - 0s 30us/sample - loss: 0.0367 - acc: 0.8089 - val_loss: 0.0364 - val_acc: 0.8000\n",
      "Epoch 21/120\n",
      "1350/1350 [==============================] - 0s 29us/sample - loss: 0.0354 - acc: 0.8207 - val_loss: 0.0351 - val_acc: 0.8133\n",
      "Epoch 22/120\n",
      "1350/1350 [==============================] - 0s 29us/sample - loss: 0.0341 - acc: 0.8237 - val_loss: 0.0340 - val_acc: 0.8133\n",
      "Epoch 23/120\n",
      "1350/1350 [==============================] - 0s 29us/sample - loss: 0.0330 - acc: 0.8289 - val_loss: 0.0331 - val_acc: 0.8200\n",
      "Epoch 24/120\n",
      "1350/1350 [==============================] - 0s 32us/sample - loss: 0.0319 - acc: 0.8341 - val_loss: 0.0321 - val_acc: 0.8267\n",
      "Epoch 25/120\n",
      "1350/1350 [==============================] - 0s 31us/sample - loss: 0.0309 - acc: 0.8370 - val_loss: 0.0313 - val_acc: 0.8400\n",
      "Epoch 26/120\n",
      "1350/1350 [==============================] - 0s 29us/sample - loss: 0.0300 - acc: 0.8415 - val_loss: 0.0304 - val_acc: 0.8400\n",
      "Epoch 27/120\n",
      "1350/1350 [==============================] - 0s 30us/sample - loss: 0.0292 - acc: 0.8422 - val_loss: 0.0297 - val_acc: 0.8400\n",
      "Epoch 28/120\n",
      "1350/1350 [==============================] - 0s 28us/sample - loss: 0.0284 - acc: 0.8444 - val_loss: 0.0290 - val_acc: 0.8400\n",
      "Epoch 29/120\n",
      "1350/1350 [==============================] - 0s 32us/sample - loss: 0.0277 - acc: 0.8467 - val_loss: 0.0281 - val_acc: 0.8533\n",
      "Epoch 30/120\n",
      "1350/1350 [==============================] - 0s 29us/sample - loss: 0.0270 - acc: 0.8533 - val_loss: 0.0279 - val_acc: 0.8467\n",
      "Epoch 31/120\n",
      "1350/1350 [==============================] - 0s 31us/sample - loss: 0.0264 - acc: 0.8563 - val_loss: 0.0272 - val_acc: 0.8533\n",
      "Epoch 32/120\n",
      "1350/1350 [==============================] - 0s 30us/sample - loss: 0.0258 - acc: 0.8578 - val_loss: 0.0269 - val_acc: 0.8600\n",
      "Epoch 33/120\n",
      "1350/1350 [==============================] - 0s 31us/sample - loss: 0.0252 - acc: 0.8578 - val_loss: 0.0262 - val_acc: 0.8667\n",
      "Epoch 34/120\n",
      "1350/1350 [==============================] - 0s 29us/sample - loss: 0.0247 - acc: 0.8659 - val_loss: 0.0259 - val_acc: 0.8667\n",
      "Epoch 35/120\n",
      "1350/1350 [==============================] - 0s 31us/sample - loss: 0.0242 - acc: 0.8689 - val_loss: 0.0256 - val_acc: 0.8667\n",
      "Epoch 36/120\n",
      "1350/1350 [==============================] - 0s 32us/sample - loss: 0.0238 - acc: 0.8711 - val_loss: 0.0251 - val_acc: 0.8733\n",
      "Epoch 37/120\n",
      "1350/1350 [==============================] - 0s 31us/sample - loss: 0.0233 - acc: 0.8733 - val_loss: 0.0247 - val_acc: 0.8733\n",
      "Epoch 38/120\n",
      "1350/1350 [==============================] - 0s 32us/sample - loss: 0.0229 - acc: 0.8756 - val_loss: 0.0244 - val_acc: 0.8733\n",
      "Epoch 39/120\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1350/1350 [==============================] - 0s 29us/sample - loss: 0.0225 - acc: 0.8785 - val_loss: 0.0240 - val_acc: 0.8733\n",
      "Epoch 40/120\n",
      "1350/1350 [==============================] - 0s 30us/sample - loss: 0.0222 - acc: 0.8793 - val_loss: 0.0238 - val_acc: 0.8733\n",
      "Epoch 41/120\n",
      "1350/1350 [==============================] - 0s 30us/sample - loss: 0.0218 - acc: 0.8822 - val_loss: 0.0235 - val_acc: 0.8800\n",
      "Epoch 42/120\n",
      "1350/1350 [==============================] - 0s 30us/sample - loss: 0.0215 - acc: 0.8830 - val_loss: 0.0233 - val_acc: 0.8800\n",
      "Epoch 43/120\n",
      "1350/1350 [==============================] - 0s 30us/sample - loss: 0.0211 - acc: 0.8867 - val_loss: 0.0229 - val_acc: 0.8733\n",
      "Epoch 44/120\n",
      "1350/1350 [==============================] - 0s 30us/sample - loss: 0.0208 - acc: 0.8867 - val_loss: 0.0230 - val_acc: 0.8733\n",
      "Epoch 45/120\n",
      "1350/1350 [==============================] - 0s 30us/sample - loss: 0.0206 - acc: 0.8889 - val_loss: 0.0226 - val_acc: 0.8733\n",
      "Epoch 46/120\n",
      "1350/1350 [==============================] - 0s 32us/sample - loss: 0.0203 - acc: 0.8859 - val_loss: 0.0222 - val_acc: 0.8800\n",
      "Epoch 47/120\n",
      "1350/1350 [==============================] - 0s 32us/sample - loss: 0.0200 - acc: 0.8874 - val_loss: 0.0217 - val_acc: 0.8867\n",
      "Epoch 48/120\n",
      "1350/1350 [==============================] - 0s 30us/sample - loss: 0.0197 - acc: 0.8911 - val_loss: 0.0216 - val_acc: 0.8867\n",
      "Epoch 49/120\n",
      "1350/1350 [==============================] - 0s 33us/sample - loss: 0.0195 - acc: 0.8926 - val_loss: 0.0216 - val_acc: 0.8867\n",
      "Epoch 50/120\n",
      "1350/1350 [==============================] - 0s 31us/sample - loss: 0.0192 - acc: 0.8941 - val_loss: 0.0214 - val_acc: 0.8800\n",
      "Epoch 51/120\n",
      "1350/1350 [==============================] - 0s 30us/sample - loss: 0.0190 - acc: 0.8956 - val_loss: 0.0214 - val_acc: 0.8800\n",
      "Epoch 52/120\n",
      "1350/1350 [==============================] - 0s 30us/sample - loss: 0.0187 - acc: 0.8956 - val_loss: 0.0212 - val_acc: 0.8733\n",
      "Epoch 53/120\n",
      "1350/1350 [==============================] - 0s 29us/sample - loss: 0.0185 - acc: 0.8985 - val_loss: 0.0212 - val_acc: 0.8733\n",
      "Epoch 54/120\n",
      "1350/1350 [==============================] - 0s 31us/sample - loss: 0.0183 - acc: 0.8963 - val_loss: 0.0208 - val_acc: 0.8800\n",
      "Epoch 55/120\n",
      "1350/1350 [==============================] - 0s 30us/sample - loss: 0.0181 - acc: 0.8985 - val_loss: 0.0207 - val_acc: 0.8800\n",
      "Epoch 56/120\n",
      "1350/1350 [==============================] - 0s 31us/sample - loss: 0.0179 - acc: 0.8978 - val_loss: 0.0204 - val_acc: 0.8800\n",
      "Epoch 57/120\n",
      "1350/1350 [==============================] - 0s 31us/sample - loss: 0.0177 - acc: 0.9022 - val_loss: 0.0203 - val_acc: 0.8733\n",
      "Epoch 58/120\n",
      "1350/1350 [==============================] - 0s 31us/sample - loss: 0.0175 - acc: 0.9022 - val_loss: 0.0202 - val_acc: 0.8733\n",
      "Epoch 59/120\n",
      "1350/1350 [==============================] - 0s 34us/sample - loss: 0.0173 - acc: 0.9022 - val_loss: 0.0200 - val_acc: 0.8733\n",
      "Epoch 60/120\n",
      "1350/1350 [==============================] - 0s 32us/sample - loss: 0.0171 - acc: 0.9022 - val_loss: 0.0200 - val_acc: 0.8667\n",
      "Epoch 61/120\n",
      "1350/1350 [==============================] - 0s 30us/sample - loss: 0.0170 - acc: 0.9000 - val_loss: 0.0195 - val_acc: 0.8800\n",
      "Epoch 62/120\n",
      "1350/1350 [==============================] - 0s 30us/sample - loss: 0.0168 - acc: 0.9052 - val_loss: 0.0197 - val_acc: 0.8667\n",
      "Epoch 63/120\n",
      "1350/1350 [==============================] - 0s 29us/sample - loss: 0.0166 - acc: 0.9052 - val_loss: 0.0196 - val_acc: 0.8733\n",
      "Epoch 64/120\n",
      "1350/1350 [==============================] - 0s 30us/sample - loss: 0.0165 - acc: 0.9059 - val_loss: 0.0191 - val_acc: 0.8800\n",
      "Epoch 65/120\n",
      "1350/1350 [==============================] - 0s 30us/sample - loss: 0.0163 - acc: 0.9067 - val_loss: 0.0194 - val_acc: 0.8667\n",
      "Epoch 66/120\n",
      "1350/1350 [==============================] - 0s 31us/sample - loss: 0.0162 - acc: 0.9067 - val_loss: 0.0192 - val_acc: 0.8667\n",
      "Epoch 67/120\n",
      "1350/1350 [==============================] - 0s 31us/sample - loss: 0.0160 - acc: 0.9089 - val_loss: 0.0189 - val_acc: 0.8800\n",
      "Epoch 68/120\n",
      "1350/1350 [==============================] - 0s 31us/sample - loss: 0.0159 - acc: 0.9104 - val_loss: 0.0192 - val_acc: 0.8800\n",
      "Epoch 69/120\n",
      "1350/1350 [==============================] - 0s 31us/sample - loss: 0.0158 - acc: 0.9104 - val_loss: 0.0187 - val_acc: 0.8800\n",
      "Epoch 70/120\n",
      "1350/1350 [==============================] - 0s 29us/sample - loss: 0.0156 - acc: 0.9111 - val_loss: 0.0189 - val_acc: 0.8733\n",
      "Epoch 71/120\n",
      "1350/1350 [==============================] - 0s 32us/sample - loss: 0.0155 - acc: 0.9126 - val_loss: 0.0185 - val_acc: 0.8800\n",
      "Epoch 72/120\n",
      "1350/1350 [==============================] - 0s 30us/sample - loss: 0.0153 - acc: 0.9126 - val_loss: 0.0185 - val_acc: 0.8800\n",
      "Epoch 73/120\n",
      "1350/1350 [==============================] - 0s 33us/sample - loss: 0.0152 - acc: 0.9148 - val_loss: 0.0185 - val_acc: 0.8733\n",
      "Epoch 74/120\n",
      "1350/1350 [==============================] - 0s 29us/sample - loss: 0.0151 - acc: 0.9133 - val_loss: 0.0183 - val_acc: 0.8733\n",
      "Epoch 75/120\n",
      "1350/1350 [==============================] - 0s 30us/sample - loss: 0.0149 - acc: 0.9141 - val_loss: 0.0184 - val_acc: 0.8800\n",
      "Epoch 76/120\n",
      "1350/1350 [==============================] - 0s 30us/sample - loss: 0.0148 - acc: 0.9170 - val_loss: 0.0182 - val_acc: 0.8733\n",
      "Epoch 77/120\n",
      "1350/1350 [==============================] - 0s 30us/sample - loss: 0.0147 - acc: 0.9193 - val_loss: 0.0183 - val_acc: 0.8800\n",
      "Epoch 78/120\n",
      "1350/1350 [==============================] - 0s 29us/sample - loss: 0.0146 - acc: 0.9193 - val_loss: 0.0181 - val_acc: 0.8800\n",
      "Epoch 79/120\n",
      "1350/1350 [==============================] - 0s 30us/sample - loss: 0.0144 - acc: 0.9193 - val_loss: 0.0181 - val_acc: 0.8667\n",
      "Epoch 80/120\n",
      "1350/1350 [==============================] - 0s 31us/sample - loss: 0.0143 - acc: 0.9222 - val_loss: 0.0181 - val_acc: 0.8733\n",
      "Epoch 81/120\n",
      "1350/1350 [==============================] - 0s 29us/sample - loss: 0.0142 - acc: 0.9207 - val_loss: 0.0178 - val_acc: 0.8867\n",
      "Epoch 82/120\n",
      "1350/1350 [==============================] - 0s 32us/sample - loss: 0.0141 - acc: 0.9222 - val_loss: 0.0177 - val_acc: 0.8800\n",
      "Epoch 83/120\n",
      "1350/1350 [==============================] - 0s 33us/sample - loss: 0.0140 - acc: 0.9222 - val_loss: 0.0175 - val_acc: 0.8800\n",
      "Epoch 84/120\n",
      "1350/1350 [==============================] - 0s 31us/sample - loss: 0.0139 - acc: 0.9222 - val_loss: 0.0176 - val_acc: 0.8733\n",
      "Epoch 85/120\n",
      "1350/1350 [==============================] - 0s 30us/sample - loss: 0.0137 - acc: 0.9244 - val_loss: 0.0178 - val_acc: 0.8800\n",
      "Epoch 86/120\n",
      "1350/1350 [==============================] - 0s 31us/sample - loss: 0.0136 - acc: 0.9244 - val_loss: 0.0177 - val_acc: 0.8800\n",
      "Epoch 87/120\n",
      "1350/1350 [==============================] - 0s 30us/sample - loss: 0.0135 - acc: 0.9259 - val_loss: 0.0178 - val_acc: 0.8733\n",
      "Epoch 88/120\n",
      "1350/1350 [==============================] - 0s 29us/sample - loss: 0.0134 - acc: 0.9237 - val_loss: 0.0173 - val_acc: 0.8867\n",
      "Epoch 89/120\n",
      "1350/1350 [==============================] - 0s 29us/sample - loss: 0.0133 - acc: 0.9252 - val_loss: 0.0171 - val_acc: 0.8867\n",
      "Epoch 90/120\n",
      "1350/1350 [==============================] - 0s 29us/sample - loss: 0.0132 - acc: 0.9259 - val_loss: 0.0174 - val_acc: 0.8933\n",
      "Epoch 91/120\n",
      "1350/1350 [==============================] - 0s 31us/sample - loss: 0.0131 - acc: 0.9259 - val_loss: 0.0174 - val_acc: 0.8800\n",
      "Epoch 92/120\n",
      "1350/1350 [==============================] - 0s 31us/sample - loss: 0.0130 - acc: 0.9259 - val_loss: 0.0172 - val_acc: 0.8867\n",
      "Epoch 93/120\n",
      "1350/1350 [==============================] - 0s 29us/sample - loss: 0.0129 - acc: 0.9259 - val_loss: 0.0172 - val_acc: 0.8800\n",
      "Epoch 94/120\n",
      "1350/1350 [==============================] - 0s 30us/sample - loss: 0.0128 - acc: 0.9274 - val_loss: 0.0172 - val_acc: 0.8800\n",
      "Epoch 95/120\n",
      "1350/1350 [==============================] - 0s 31us/sample - loss: 0.0127 - acc: 0.9267 - val_loss: 0.0173 - val_acc: 0.8733\n",
      "Epoch 96/120\n",
      "1350/1350 [==============================] - 0s 31us/sample - loss: 0.0126 - acc: 0.9289 - val_loss: 0.0171 - val_acc: 0.8867\n",
      "Epoch 97/120\n",
      "1350/1350 [==============================] - 0s 30us/sample - loss: 0.0125 - acc: 0.9326 - val_loss: 0.0171 - val_acc: 0.8800\n",
      "Epoch 98/120\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1350/1350 [==============================] - 0s 30us/sample - loss: 0.0124 - acc: 0.9289 - val_loss: 0.0170 - val_acc: 0.8800\n",
      "Epoch 99/120\n",
      "1350/1350 [==============================] - 0s 30us/sample - loss: 0.0123 - acc: 0.9296 - val_loss: 0.0167 - val_acc: 0.8800\n",
      "Epoch 100/120\n",
      "1350/1350 [==============================] - 0s 29us/sample - loss: 0.0122 - acc: 0.9326 - val_loss: 0.0167 - val_acc: 0.8800\n",
      "Epoch 101/120\n",
      "1350/1350 [==============================] - 0s 29us/sample - loss: 0.0121 - acc: 0.9348 - val_loss: 0.0170 - val_acc: 0.8867\n",
      "Epoch 102/120\n",
      "1350/1350 [==============================] - 0s 30us/sample - loss: 0.0121 - acc: 0.9333 - val_loss: 0.0167 - val_acc: 0.8867\n",
      "Epoch 103/120\n",
      "1350/1350 [==============================] - 0s 30us/sample - loss: 0.0119 - acc: 0.9348 - val_loss: 0.0169 - val_acc: 0.8867\n",
      "Epoch 104/120\n",
      "1350/1350 [==============================] - 0s 30us/sample - loss: 0.0119 - acc: 0.9341 - val_loss: 0.0167 - val_acc: 0.8800\n",
      "Epoch 105/120\n",
      "1350/1350 [==============================] - 0s 30us/sample - loss: 0.0118 - acc: 0.9370 - val_loss: 0.0166 - val_acc: 0.8867\n",
      "Epoch 106/120\n",
      "1350/1350 [==============================] - 0s 32us/sample - loss: 0.0117 - acc: 0.9356 - val_loss: 0.0166 - val_acc: 0.8867\n",
      "Epoch 107/120\n",
      "1350/1350 [==============================] - 0s 33us/sample - loss: 0.0116 - acc: 0.9393 - val_loss: 0.0166 - val_acc: 0.8933\n",
      "Epoch 108/120\n",
      "1350/1350 [==============================] - 0s 31us/sample - loss: 0.0115 - acc: 0.9407 - val_loss: 0.0163 - val_acc: 0.8867\n",
      "Epoch 109/120\n",
      "1350/1350 [==============================] - 0s 32us/sample - loss: 0.0114 - acc: 0.9415 - val_loss: 0.0166 - val_acc: 0.8867\n",
      "Epoch 110/120\n",
      "1350/1350 [==============================] - 0s 31us/sample - loss: 0.0113 - acc: 0.9407 - val_loss: 0.0165 - val_acc: 0.8867\n",
      "Epoch 111/120\n",
      "1350/1350 [==============================] - 0s 31us/sample - loss: 0.0112 - acc: 0.9415 - val_loss: 0.0165 - val_acc: 0.8933\n",
      "Epoch 112/120\n",
      "1350/1350 [==============================] - 0s 32us/sample - loss: 0.0111 - acc: 0.9422 - val_loss: 0.0166 - val_acc: 0.8867\n",
      "Epoch 113/120\n",
      "1350/1350 [==============================] - 0s 31us/sample - loss: 0.0111 - acc: 0.9430 - val_loss: 0.0164 - val_acc: 0.8933\n",
      "Epoch 114/120\n",
      "1350/1350 [==============================] - 0s 30us/sample - loss: 0.0110 - acc: 0.9444 - val_loss: 0.0166 - val_acc: 0.8867\n",
      "Epoch 115/120\n",
      "1350/1350 [==============================] - 0s 33us/sample - loss: 0.0109 - acc: 0.9430 - val_loss: 0.0165 - val_acc: 0.8867\n",
      "Epoch 116/120\n",
      "1350/1350 [==============================] - 0s 30us/sample - loss: 0.0108 - acc: 0.9452 - val_loss: 0.0163 - val_acc: 0.8867\n",
      "Epoch 117/120\n",
      "1350/1350 [==============================] - 0s 32us/sample - loss: 0.0107 - acc: 0.9474 - val_loss: 0.0163 - val_acc: 0.9000\n",
      "Epoch 118/120\n",
      "1350/1350 [==============================] - 0s 32us/sample - loss: 0.0106 - acc: 0.9489 - val_loss: 0.0164 - val_acc: 0.9000\n",
      "Epoch 119/120\n",
      "1350/1350 [==============================] - 0s 29us/sample - loss: 0.0105 - acc: 0.9474 - val_loss: 0.0163 - val_acc: 0.8933\n",
      "Epoch 120/120\n",
      "1350/1350 [==============================] - 0s 31us/sample - loss: 0.0104 - acc: 0.9481 - val_loss: 0.0164 - val_acc: 0.8933\n",
      "Elasped Time: 0:00:05.512253\n",
      "Training date and time : \n",
      "2020-04-25 21:08:24\n",
      "Train on 1350 samples, validate on 150 samples\n",
      "Epoch 1/130\n",
      "1350/1350 [==============================] - 0s 136us/sample - loss: 0.0685 - acc: 0.5511 - val_loss: 0.0661 - val_acc: 0.5067\n",
      "Epoch 2/130\n",
      "1350/1350 [==============================] - 0s 32us/sample - loss: 0.0663 - acc: 0.5652 - val_loss: 0.0642 - val_acc: 0.5067\n",
      "Epoch 3/130\n",
      "1350/1350 [==============================] - 0s 32us/sample - loss: 0.0642 - acc: 0.5763 - val_loss: 0.0622 - val_acc: 0.5400\n",
      "Epoch 4/130\n",
      "1350/1350 [==============================] - 0s 30us/sample - loss: 0.0620 - acc: 0.5956 - val_loss: 0.0601 - val_acc: 0.5533\n",
      "Epoch 5/130\n",
      "1350/1350 [==============================] - 0s 32us/sample - loss: 0.0598 - acc: 0.6111 - val_loss: 0.0581 - val_acc: 0.5667\n",
      "Epoch 6/130\n",
      "1350/1350 [==============================] - 0s 30us/sample - loss: 0.0575 - acc: 0.6385 - val_loss: 0.0562 - val_acc: 0.5867\n",
      "Epoch 7/130\n",
      "1350/1350 [==============================] - 0s 30us/sample - loss: 0.0553 - acc: 0.6667 - val_loss: 0.0542 - val_acc: 0.6000\n",
      "Epoch 8/130\n",
      "1350/1350 [==============================] - 0s 30us/sample - loss: 0.0531 - acc: 0.6874 - val_loss: 0.0522 - val_acc: 0.6333\n",
      "Epoch 9/130\n",
      "1350/1350 [==============================] - 0s 33us/sample - loss: 0.0509 - acc: 0.7037 - val_loss: 0.0502 - val_acc: 0.6467\n",
      "Epoch 10/130\n",
      "1350/1350 [==============================] - 0s 30us/sample - loss: 0.0488 - acc: 0.7215 - val_loss: 0.0482 - val_acc: 0.6933\n",
      "Epoch 11/130\n",
      "1350/1350 [==============================] - 0s 31us/sample - loss: 0.0467 - acc: 0.7385 - val_loss: 0.0461 - val_acc: 0.7133\n",
      "Epoch 12/130\n",
      "1350/1350 [==============================] - 0s 32us/sample - loss: 0.0447 - acc: 0.7563 - val_loss: 0.0440 - val_acc: 0.7333\n",
      "Epoch 13/130\n",
      "1350/1350 [==============================] - 0s 29us/sample - loss: 0.0427 - acc: 0.7807 - val_loss: 0.0418 - val_acc: 0.7667\n",
      "Epoch 14/130\n",
      "1350/1350 [==============================] - 0s 31us/sample - loss: 0.0408 - acc: 0.7919 - val_loss: 0.0398 - val_acc: 0.7867\n",
      "Epoch 15/130\n",
      "1350/1350 [==============================] - 0s 31us/sample - loss: 0.0389 - acc: 0.8126 - val_loss: 0.0379 - val_acc: 0.8000\n",
      "Epoch 16/130\n",
      "1350/1350 [==============================] - 0s 31us/sample - loss: 0.0372 - acc: 0.8267 - val_loss: 0.0362 - val_acc: 0.8000\n",
      "Epoch 17/130\n",
      "1350/1350 [==============================] - 0s 30us/sample - loss: 0.0355 - acc: 0.8459 - val_loss: 0.0345 - val_acc: 0.8067\n",
      "Epoch 18/130\n",
      "1350/1350 [==============================] - 0s 30us/sample - loss: 0.0340 - acc: 0.8496 - val_loss: 0.0332 - val_acc: 0.8200\n",
      "Epoch 19/130\n",
      "1350/1350 [==============================] - 0s 31us/sample - loss: 0.0326 - acc: 0.8548 - val_loss: 0.0318 - val_acc: 0.8267\n",
      "Epoch 20/130\n",
      "1350/1350 [==============================] - 0s 29us/sample - loss: 0.0312 - acc: 0.8652 - val_loss: 0.0306 - val_acc: 0.8333\n",
      "Epoch 21/130\n",
      "1350/1350 [==============================] - 0s 30us/sample - loss: 0.0299 - acc: 0.8726 - val_loss: 0.0294 - val_acc: 0.8333\n",
      "Epoch 22/130\n",
      "1350/1350 [==============================] - 0s 31us/sample - loss: 0.0288 - acc: 0.8719 - val_loss: 0.0285 - val_acc: 0.8467\n",
      "Epoch 23/130\n",
      "1350/1350 [==============================] - 0s 31us/sample - loss: 0.0277 - acc: 0.8815 - val_loss: 0.0277 - val_acc: 0.8467\n",
      "Epoch 24/130\n",
      "1350/1350 [==============================] - 0s 32us/sample - loss: 0.0267 - acc: 0.8852 - val_loss: 0.0272 - val_acc: 0.8600\n",
      "Epoch 25/130\n",
      "1350/1350 [==============================] - 0s 31us/sample - loss: 0.0258 - acc: 0.8896 - val_loss: 0.0265 - val_acc: 0.8333\n",
      "Epoch 26/130\n",
      "1350/1350 [==============================] - 0s 30us/sample - loss: 0.0249 - acc: 0.8926 - val_loss: 0.0257 - val_acc: 0.8400\n",
      "Epoch 27/130\n",
      "1350/1350 [==============================] - 0s 33us/sample - loss: 0.0241 - acc: 0.8948 - val_loss: 0.0251 - val_acc: 0.8533\n",
      "Epoch 28/130\n",
      "1350/1350 [==============================] - 0s 31us/sample - loss: 0.0234 - acc: 0.8985 - val_loss: 0.0247 - val_acc: 0.8333\n",
      "Epoch 29/130\n",
      "1350/1350 [==============================] - 0s 29us/sample - loss: 0.0226 - acc: 0.8978 - val_loss: 0.0241 - val_acc: 0.8533\n",
      "Epoch 30/130\n",
      "1350/1350 [==============================] - 0s 30us/sample - loss: 0.0220 - acc: 0.9037 - val_loss: 0.0238 - val_acc: 0.8533\n",
      "Epoch 31/130\n",
      "1350/1350 [==============================] - 0s 30us/sample - loss: 0.0214 - acc: 0.9059 - val_loss: 0.0234 - val_acc: 0.8533\n",
      "Epoch 32/130\n",
      "1350/1350 [==============================] - 0s 29us/sample - loss: 0.0208 - acc: 0.9037 - val_loss: 0.0232 - val_acc: 0.8467\n",
      "Epoch 33/130\n",
      "1350/1350 [==============================] - 0s 30us/sample - loss: 0.0202 - acc: 0.9067 - val_loss: 0.0229 - val_acc: 0.8533\n",
      "Epoch 34/130\n",
      "1350/1350 [==============================] - 0s 30us/sample - loss: 0.0198 - acc: 0.9104 - val_loss: 0.0228 - val_acc: 0.8400\n",
      "Epoch 35/130\n",
      "1350/1350 [==============================] - 0s 30us/sample - loss: 0.0193 - acc: 0.9111 - val_loss: 0.0223 - val_acc: 0.8400\n",
      "Epoch 36/130\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1350/1350 [==============================] - 0s 32us/sample - loss: 0.0188 - acc: 0.9178 - val_loss: 0.0222 - val_acc: 0.8400\n",
      "Epoch 37/130\n",
      "1350/1350 [==============================] - 0s 30us/sample - loss: 0.0184 - acc: 0.9156 - val_loss: 0.0218 - val_acc: 0.8533\n",
      "Epoch 38/130\n",
      "1350/1350 [==============================] - 0s 30us/sample - loss: 0.0180 - acc: 0.9170 - val_loss: 0.0216 - val_acc: 0.8400\n",
      "Epoch 39/130\n",
      "1350/1350 [==============================] - 0s 30us/sample - loss: 0.0176 - acc: 0.9200 - val_loss: 0.0214 - val_acc: 0.8600\n",
      "Epoch 40/130\n",
      "1350/1350 [==============================] - 0s 30us/sample - loss: 0.0172 - acc: 0.9230 - val_loss: 0.0212 - val_acc: 0.8467\n",
      "Epoch 41/130\n",
      "1350/1350 [==============================] - 0s 31us/sample - loss: 0.0169 - acc: 0.9252 - val_loss: 0.0210 - val_acc: 0.8467\n",
      "Epoch 42/130\n",
      "1350/1350 [==============================] - 0s 32us/sample - loss: 0.0166 - acc: 0.9244 - val_loss: 0.0210 - val_acc: 0.8400\n",
      "Epoch 43/130\n",
      "1350/1350 [==============================] - 0s 32us/sample - loss: 0.0163 - acc: 0.9252 - val_loss: 0.0209 - val_acc: 0.8400\n",
      "Epoch 44/130\n",
      "1350/1350 [==============================] - 0s 30us/sample - loss: 0.0159 - acc: 0.9289 - val_loss: 0.0207 - val_acc: 0.8467\n",
      "Epoch 45/130\n",
      "1350/1350 [==============================] - 0s 29us/sample - loss: 0.0157 - acc: 0.9274 - val_loss: 0.0207 - val_acc: 0.8467\n",
      "Epoch 46/130\n",
      "1350/1350 [==============================] - 0s 31us/sample - loss: 0.0154 - acc: 0.9296 - val_loss: 0.0205 - val_acc: 0.8533\n",
      "Epoch 47/130\n",
      "1350/1350 [==============================] - 0s 31us/sample - loss: 0.0151 - acc: 0.9304 - val_loss: 0.0202 - val_acc: 0.8467\n",
      "Epoch 48/130\n",
      "1350/1350 [==============================] - 0s 31us/sample - loss: 0.0149 - acc: 0.9311 - val_loss: 0.0201 - val_acc: 0.8467\n",
      "Epoch 49/130\n",
      "1350/1350 [==============================] - 0s 31us/sample - loss: 0.0146 - acc: 0.9333 - val_loss: 0.0200 - val_acc: 0.8533\n",
      "Epoch 50/130\n",
      "1350/1350 [==============================] - 0s 29us/sample - loss: 0.0144 - acc: 0.9311 - val_loss: 0.0200 - val_acc: 0.8400\n",
      "Epoch 51/130\n",
      "1350/1350 [==============================] - 0s 31us/sample - loss: 0.0141 - acc: 0.9341 - val_loss: 0.0201 - val_acc: 0.8400\n",
      "Epoch 52/130\n",
      "1350/1350 [==============================] - 0s 29us/sample - loss: 0.0139 - acc: 0.9341 - val_loss: 0.0201 - val_acc: 0.8467\n",
      "Epoch 53/130\n",
      "1350/1350 [==============================] - 0s 30us/sample - loss: 0.0137 - acc: 0.9333 - val_loss: 0.0200 - val_acc: 0.8467\n",
      "Epoch 54/130\n",
      "1350/1350 [==============================] - 0s 32us/sample - loss: 0.0135 - acc: 0.9333 - val_loss: 0.0198 - val_acc: 0.8533\n",
      "Epoch 55/130\n",
      "1350/1350 [==============================] - 0s 30us/sample - loss: 0.0133 - acc: 0.9370 - val_loss: 0.0198 - val_acc: 0.8533\n",
      "Epoch 56/130\n",
      "1350/1350 [==============================] - 0s 30us/sample - loss: 0.0131 - acc: 0.9363 - val_loss: 0.0198 - val_acc: 0.8400\n",
      "Epoch 57/130\n",
      "1350/1350 [==============================] - 0s 31us/sample - loss: 0.0129 - acc: 0.9378 - val_loss: 0.0198 - val_acc: 0.8400\n",
      "Epoch 58/130\n",
      "1350/1350 [==============================] - 0s 30us/sample - loss: 0.0127 - acc: 0.9370 - val_loss: 0.0197 - val_acc: 0.8400\n",
      "Epoch 59/130\n",
      "1350/1350 [==============================] - 0s 32us/sample - loss: 0.0125 - acc: 0.9393 - val_loss: 0.0199 - val_acc: 0.8333\n",
      "Epoch 60/130\n",
      "1350/1350 [==============================] - 0s 30us/sample - loss: 0.0123 - acc: 0.9422 - val_loss: 0.0196 - val_acc: 0.8467\n",
      "Epoch 61/130\n",
      "1350/1350 [==============================] - 0s 30us/sample - loss: 0.0122 - acc: 0.9415 - val_loss: 0.0195 - val_acc: 0.8400\n",
      "Epoch 62/130\n",
      "1350/1350 [==============================] - 0s 32us/sample - loss: 0.0120 - acc: 0.9422 - val_loss: 0.0195 - val_acc: 0.8533\n",
      "Epoch 63/130\n",
      "1350/1350 [==============================] - 0s 30us/sample - loss: 0.0119 - acc: 0.9415 - val_loss: 0.0195 - val_acc: 0.8533\n",
      "Epoch 64/130\n",
      "1350/1350 [==============================] - 0s 30us/sample - loss: 0.0117 - acc: 0.9422 - val_loss: 0.0195 - val_acc: 0.8467\n",
      "Epoch 65/130\n",
      "1350/1350 [==============================] - 0s 31us/sample - loss: 0.0116 - acc: 0.9430 - val_loss: 0.0194 - val_acc: 0.8533\n",
      "Epoch 66/130\n",
      "1350/1350 [==============================] - 0s 29us/sample - loss: 0.0114 - acc: 0.9437 - val_loss: 0.0196 - val_acc: 0.8533\n",
      "Epoch 67/130\n",
      "1350/1350 [==============================] - 0s 30us/sample - loss: 0.0113 - acc: 0.9444 - val_loss: 0.0194 - val_acc: 0.8467\n",
      "Epoch 68/130\n",
      "1350/1350 [==============================] - 0s 30us/sample - loss: 0.0111 - acc: 0.9444 - val_loss: 0.0194 - val_acc: 0.8533\n",
      "Epoch 69/130\n",
      "1350/1350 [==============================] - 0s 30us/sample - loss: 0.0110 - acc: 0.9459 - val_loss: 0.0196 - val_acc: 0.8467\n",
      "Epoch 70/130\n",
      "1350/1350 [==============================] - 0s 32us/sample - loss: 0.0108 - acc: 0.9459 - val_loss: 0.0194 - val_acc: 0.8467\n",
      "Epoch 71/130\n",
      "1350/1350 [==============================] - 0s 31us/sample - loss: 0.0107 - acc: 0.9474 - val_loss: 0.0194 - val_acc: 0.8533\n",
      "Epoch 72/130\n",
      "1350/1350 [==============================] - 0s 32us/sample - loss: 0.0106 - acc: 0.9467 - val_loss: 0.0194 - val_acc: 0.8400\n",
      "Epoch 73/130\n",
      "1350/1350 [==============================] - 0s 30us/sample - loss: 0.0105 - acc: 0.9481 - val_loss: 0.0194 - val_acc: 0.8533\n",
      "Epoch 74/130\n",
      "1350/1350 [==============================] - 0s 30us/sample - loss: 0.0104 - acc: 0.9496 - val_loss: 0.0194 - val_acc: 0.8533\n",
      "Epoch 75/130\n",
      "1350/1350 [==============================] - 0s 30us/sample - loss: 0.0102 - acc: 0.9496 - val_loss: 0.0194 - val_acc: 0.8533\n",
      "Epoch 76/130\n",
      "1350/1350 [==============================] - 0s 32us/sample - loss: 0.0101 - acc: 0.9511 - val_loss: 0.0193 - val_acc: 0.8533\n",
      "Epoch 77/130\n",
      "1350/1350 [==============================] - 0s 30us/sample - loss: 0.0100 - acc: 0.9496 - val_loss: 0.0194 - val_acc: 0.8533\n",
      "Epoch 78/130\n",
      "1350/1350 [==============================] - 0s 30us/sample - loss: 0.0099 - acc: 0.9511 - val_loss: 0.0194 - val_acc: 0.8533\n",
      "Epoch 79/130\n",
      "1350/1350 [==============================] - 0s 29us/sample - loss: 0.0098 - acc: 0.9533 - val_loss: 0.0194 - val_acc: 0.8533\n",
      "Epoch 80/130\n",
      "1350/1350 [==============================] - 0s 30us/sample - loss: 0.0097 - acc: 0.9533 - val_loss: 0.0196 - val_acc: 0.8533\n",
      "Epoch 81/130\n",
      "1350/1350 [==============================] - 0s 31us/sample - loss: 0.0096 - acc: 0.9526 - val_loss: 0.0193 - val_acc: 0.8533\n",
      "Epoch 82/130\n",
      "1350/1350 [==============================] - 0s 30us/sample - loss: 0.0094 - acc: 0.9541 - val_loss: 0.0193 - val_acc: 0.8533\n",
      "Epoch 83/130\n",
      "1350/1350 [==============================] - 0s 30us/sample - loss: 0.0094 - acc: 0.9570 - val_loss: 0.0194 - val_acc: 0.8533\n",
      "Epoch 84/130\n",
      "1350/1350 [==============================] - 0s 29us/sample - loss: 0.0092 - acc: 0.9548 - val_loss: 0.0192 - val_acc: 0.8533\n",
      "Epoch 85/130\n",
      "1350/1350 [==============================] - 0s 29us/sample - loss: 0.0091 - acc: 0.9585 - val_loss: 0.0195 - val_acc: 0.8533\n",
      "Epoch 86/130\n",
      "1350/1350 [==============================] - 0s 33us/sample - loss: 0.0090 - acc: 0.9578 - val_loss: 0.0192 - val_acc: 0.8533\n",
      "Epoch 87/130\n",
      "1350/1350 [==============================] - 0s 31us/sample - loss: 0.0090 - acc: 0.9593 - val_loss: 0.0191 - val_acc: 0.8533\n",
      "Epoch 88/130\n",
      "1350/1350 [==============================] - 0s 30us/sample - loss: 0.0089 - acc: 0.9607 - val_loss: 0.0192 - val_acc: 0.8533\n",
      "Epoch 89/130\n",
      "1350/1350 [==============================] - 0s 30us/sample - loss: 0.0088 - acc: 0.9615 - val_loss: 0.0194 - val_acc: 0.8533\n",
      "Epoch 90/130\n",
      "1350/1350 [==============================] - 0s 30us/sample - loss: 0.0087 - acc: 0.9615 - val_loss: 0.0193 - val_acc: 0.8533\n",
      "Epoch 91/130\n",
      "1350/1350 [==============================] - 0s 31us/sample - loss: 0.0086 - acc: 0.9615 - val_loss: 0.0192 - val_acc: 0.8533\n",
      "Epoch 92/130\n",
      "1350/1350 [==============================] - 0s 32us/sample - loss: 0.0085 - acc: 0.9622 - val_loss: 0.0191 - val_acc: 0.8533\n",
      "Epoch 93/130\n",
      "1350/1350 [==============================] - 0s 31us/sample - loss: 0.0084 - acc: 0.9615 - val_loss: 0.0192 - val_acc: 0.8600\n",
      "Epoch 94/130\n",
      "1350/1350 [==============================] - 0s 31us/sample - loss: 0.0083 - acc: 0.9630 - val_loss: 0.0194 - val_acc: 0.8533\n",
      "Epoch 95/130\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1350/1350 [==============================] - 0s 31us/sample - loss: 0.0083 - acc: 0.9630 - val_loss: 0.0193 - val_acc: 0.8533\n",
      "Epoch 96/130\n",
      "1350/1350 [==============================] - 0s 31us/sample - loss: 0.0082 - acc: 0.9644 - val_loss: 0.0193 - val_acc: 0.8533\n",
      "Epoch 97/130\n",
      "1350/1350 [==============================] - 0s 30us/sample - loss: 0.0081 - acc: 0.9630 - val_loss: 0.0194 - val_acc: 0.8533\n",
      "Epoch 98/130\n",
      "1350/1350 [==============================] - 0s 35us/sample - loss: 0.0080 - acc: 0.9644 - val_loss: 0.0193 - val_acc: 0.8600\n",
      "Epoch 99/130\n",
      "1350/1350 [==============================] - 0s 31us/sample - loss: 0.0079 - acc: 0.9674 - val_loss: 0.0193 - val_acc: 0.8600\n",
      "Epoch 100/130\n",
      "1350/1350 [==============================] - 0s 30us/sample - loss: 0.0079 - acc: 0.9674 - val_loss: 0.0192 - val_acc: 0.8533\n",
      "Epoch 101/130\n",
      "1350/1350 [==============================] - 0s 30us/sample - loss: 0.0078 - acc: 0.9667 - val_loss: 0.0193 - val_acc: 0.8600\n",
      "Epoch 102/130\n",
      "1350/1350 [==============================] - 0s 31us/sample - loss: 0.0077 - acc: 0.9667 - val_loss: 0.0190 - val_acc: 0.8533\n",
      "Epoch 103/130\n",
      "1350/1350 [==============================] - 0s 32us/sample - loss: 0.0076 - acc: 0.9667 - val_loss: 0.0193 - val_acc: 0.8600\n",
      "Epoch 104/130\n",
      "1350/1350 [==============================] - 0s 29us/sample - loss: 0.0076 - acc: 0.9667 - val_loss: 0.0193 - val_acc: 0.8600\n",
      "Epoch 105/130\n",
      "1350/1350 [==============================] - 0s 30us/sample - loss: 0.0075 - acc: 0.9689 - val_loss: 0.0193 - val_acc: 0.8600\n",
      "Epoch 106/130\n",
      "1350/1350 [==============================] - 0s 31us/sample - loss: 0.0074 - acc: 0.9681 - val_loss: 0.0192 - val_acc: 0.8600\n",
      "Epoch 107/130\n",
      "1350/1350 [==============================] - 0s 31us/sample - loss: 0.0073 - acc: 0.9681 - val_loss: 0.0191 - val_acc: 0.8600\n",
      "Epoch 108/130\n",
      "1350/1350 [==============================] - 0s 32us/sample - loss: 0.0073 - acc: 0.9689 - val_loss: 0.0190 - val_acc: 0.8600\n",
      "Epoch 109/130\n",
      "1350/1350 [==============================] - 0s 31us/sample - loss: 0.0072 - acc: 0.9681 - val_loss: 0.0190 - val_acc: 0.8600\n",
      "Epoch 110/130\n",
      "1350/1350 [==============================] - 0s 31us/sample - loss: 0.0071 - acc: 0.9689 - val_loss: 0.0191 - val_acc: 0.8600\n",
      "Epoch 111/130\n",
      "1350/1350 [==============================] - 0s 30us/sample - loss: 0.0071 - acc: 0.9674 - val_loss: 0.0191 - val_acc: 0.8600\n",
      "Epoch 112/130\n",
      "1350/1350 [==============================] - 0s 31us/sample - loss: 0.0070 - acc: 0.9689 - val_loss: 0.0192 - val_acc: 0.8600\n",
      "Epoch 113/130\n",
      "1350/1350 [==============================] - 0s 31us/sample - loss: 0.0069 - acc: 0.9696 - val_loss: 0.0192 - val_acc: 0.8600\n",
      "Epoch 114/130\n",
      "1350/1350 [==============================] - 0s 30us/sample - loss: 0.0069 - acc: 0.9689 - val_loss: 0.0192 - val_acc: 0.8600\n",
      "Epoch 115/130\n",
      "1350/1350 [==============================] - 0s 32us/sample - loss: 0.0068 - acc: 0.9689 - val_loss: 0.0188 - val_acc: 0.8600\n",
      "Epoch 116/130\n",
      "1350/1350 [==============================] - 0s 30us/sample - loss: 0.0068 - acc: 0.9696 - val_loss: 0.0190 - val_acc: 0.8600\n",
      "Epoch 117/130\n",
      "1350/1350 [==============================] - 0s 33us/sample - loss: 0.0067 - acc: 0.9711 - val_loss: 0.0191 - val_acc: 0.8600\n",
      "Epoch 118/130\n",
      "1350/1350 [==============================] - 0s 30us/sample - loss: 0.0066 - acc: 0.9704 - val_loss: 0.0190 - val_acc: 0.8600\n",
      "Epoch 119/130\n",
      "1350/1350 [==============================] - 0s 30us/sample - loss: 0.0066 - acc: 0.9719 - val_loss: 0.0190 - val_acc: 0.8533\n",
      "Epoch 120/130\n",
      "1350/1350 [==============================] - 0s 29us/sample - loss: 0.0065 - acc: 0.9704 - val_loss: 0.0190 - val_acc: 0.8533\n",
      "Epoch 121/130\n",
      "1350/1350 [==============================] - 0s 31us/sample - loss: 0.0064 - acc: 0.9719 - val_loss: 0.0191 - val_acc: 0.8600\n",
      "Epoch 122/130\n",
      "1350/1350 [==============================] - 0s 30us/sample - loss: 0.0064 - acc: 0.9704 - val_loss: 0.0192 - val_acc: 0.8533\n",
      "Epoch 123/130\n",
      "1350/1350 [==============================] - 0s 31us/sample - loss: 0.0063 - acc: 0.9719 - val_loss: 0.0191 - val_acc: 0.8600\n",
      "Epoch 124/130\n",
      "1350/1350 [==============================] - 0s 31us/sample - loss: 0.0063 - acc: 0.9726 - val_loss: 0.0194 - val_acc: 0.8533\n",
      "Epoch 125/130\n",
      "1350/1350 [==============================] - 0s 31us/sample - loss: 0.0062 - acc: 0.9719 - val_loss: 0.0192 - val_acc: 0.8533\n",
      "Epoch 126/130\n",
      "1350/1350 [==============================] - 0s 31us/sample - loss: 0.0062 - acc: 0.9719 - val_loss: 0.0191 - val_acc: 0.8533\n",
      "Epoch 127/130\n",
      "1350/1350 [==============================] - 0s 31us/sample - loss: 0.0061 - acc: 0.9726 - val_loss: 0.0189 - val_acc: 0.8600\n",
      "Epoch 128/130\n",
      "1350/1350 [==============================] - 0s 31us/sample - loss: 0.0061 - acc: 0.9726 - val_loss: 0.0189 - val_acc: 0.8600\n",
      "Epoch 129/130\n",
      "1350/1350 [==============================] - 0s 30us/sample - loss: 0.0060 - acc: 0.9726 - val_loss: 0.0191 - val_acc: 0.8533\n",
      "Epoch 130/130\n",
      "1350/1350 [==============================] - 0s 32us/sample - loss: 0.0060 - acc: 0.9741 - val_loss: 0.0191 - val_acc: 0.8533\n",
      "Elasped Time: 0:00:05.933602\n",
      "Training date and time : \n",
      "2020-04-25 21:08:30\n",
      "Train on 1350 samples, validate on 150 samples\n",
      "Epoch 1/140\n",
      "1350/1350 [==============================] - 0s 139us/sample - loss: 0.0670 - acc: 0.5415 - val_loss: 0.0681 - val_acc: 0.5200\n",
      "Epoch 2/140\n",
      "1350/1350 [==============================] - 0s 31us/sample - loss: 0.0647 - acc: 0.5585 - val_loss: 0.0665 - val_acc: 0.5200\n",
      "Epoch 3/140\n",
      "1350/1350 [==============================] - 0s 32us/sample - loss: 0.0623 - acc: 0.5763 - val_loss: 0.0648 - val_acc: 0.5467\n",
      "Epoch 4/140\n",
      "1350/1350 [==============================] - 0s 32us/sample - loss: 0.0599 - acc: 0.5904 - val_loss: 0.0632 - val_acc: 0.5733\n",
      "Epoch 5/140\n",
      "1350/1350 [==============================] - 0s 30us/sample - loss: 0.0575 - acc: 0.6207 - val_loss: 0.0615 - val_acc: 0.5800\n",
      "Epoch 6/140\n",
      "1350/1350 [==============================] - 0s 30us/sample - loss: 0.0552 - acc: 0.6422 - val_loss: 0.0599 - val_acc: 0.5867\n",
      "Epoch 7/140\n",
      "1350/1350 [==============================] - 0s 30us/sample - loss: 0.0529 - acc: 0.6659 - val_loss: 0.0582 - val_acc: 0.6133\n",
      "Epoch 8/140\n",
      "1350/1350 [==============================] - 0s 30us/sample - loss: 0.0507 - acc: 0.6911 - val_loss: 0.0565 - val_acc: 0.6133\n",
      "Epoch 9/140\n",
      "1350/1350 [==============================] - 0s 30us/sample - loss: 0.0485 - acc: 0.7141 - val_loss: 0.0549 - val_acc: 0.6333\n",
      "Epoch 10/140\n",
      "1350/1350 [==============================] - 0s 31us/sample - loss: 0.0463 - acc: 0.7400 - val_loss: 0.0533 - val_acc: 0.6667\n",
      "Epoch 11/140\n",
      "1350/1350 [==============================] - 0s 31us/sample - loss: 0.0442 - acc: 0.7719 - val_loss: 0.0517 - val_acc: 0.6867\n",
      "Epoch 12/140\n",
      "1350/1350 [==============================] - 0s 32us/sample - loss: 0.0422 - acc: 0.7926 - val_loss: 0.0502 - val_acc: 0.7200\n",
      "Epoch 13/140\n",
      "1350/1350 [==============================] - 0s 33us/sample - loss: 0.0402 - acc: 0.8096 - val_loss: 0.0488 - val_acc: 0.7200\n",
      "Epoch 14/140\n",
      "1350/1350 [==============================] - 0s 32us/sample - loss: 0.0384 - acc: 0.8200 - val_loss: 0.0473 - val_acc: 0.7333\n",
      "Epoch 15/140\n",
      "1350/1350 [==============================] - 0s 31us/sample - loss: 0.0367 - acc: 0.8341 - val_loss: 0.0460 - val_acc: 0.7533\n",
      "Epoch 16/140\n",
      "1350/1350 [==============================] - 0s 31us/sample - loss: 0.0350 - acc: 0.8385 - val_loss: 0.0449 - val_acc: 0.7667\n",
      "Epoch 17/140\n",
      "1350/1350 [==============================] - 0s 31us/sample - loss: 0.0336 - acc: 0.8563 - val_loss: 0.0436 - val_acc: 0.7733\n",
      "Epoch 18/140\n",
      "1350/1350 [==============================] - 0s 32us/sample - loss: 0.0322 - acc: 0.8607 - val_loss: 0.0426 - val_acc: 0.7733\n",
      "Epoch 19/140\n",
      "1350/1350 [==============================] - 0s 30us/sample - loss: 0.0310 - acc: 0.8637 - val_loss: 0.0414 - val_acc: 0.7800\n",
      "Epoch 20/140\n",
      "1350/1350 [==============================] - 0s 30us/sample - loss: 0.0298 - acc: 0.8689 - val_loss: 0.0404 - val_acc: 0.7733\n",
      "Epoch 21/140\n",
      "1350/1350 [==============================] - 0s 30us/sample - loss: 0.0287 - acc: 0.8726 - val_loss: 0.0398 - val_acc: 0.7800\n",
      "Epoch 22/140\n",
      "1350/1350 [==============================] - 0s 32us/sample - loss: 0.0277 - acc: 0.8807 - val_loss: 0.0387 - val_acc: 0.7867\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 23/140\n",
      "1350/1350 [==============================] - 0s 32us/sample - loss: 0.0269 - acc: 0.8822 - val_loss: 0.0382 - val_acc: 0.7867\n",
      "Epoch 24/140\n",
      "1350/1350 [==============================] - 0s 32us/sample - loss: 0.0260 - acc: 0.8904 - val_loss: 0.0375 - val_acc: 0.8000\n",
      "Epoch 25/140\n",
      "1350/1350 [==============================] - 0s 31us/sample - loss: 0.0252 - acc: 0.8881 - val_loss: 0.0370 - val_acc: 0.8000\n",
      "Epoch 26/140\n",
      "1350/1350 [==============================] - 0s 30us/sample - loss: 0.0245 - acc: 0.8948 - val_loss: 0.0361 - val_acc: 0.8000\n",
      "Epoch 27/140\n",
      "1350/1350 [==============================] - 0s 31us/sample - loss: 0.0238 - acc: 0.8970 - val_loss: 0.0356 - val_acc: 0.8133\n",
      "Epoch 28/140\n",
      "1350/1350 [==============================] - 0s 30us/sample - loss: 0.0232 - acc: 0.8970 - val_loss: 0.0354 - val_acc: 0.8000\n",
      "Epoch 29/140\n",
      "1350/1350 [==============================] - 0s 32us/sample - loss: 0.0226 - acc: 0.9000 - val_loss: 0.0348 - val_acc: 0.8067\n",
      "Epoch 30/140\n",
      "1350/1350 [==============================] - 0s 31us/sample - loss: 0.0220 - acc: 0.9044 - val_loss: 0.0344 - val_acc: 0.8067\n",
      "Epoch 31/140\n",
      "1350/1350 [==============================] - 0s 31us/sample - loss: 0.0215 - acc: 0.9059 - val_loss: 0.0335 - val_acc: 0.8267\n",
      "Epoch 32/140\n",
      "1350/1350 [==============================] - 0s 33us/sample - loss: 0.0209 - acc: 0.9111 - val_loss: 0.0335 - val_acc: 0.8133\n",
      "Epoch 33/140\n",
      "1350/1350 [==============================] - 0s 31us/sample - loss: 0.0205 - acc: 0.9119 - val_loss: 0.0329 - val_acc: 0.8200\n",
      "Epoch 34/140\n",
      "1350/1350 [==============================] - 0s 32us/sample - loss: 0.0201 - acc: 0.9111 - val_loss: 0.0324 - val_acc: 0.8267\n",
      "Epoch 35/140\n",
      "1350/1350 [==============================] - 0s 31us/sample - loss: 0.0196 - acc: 0.9185 - val_loss: 0.0324 - val_acc: 0.8133\n",
      "Epoch 36/140\n",
      "1350/1350 [==============================] - 0s 30us/sample - loss: 0.0192 - acc: 0.9156 - val_loss: 0.0322 - val_acc: 0.8133\n",
      "Epoch 37/140\n",
      "1350/1350 [==============================] - 0s 31us/sample - loss: 0.0188 - acc: 0.9193 - val_loss: 0.0316 - val_acc: 0.8133\n",
      "Epoch 38/140\n",
      "1350/1350 [==============================] - 0s 31us/sample - loss: 0.0184 - acc: 0.9200 - val_loss: 0.0313 - val_acc: 0.8133\n",
      "Epoch 39/140\n",
      "1350/1350 [==============================] - 0s 30us/sample - loss: 0.0182 - acc: 0.9237 - val_loss: 0.0311 - val_acc: 0.8267\n",
      "Epoch 40/140\n",
      "1350/1350 [==============================] - 0s 32us/sample - loss: 0.0178 - acc: 0.9274 - val_loss: 0.0309 - val_acc: 0.8267\n",
      "Epoch 41/140\n",
      "1350/1350 [==============================] - 0s 31us/sample - loss: 0.0175 - acc: 0.9252 - val_loss: 0.0304 - val_acc: 0.8267\n",
      "Epoch 42/140\n",
      "1350/1350 [==============================] - 0s 32us/sample - loss: 0.0172 - acc: 0.9274 - val_loss: 0.0303 - val_acc: 0.8200\n",
      "Epoch 43/140\n",
      "1350/1350 [==============================] - 0s 31us/sample - loss: 0.0169 - acc: 0.9274 - val_loss: 0.0301 - val_acc: 0.8200\n",
      "Epoch 44/140\n",
      "1350/1350 [==============================] - 0s 31us/sample - loss: 0.0166 - acc: 0.9259 - val_loss: 0.0301 - val_acc: 0.8200\n",
      "Epoch 45/140\n",
      "1350/1350 [==============================] - 0s 32us/sample - loss: 0.0164 - acc: 0.9281 - val_loss: 0.0296 - val_acc: 0.8333\n",
      "Epoch 46/140\n",
      "1350/1350 [==============================] - 0s 30us/sample - loss: 0.0161 - acc: 0.9333 - val_loss: 0.0296 - val_acc: 0.8200\n",
      "Epoch 47/140\n",
      "1350/1350 [==============================] - 0s 29us/sample - loss: 0.0158 - acc: 0.9319 - val_loss: 0.0296 - val_acc: 0.8200\n",
      "Epoch 48/140\n",
      "1350/1350 [==============================] - 0s 33us/sample - loss: 0.0156 - acc: 0.9311 - val_loss: 0.0291 - val_acc: 0.8333\n",
      "Epoch 49/140\n",
      "1350/1350 [==============================] - 0s 30us/sample - loss: 0.0154 - acc: 0.9356 - val_loss: 0.0288 - val_acc: 0.8400\n",
      "Epoch 50/140\n",
      "1350/1350 [==============================] - 0s 30us/sample - loss: 0.0152 - acc: 0.9356 - val_loss: 0.0285 - val_acc: 0.8400\n",
      "Epoch 51/140\n",
      "1350/1350 [==============================] - 0s 29us/sample - loss: 0.0149 - acc: 0.9370 - val_loss: 0.0285 - val_acc: 0.8333\n",
      "Epoch 52/140\n",
      "1350/1350 [==============================] - 0s 31us/sample - loss: 0.0148 - acc: 0.9356 - val_loss: 0.0283 - val_acc: 0.8267\n",
      "Epoch 53/140\n",
      "1350/1350 [==============================] - 0s 32us/sample - loss: 0.0146 - acc: 0.9370 - val_loss: 0.0281 - val_acc: 0.8400\n",
      "Epoch 54/140\n",
      "1350/1350 [==============================] - 0s 30us/sample - loss: 0.0144 - acc: 0.9363 - val_loss: 0.0282 - val_acc: 0.8400\n",
      "Epoch 55/140\n",
      "1350/1350 [==============================] - 0s 35us/sample - loss: 0.0142 - acc: 0.9363 - val_loss: 0.0280 - val_acc: 0.8400\n",
      "Epoch 56/140\n",
      "1350/1350 [==============================] - 0s 29us/sample - loss: 0.0140 - acc: 0.9378 - val_loss: 0.0274 - val_acc: 0.8400\n",
      "Epoch 57/140\n",
      "1350/1350 [==============================] - 0s 30us/sample - loss: 0.0139 - acc: 0.9393 - val_loss: 0.0275 - val_acc: 0.8400\n",
      "Epoch 58/140\n",
      "1350/1350 [==============================] - 0s 32us/sample - loss: 0.0137 - acc: 0.9393 - val_loss: 0.0276 - val_acc: 0.8400\n",
      "Epoch 59/140\n",
      "1350/1350 [==============================] - 0s 30us/sample - loss: 0.0135 - acc: 0.9385 - val_loss: 0.0275 - val_acc: 0.8400\n",
      "Epoch 60/140\n",
      "1350/1350 [==============================] - 0s 32us/sample - loss: 0.0133 - acc: 0.9400 - val_loss: 0.0273 - val_acc: 0.8467\n",
      "Epoch 61/140\n",
      "1350/1350 [==============================] - 0s 31us/sample - loss: 0.0132 - acc: 0.9393 - val_loss: 0.0270 - val_acc: 0.8400\n",
      "Epoch 62/140\n",
      "1350/1350 [==============================] - 0s 32us/sample - loss: 0.0130 - acc: 0.9407 - val_loss: 0.0269 - val_acc: 0.8467\n",
      "Epoch 63/140\n",
      "1350/1350 [==============================] - 0s 30us/sample - loss: 0.0129 - acc: 0.9393 - val_loss: 0.0269 - val_acc: 0.8467\n",
      "Epoch 64/140\n",
      "1350/1350 [==============================] - 0s 31us/sample - loss: 0.0128 - acc: 0.9407 - val_loss: 0.0270 - val_acc: 0.8467\n",
      "Epoch 65/140\n",
      "1350/1350 [==============================] - 0s 31us/sample - loss: 0.0126 - acc: 0.9437 - val_loss: 0.0268 - val_acc: 0.8467\n",
      "Epoch 66/140\n",
      "1350/1350 [==============================] - 0s 31us/sample - loss: 0.0125 - acc: 0.9422 - val_loss: 0.0267 - val_acc: 0.8467\n",
      "Epoch 67/140\n",
      "1350/1350 [==============================] - 0s 31us/sample - loss: 0.0124 - acc: 0.9415 - val_loss: 0.0265 - val_acc: 0.8400\n",
      "Epoch 68/140\n",
      "1350/1350 [==============================] - 0s 32us/sample - loss: 0.0122 - acc: 0.9444 - val_loss: 0.0266 - val_acc: 0.8467\n",
      "Epoch 69/140\n",
      "1350/1350 [==============================] - 0s 32us/sample - loss: 0.0121 - acc: 0.9444 - val_loss: 0.0266 - val_acc: 0.8400\n",
      "Epoch 70/140\n",
      "1350/1350 [==============================] - 0s 30us/sample - loss: 0.0120 - acc: 0.9452 - val_loss: 0.0263 - val_acc: 0.8400\n",
      "Epoch 71/140\n",
      "1350/1350 [==============================] - 0s 32us/sample - loss: 0.0118 - acc: 0.9444 - val_loss: 0.0263 - val_acc: 0.8467\n",
      "Epoch 72/140\n",
      "1350/1350 [==============================] - 0s 31us/sample - loss: 0.0117 - acc: 0.9444 - val_loss: 0.0263 - val_acc: 0.8400\n",
      "Epoch 73/140\n",
      "1350/1350 [==============================] - 0s 30us/sample - loss: 0.0116 - acc: 0.9452 - val_loss: 0.0261 - val_acc: 0.8467\n",
      "Epoch 74/140\n",
      "1350/1350 [==============================] - 0s 30us/sample - loss: 0.0115 - acc: 0.9444 - val_loss: 0.0261 - val_acc: 0.8467\n",
      "Epoch 75/140\n",
      "1350/1350 [==============================] - 0s 31us/sample - loss: 0.0114 - acc: 0.9467 - val_loss: 0.0262 - val_acc: 0.8400\n",
      "Epoch 76/140\n",
      "1350/1350 [==============================] - 0s 30us/sample - loss: 0.0113 - acc: 0.9459 - val_loss: 0.0258 - val_acc: 0.8400\n",
      "Epoch 77/140\n",
      "1350/1350 [==============================] - 0s 30us/sample - loss: 0.0112 - acc: 0.9467 - val_loss: 0.0259 - val_acc: 0.8400\n",
      "Epoch 78/140\n",
      "1350/1350 [==============================] - 0s 31us/sample - loss: 0.0111 - acc: 0.9459 - val_loss: 0.0257 - val_acc: 0.8467\n",
      "Epoch 79/140\n",
      "1350/1350 [==============================] - 0s 32us/sample - loss: 0.0110 - acc: 0.9489 - val_loss: 0.0257 - val_acc: 0.8467\n",
      "Epoch 80/140\n",
      "1350/1350 [==============================] - 0s 31us/sample - loss: 0.0109 - acc: 0.9481 - val_loss: 0.0257 - val_acc: 0.8400\n",
      "Epoch 81/140\n",
      "1350/1350 [==============================] - 0s 33us/sample - loss: 0.0108 - acc: 0.9496 - val_loss: 0.0257 - val_acc: 0.8467\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 82/140\n",
      "1350/1350 [==============================] - 0s 30us/sample - loss: 0.0107 - acc: 0.9504 - val_loss: 0.0258 - val_acc: 0.8467\n",
      "Epoch 83/140\n",
      "1350/1350 [==============================] - 0s 31us/sample - loss: 0.0106 - acc: 0.9489 - val_loss: 0.0254 - val_acc: 0.8467\n",
      "Epoch 84/140\n",
      "1350/1350 [==============================] - 0s 30us/sample - loss: 0.0105 - acc: 0.9496 - val_loss: 0.0255 - val_acc: 0.8467\n",
      "Epoch 85/140\n",
      "1350/1350 [==============================] - 0s 30us/sample - loss: 0.0104 - acc: 0.9519 - val_loss: 0.0253 - val_acc: 0.8467\n",
      "Epoch 86/140\n",
      "1350/1350 [==============================] - 0s 31us/sample - loss: 0.0103 - acc: 0.9526 - val_loss: 0.0254 - val_acc: 0.8467\n",
      "Epoch 87/140\n",
      "1350/1350 [==============================] - 0s 33us/sample - loss: 0.0103 - acc: 0.9511 - val_loss: 0.0254 - val_acc: 0.8400\n",
      "Epoch 88/140\n",
      "1350/1350 [==============================] - 0s 34us/sample - loss: 0.0102 - acc: 0.9519 - val_loss: 0.0253 - val_acc: 0.8467\n",
      "Epoch 89/140\n",
      "1350/1350 [==============================] - 0s 32us/sample - loss: 0.0101 - acc: 0.9511 - val_loss: 0.0251 - val_acc: 0.8467\n",
      "Epoch 90/140\n",
      "1350/1350 [==============================] - 0s 31us/sample - loss: 0.0100 - acc: 0.9526 - val_loss: 0.0251 - val_acc: 0.8533\n",
      "Epoch 91/140\n",
      "1350/1350 [==============================] - 0s 30us/sample - loss: 0.0099 - acc: 0.9526 - val_loss: 0.0253 - val_acc: 0.8400\n",
      "Epoch 92/140\n",
      "1350/1350 [==============================] - 0s 29us/sample - loss: 0.0098 - acc: 0.9533 - val_loss: 0.0251 - val_acc: 0.8467\n",
      "Epoch 93/140\n",
      "1350/1350 [==============================] - 0s 31us/sample - loss: 0.0097 - acc: 0.9533 - val_loss: 0.0254 - val_acc: 0.8400\n",
      "Epoch 94/140\n",
      "1350/1350 [==============================] - 0s 30us/sample - loss: 0.0097 - acc: 0.9511 - val_loss: 0.0252 - val_acc: 0.8467\n",
      "Epoch 95/140\n",
      "1350/1350 [==============================] - 0s 32us/sample - loss: 0.0096 - acc: 0.9541 - val_loss: 0.0252 - val_acc: 0.8467\n",
      "Epoch 96/140\n",
      "1350/1350 [==============================] - 0s 30us/sample - loss: 0.0095 - acc: 0.9519 - val_loss: 0.0249 - val_acc: 0.8533\n",
      "Epoch 97/140\n",
      "1350/1350 [==============================] - 0s 31us/sample - loss: 0.0095 - acc: 0.9548 - val_loss: 0.0249 - val_acc: 0.8467\n",
      "Epoch 98/140\n",
      "1350/1350 [==============================] - 0s 30us/sample - loss: 0.0094 - acc: 0.9541 - val_loss: 0.0249 - val_acc: 0.8467\n",
      "Epoch 99/140\n",
      "1350/1350 [==============================] - 0s 30us/sample - loss: 0.0093 - acc: 0.9548 - val_loss: 0.0248 - val_acc: 0.8400\n",
      "Epoch 100/140\n",
      "1350/1350 [==============================] - 0s 30us/sample - loss: 0.0093 - acc: 0.9541 - val_loss: 0.0247 - val_acc: 0.8467\n",
      "Epoch 101/140\n",
      "1350/1350 [==============================] - 0s 29us/sample - loss: 0.0092 - acc: 0.9556 - val_loss: 0.0246 - val_acc: 0.8467\n",
      "Epoch 102/140\n",
      "1350/1350 [==============================] - 0s 30us/sample - loss: 0.0091 - acc: 0.9556 - val_loss: 0.0246 - val_acc: 0.8533\n",
      "Epoch 103/140\n",
      "1350/1350 [==============================] - 0s 32us/sample - loss: 0.0091 - acc: 0.9556 - val_loss: 0.0246 - val_acc: 0.8467\n",
      "Epoch 104/140\n",
      "1350/1350 [==============================] - 0s 33us/sample - loss: 0.0090 - acc: 0.9541 - val_loss: 0.0247 - val_acc: 0.8467\n",
      "Epoch 105/140\n",
      "1350/1350 [==============================] - 0s 31us/sample - loss: 0.0089 - acc: 0.9548 - val_loss: 0.0246 - val_acc: 0.8467\n",
      "Epoch 106/140\n",
      "1350/1350 [==============================] - 0s 30us/sample - loss: 0.0089 - acc: 0.9556 - val_loss: 0.0245 - val_acc: 0.8533\n",
      "Epoch 107/140\n",
      "1350/1350 [==============================] - 0s 29us/sample - loss: 0.0088 - acc: 0.9578 - val_loss: 0.0246 - val_acc: 0.8533\n",
      "Epoch 108/140\n",
      "1350/1350 [==============================] - 0s 31us/sample - loss: 0.0087 - acc: 0.9563 - val_loss: 0.0247 - val_acc: 0.8533\n",
      "Epoch 109/140\n",
      "1350/1350 [==============================] - 0s 31us/sample - loss: 0.0087 - acc: 0.9585 - val_loss: 0.0244 - val_acc: 0.8533\n",
      "Epoch 110/140\n",
      "1350/1350 [==============================] - 0s 32us/sample - loss: 0.0086 - acc: 0.9563 - val_loss: 0.0246 - val_acc: 0.8533\n",
      "Epoch 111/140\n",
      "1350/1350 [==============================] - 0s 30us/sample - loss: 0.0086 - acc: 0.9593 - val_loss: 0.0243 - val_acc: 0.8600\n",
      "Epoch 112/140\n",
      "1350/1350 [==============================] - 0s 31us/sample - loss: 0.0085 - acc: 0.9563 - val_loss: 0.0245 - val_acc: 0.8533\n",
      "Epoch 113/140\n",
      "1350/1350 [==============================] - 0s 30us/sample - loss: 0.0084 - acc: 0.9563 - val_loss: 0.0246 - val_acc: 0.8533\n",
      "Epoch 114/140\n",
      "1350/1350 [==============================] - 0s 32us/sample - loss: 0.0084 - acc: 0.9600 - val_loss: 0.0244 - val_acc: 0.8533\n",
      "Epoch 115/140\n",
      "1350/1350 [==============================] - 0s 31us/sample - loss: 0.0083 - acc: 0.9600 - val_loss: 0.0245 - val_acc: 0.8467\n",
      "Epoch 116/140\n",
      "1350/1350 [==============================] - 0s 31us/sample - loss: 0.0083 - acc: 0.9593 - val_loss: 0.0245 - val_acc: 0.8533\n",
      "Epoch 117/140\n",
      "1350/1350 [==============================] - 0s 31us/sample - loss: 0.0082 - acc: 0.9615 - val_loss: 0.0247 - val_acc: 0.8467\n",
      "Epoch 118/140\n",
      "1350/1350 [==============================] - 0s 31us/sample - loss: 0.0081 - acc: 0.9622 - val_loss: 0.0248 - val_acc: 0.8467\n",
      "Epoch 119/140\n",
      "1350/1350 [==============================] - 0s 31us/sample - loss: 0.0081 - acc: 0.9622 - val_loss: 0.0246 - val_acc: 0.8467\n",
      "Epoch 120/140\n",
      "1350/1350 [==============================] - 0s 31us/sample - loss: 0.0080 - acc: 0.9615 - val_loss: 0.0245 - val_acc: 0.8533\n",
      "Epoch 121/140\n",
      "1350/1350 [==============================] - 0s 32us/sample - loss: 0.0080 - acc: 0.9622 - val_loss: 0.0245 - val_acc: 0.8533\n",
      "Epoch 122/140\n",
      "1350/1350 [==============================] - 0s 30us/sample - loss: 0.0080 - acc: 0.9637 - val_loss: 0.0244 - val_acc: 0.8533\n",
      "Epoch 123/140\n",
      "1350/1350 [==============================] - 0s 30us/sample - loss: 0.0079 - acc: 0.9630 - val_loss: 0.0245 - val_acc: 0.8533\n",
      "Epoch 124/140\n",
      "1350/1350 [==============================] - 0s 30us/sample - loss: 0.0078 - acc: 0.9630 - val_loss: 0.0245 - val_acc: 0.8533\n",
      "Epoch 125/140\n",
      "1350/1350 [==============================] - 0s 34us/sample - loss: 0.0078 - acc: 0.9622 - val_loss: 0.0243 - val_acc: 0.8533\n",
      "Epoch 126/140\n",
      "1350/1350 [==============================] - 0s 30us/sample - loss: 0.0077 - acc: 0.9637 - val_loss: 0.0241 - val_acc: 0.8533\n",
      "Epoch 127/140\n",
      "1350/1350 [==============================] - 0s 31us/sample - loss: 0.0077 - acc: 0.9630 - val_loss: 0.0244 - val_acc: 0.8533\n",
      "Epoch 128/140\n",
      "1350/1350 [==============================] - 0s 31us/sample - loss: 0.0076 - acc: 0.9644 - val_loss: 0.0241 - val_acc: 0.8600\n",
      "Epoch 129/140\n",
      "1350/1350 [==============================] - 0s 32us/sample - loss: 0.0076 - acc: 0.9637 - val_loss: 0.0241 - val_acc: 0.8600\n",
      "Epoch 130/140\n",
      "1350/1350 [==============================] - 0s 30us/sample - loss: 0.0075 - acc: 0.9637 - val_loss: 0.0241 - val_acc: 0.8600\n",
      "Epoch 131/140\n",
      "1350/1350 [==============================] - 0s 32us/sample - loss: 0.0075 - acc: 0.9652 - val_loss: 0.0241 - val_acc: 0.8600\n",
      "Epoch 132/140\n",
      "1350/1350 [==============================] - 0s 31us/sample - loss: 0.0074 - acc: 0.9652 - val_loss: 0.0242 - val_acc: 0.8533\n",
      "Epoch 133/140\n",
      "1350/1350 [==============================] - 0s 32us/sample - loss: 0.0074 - acc: 0.9659 - val_loss: 0.0242 - val_acc: 0.8600\n",
      "Epoch 134/140\n",
      "1350/1350 [==============================] - 0s 30us/sample - loss: 0.0074 - acc: 0.9652 - val_loss: 0.0242 - val_acc: 0.8533\n",
      "Epoch 135/140\n",
      "1350/1350 [==============================] - 0s 31us/sample - loss: 0.0073 - acc: 0.9659 - val_loss: 0.0241 - val_acc: 0.8600\n",
      "Epoch 136/140\n",
      "1350/1350 [==============================] - 0s 31us/sample - loss: 0.0073 - acc: 0.9659 - val_loss: 0.0242 - val_acc: 0.8600\n",
      "Epoch 137/140\n",
      "1350/1350 [==============================] - 0s 31us/sample - loss: 0.0072 - acc: 0.9659 - val_loss: 0.0241 - val_acc: 0.8533\n",
      "Epoch 138/140\n",
      "1350/1350 [==============================] - 0s 32us/sample - loss: 0.0072 - acc: 0.9659 - val_loss: 0.0243 - val_acc: 0.8533\n",
      "Epoch 139/140\n",
      "1350/1350 [==============================] - 0s 30us/sample - loss: 0.0071 - acc: 0.9652 - val_loss: 0.0239 - val_acc: 0.8600\n",
      "Epoch 140/140\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1350/1350 [==============================] - 0s 29us/sample - loss: 0.0071 - acc: 0.9659 - val_loss: 0.0240 - val_acc: 0.8600\n",
      "Elasped Time: 0:00:06.444142\n",
      "Training date and time : \n",
      "2020-04-25 21:08:36\n",
      "Train on 1350 samples, validate on 150 samples\n",
      "Epoch 1/150\n",
      "1350/1350 [==============================] - 0s 143us/sample - loss: 0.0681 - acc: 0.5452 - val_loss: 0.0678 - val_acc: 0.6000\n",
      "Epoch 2/150\n",
      "1350/1350 [==============================] - 0s 31us/sample - loss: 0.0656 - acc: 0.5652 - val_loss: 0.0657 - val_acc: 0.6067\n",
      "Epoch 3/150\n",
      "1350/1350 [==============================] - 0s 32us/sample - loss: 0.0631 - acc: 0.5770 - val_loss: 0.0636 - val_acc: 0.6200\n",
      "Epoch 4/150\n",
      "1350/1350 [==============================] - 0s 33us/sample - loss: 0.0606 - acc: 0.5941 - val_loss: 0.0615 - val_acc: 0.6333\n",
      "Epoch 5/150\n",
      "1350/1350 [==============================] - 0s 31us/sample - loss: 0.0580 - acc: 0.6193 - val_loss: 0.0595 - val_acc: 0.6533\n",
      "Epoch 6/150\n",
      "1350/1350 [==============================] - 0s 32us/sample - loss: 0.0556 - acc: 0.6474 - val_loss: 0.0574 - val_acc: 0.6867\n",
      "Epoch 7/150\n",
      "1350/1350 [==============================] - 0s 32us/sample - loss: 0.0531 - acc: 0.6748 - val_loss: 0.0556 - val_acc: 0.6933\n",
      "Epoch 8/150\n",
      "1350/1350 [==============================] - 0s 30us/sample - loss: 0.0507 - acc: 0.7081 - val_loss: 0.0538 - val_acc: 0.7067\n",
      "Epoch 9/150\n",
      "1350/1350 [==============================] - 0s 33us/sample - loss: 0.0484 - acc: 0.7311 - val_loss: 0.0520 - val_acc: 0.7267\n",
      "Epoch 10/150\n",
      "1350/1350 [==============================] - 0s 32us/sample - loss: 0.0460 - acc: 0.7563 - val_loss: 0.0503 - val_acc: 0.7333\n",
      "Epoch 11/150\n",
      "1350/1350 [==============================] - 0s 33us/sample - loss: 0.0438 - acc: 0.7881 - val_loss: 0.0487 - val_acc: 0.7400\n",
      "Epoch 12/150\n",
      "1350/1350 [==============================] - 0s 31us/sample - loss: 0.0417 - acc: 0.8074 - val_loss: 0.0472 - val_acc: 0.7400\n",
      "Epoch 13/150\n",
      "1350/1350 [==============================] - 0s 31us/sample - loss: 0.0396 - acc: 0.8178 - val_loss: 0.0456 - val_acc: 0.7467\n",
      "Epoch 14/150\n",
      "1350/1350 [==============================] - 0s 31us/sample - loss: 0.0377 - acc: 0.8333 - val_loss: 0.0442 - val_acc: 0.7533\n",
      "Epoch 15/150\n",
      "1350/1350 [==============================] - 0s 34us/sample - loss: 0.0359 - acc: 0.8444 - val_loss: 0.0429 - val_acc: 0.7533\n",
      "Epoch 16/150\n",
      "1350/1350 [==============================] - 0s 30us/sample - loss: 0.0342 - acc: 0.8511 - val_loss: 0.0417 - val_acc: 0.7600\n",
      "Epoch 17/150\n",
      "1350/1350 [==============================] - 0s 30us/sample - loss: 0.0326 - acc: 0.8556 - val_loss: 0.0408 - val_acc: 0.7533\n",
      "Epoch 18/150\n",
      "1350/1350 [==============================] - 0s 31us/sample - loss: 0.0312 - acc: 0.8630 - val_loss: 0.0398 - val_acc: 0.7467\n",
      "Epoch 19/150\n",
      "1350/1350 [==============================] - 0s 31us/sample - loss: 0.0299 - acc: 0.8659 - val_loss: 0.0388 - val_acc: 0.7533\n",
      "Epoch 20/150\n",
      "1350/1350 [==============================] - 0s 30us/sample - loss: 0.0287 - acc: 0.8748 - val_loss: 0.0379 - val_acc: 0.7533\n",
      "Epoch 21/150\n",
      "1350/1350 [==============================] - 0s 30us/sample - loss: 0.0276 - acc: 0.8748 - val_loss: 0.0369 - val_acc: 0.7533\n",
      "Epoch 22/150\n",
      "1350/1350 [==============================] - 0s 31us/sample - loss: 0.0265 - acc: 0.8844 - val_loss: 0.0361 - val_acc: 0.7733\n",
      "Epoch 23/150\n",
      "1350/1350 [==============================] - 0s 29us/sample - loss: 0.0256 - acc: 0.8822 - val_loss: 0.0354 - val_acc: 0.7733\n",
      "Epoch 24/150\n",
      "1350/1350 [==============================] - 0s 31us/sample - loss: 0.0247 - acc: 0.8852 - val_loss: 0.0349 - val_acc: 0.7867\n",
      "Epoch 25/150\n",
      "1350/1350 [==============================] - 0s 31us/sample - loss: 0.0239 - acc: 0.8881 - val_loss: 0.0343 - val_acc: 0.7867\n",
      "Epoch 26/150\n",
      "1350/1350 [==============================] - 0s 30us/sample - loss: 0.0232 - acc: 0.8941 - val_loss: 0.0336 - val_acc: 0.7933\n",
      "Epoch 27/150\n",
      "1350/1350 [==============================] - 0s 31us/sample - loss: 0.0225 - acc: 0.8956 - val_loss: 0.0332 - val_acc: 0.8067\n",
      "Epoch 28/150\n",
      "1350/1350 [==============================] - 0s 30us/sample - loss: 0.0218 - acc: 0.9007 - val_loss: 0.0325 - val_acc: 0.7933\n",
      "Epoch 29/150\n",
      "1350/1350 [==============================] - 0s 29us/sample - loss: 0.0212 - acc: 0.9037 - val_loss: 0.0320 - val_acc: 0.8000\n",
      "Epoch 30/150\n",
      "1350/1350 [==============================] - 0s 30us/sample - loss: 0.0207 - acc: 0.9059 - val_loss: 0.0315 - val_acc: 0.8067\n",
      "Epoch 31/150\n",
      "1350/1350 [==============================] - 0s 32us/sample - loss: 0.0201 - acc: 0.9104 - val_loss: 0.0310 - val_acc: 0.7933\n",
      "Epoch 32/150\n",
      "1350/1350 [==============================] - 0s 30us/sample - loss: 0.0196 - acc: 0.9104 - val_loss: 0.0306 - val_acc: 0.8000\n",
      "Epoch 33/150\n",
      "1350/1350 [==============================] - 0s 30us/sample - loss: 0.0191 - acc: 0.9119 - val_loss: 0.0301 - val_acc: 0.8133\n",
      "Epoch 34/150\n",
      "1350/1350 [==============================] - 0s 32us/sample - loss: 0.0187 - acc: 0.9119 - val_loss: 0.0299 - val_acc: 0.8200\n",
      "Epoch 35/150\n",
      "1350/1350 [==============================] - 0s 30us/sample - loss: 0.0183 - acc: 0.9133 - val_loss: 0.0293 - val_acc: 0.8133\n",
      "Epoch 36/150\n",
      "1350/1350 [==============================] - 0s 30us/sample - loss: 0.0179 - acc: 0.9126 - val_loss: 0.0292 - val_acc: 0.8200\n",
      "Epoch 37/150\n",
      "1350/1350 [==============================] - 0s 31us/sample - loss: 0.0175 - acc: 0.9148 - val_loss: 0.0288 - val_acc: 0.8200\n",
      "Epoch 38/150\n",
      "1350/1350 [==============================] - 0s 32us/sample - loss: 0.0171 - acc: 0.9163 - val_loss: 0.0284 - val_acc: 0.8200\n",
      "Epoch 39/150\n",
      "1350/1350 [==============================] - 0s 30us/sample - loss: 0.0168 - acc: 0.9156 - val_loss: 0.0280 - val_acc: 0.8133\n",
      "Epoch 40/150\n",
      "1350/1350 [==============================] - 0s 32us/sample - loss: 0.0165 - acc: 0.9148 - val_loss: 0.0276 - val_acc: 0.8200\n",
      "Epoch 41/150\n",
      "1350/1350 [==============================] - 0s 31us/sample - loss: 0.0162 - acc: 0.9200 - val_loss: 0.0276 - val_acc: 0.8267\n",
      "Epoch 42/150\n",
      "1350/1350 [==============================] - 0s 32us/sample - loss: 0.0159 - acc: 0.9178 - val_loss: 0.0271 - val_acc: 0.8200\n",
      "Epoch 43/150\n",
      "1350/1350 [==============================] - 0s 32us/sample - loss: 0.0156 - acc: 0.9193 - val_loss: 0.0268 - val_acc: 0.8200\n",
      "Epoch 44/150\n",
      "1350/1350 [==============================] - 0s 31us/sample - loss: 0.0153 - acc: 0.9230 - val_loss: 0.0266 - val_acc: 0.8333\n",
      "Epoch 45/150\n",
      "1350/1350 [==============================] - 0s 31us/sample - loss: 0.0150 - acc: 0.9230 - val_loss: 0.0265 - val_acc: 0.8333\n",
      "Epoch 46/150\n",
      "1350/1350 [==============================] - 0s 32us/sample - loss: 0.0147 - acc: 0.9289 - val_loss: 0.0262 - val_acc: 0.8333\n",
      "Epoch 47/150\n",
      "1350/1350 [==============================] - 0s 31us/sample - loss: 0.0145 - acc: 0.9244 - val_loss: 0.0259 - val_acc: 0.8333\n",
      "Epoch 48/150\n",
      "1350/1350 [==============================] - 0s 30us/sample - loss: 0.0143 - acc: 0.9274 - val_loss: 0.0255 - val_acc: 0.8267\n",
      "Epoch 49/150\n",
      "1350/1350 [==============================] - 0s 30us/sample - loss: 0.0140 - acc: 0.9274 - val_loss: 0.0256 - val_acc: 0.8267\n",
      "Epoch 50/150\n",
      "1350/1350 [==============================] - 0s 32us/sample - loss: 0.0138 - acc: 0.9311 - val_loss: 0.0249 - val_acc: 0.8333\n",
      "Epoch 51/150\n",
      "1350/1350 [==============================] - 0s 30us/sample - loss: 0.0136 - acc: 0.9326 - val_loss: 0.0249 - val_acc: 0.8400\n",
      "Epoch 52/150\n",
      "1350/1350 [==============================] - 0s 34us/sample - loss: 0.0134 - acc: 0.9348 - val_loss: 0.0246 - val_acc: 0.8333\n",
      "Epoch 53/150\n",
      "1350/1350 [==============================] - 0s 31us/sample - loss: 0.0132 - acc: 0.9333 - val_loss: 0.0246 - val_acc: 0.8400\n",
      "Epoch 54/150\n",
      "1350/1350 [==============================] - 0s 31us/sample - loss: 0.0130 - acc: 0.9370 - val_loss: 0.0243 - val_acc: 0.8333\n",
      "Epoch 55/150\n",
      "1350/1350 [==============================] - 0s 31us/sample - loss: 0.0128 - acc: 0.9370 - val_loss: 0.0240 - val_acc: 0.8400\n",
      "Epoch 56/150\n",
      "1350/1350 [==============================] - 0s 29us/sample - loss: 0.0126 - acc: 0.9370 - val_loss: 0.0240 - val_acc: 0.8333\n",
      "Epoch 57/150\n",
      "1350/1350 [==============================] - 0s 32us/sample - loss: 0.0125 - acc: 0.9393 - val_loss: 0.0239 - val_acc: 0.8400\n",
      "Epoch 58/150\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1350/1350 [==============================] - 0s 30us/sample - loss: 0.0123 - acc: 0.9393 - val_loss: 0.0238 - val_acc: 0.8333\n",
      "Epoch 59/150\n",
      "1350/1350 [==============================] - 0s 31us/sample - loss: 0.0122 - acc: 0.9393 - val_loss: 0.0235 - val_acc: 0.8400\n",
      "Epoch 60/150\n",
      "1350/1350 [==============================] - 0s 31us/sample - loss: 0.0120 - acc: 0.9393 - val_loss: 0.0231 - val_acc: 0.8267\n",
      "Epoch 61/150\n",
      "1350/1350 [==============================] - 0s 33us/sample - loss: 0.0118 - acc: 0.9430 - val_loss: 0.0231 - val_acc: 0.8400\n",
      "Epoch 62/150\n",
      "1350/1350 [==============================] - 0s 32us/sample - loss: 0.0116 - acc: 0.9400 - val_loss: 0.0231 - val_acc: 0.8467\n",
      "Epoch 63/150\n",
      "1350/1350 [==============================] - 0s 32us/sample - loss: 0.0115 - acc: 0.9452 - val_loss: 0.0229 - val_acc: 0.8533\n",
      "Epoch 64/150\n",
      "1350/1350 [==============================] - 0s 32us/sample - loss: 0.0114 - acc: 0.9452 - val_loss: 0.0227 - val_acc: 0.8467\n",
      "Epoch 65/150\n",
      "1350/1350 [==============================] - 0s 31us/sample - loss: 0.0112 - acc: 0.9452 - val_loss: 0.0225 - val_acc: 0.8400\n",
      "Epoch 66/150\n",
      "1350/1350 [==============================] - 0s 32us/sample - loss: 0.0111 - acc: 0.9481 - val_loss: 0.0225 - val_acc: 0.8467\n",
      "Epoch 67/150\n",
      "1350/1350 [==============================] - 0s 32us/sample - loss: 0.0109 - acc: 0.9481 - val_loss: 0.0222 - val_acc: 0.8400\n",
      "Epoch 68/150\n",
      "1350/1350 [==============================] - 0s 31us/sample - loss: 0.0108 - acc: 0.9474 - val_loss: 0.0220 - val_acc: 0.8467\n",
      "Epoch 69/150\n",
      "1350/1350 [==============================] - 0s 30us/sample - loss: 0.0107 - acc: 0.9489 - val_loss: 0.0218 - val_acc: 0.8467\n",
      "Epoch 70/150\n",
      "1350/1350 [==============================] - 0s 32us/sample - loss: 0.0106 - acc: 0.9474 - val_loss: 0.0219 - val_acc: 0.8400\n",
      "Epoch 71/150\n",
      "1350/1350 [==============================] - 0s 32us/sample - loss: 0.0105 - acc: 0.9474 - val_loss: 0.0218 - val_acc: 0.8467\n",
      "Epoch 72/150\n",
      "1350/1350 [==============================] - 0s 33us/sample - loss: 0.0103 - acc: 0.9489 - val_loss: 0.0218 - val_acc: 0.8467\n",
      "Epoch 73/150\n",
      "1350/1350 [==============================] - 0s 33us/sample - loss: 0.0102 - acc: 0.9496 - val_loss: 0.0218 - val_acc: 0.8467\n",
      "Epoch 74/150\n",
      "1350/1350 [==============================] - 0s 32us/sample - loss: 0.0101 - acc: 0.9519 - val_loss: 0.0216 - val_acc: 0.8533\n",
      "Epoch 75/150\n",
      "1350/1350 [==============================] - 0s 30us/sample - loss: 0.0100 - acc: 0.9541 - val_loss: 0.0213 - val_acc: 0.8333\n",
      "Epoch 76/150\n",
      "1350/1350 [==============================] - 0s 30us/sample - loss: 0.0099 - acc: 0.9504 - val_loss: 0.0212 - val_acc: 0.8333\n",
      "Epoch 77/150\n",
      "1350/1350 [==============================] - 0s 31us/sample - loss: 0.0097 - acc: 0.9556 - val_loss: 0.0213 - val_acc: 0.8533\n",
      "Epoch 78/150\n",
      "1350/1350 [==============================] - 0s 30us/sample - loss: 0.0096 - acc: 0.9541 - val_loss: 0.0212 - val_acc: 0.8467\n",
      "Epoch 79/150\n",
      "1350/1350 [==============================] - 0s 33us/sample - loss: 0.0095 - acc: 0.9541 - val_loss: 0.0209 - val_acc: 0.8400\n",
      "Epoch 80/150\n",
      "1350/1350 [==============================] - 0s 32us/sample - loss: 0.0095 - acc: 0.9585 - val_loss: 0.0210 - val_acc: 0.8533\n",
      "Epoch 81/150\n",
      "1350/1350 [==============================] - 0s 30us/sample - loss: 0.0093 - acc: 0.9578 - val_loss: 0.0209 - val_acc: 0.8467\n",
      "Epoch 82/150\n",
      "1350/1350 [==============================] - 0s 32us/sample - loss: 0.0092 - acc: 0.9556 - val_loss: 0.0208 - val_acc: 0.8467\n",
      "Epoch 83/150\n",
      "1350/1350 [==============================] - 0s 32us/sample - loss: 0.0091 - acc: 0.9570 - val_loss: 0.0206 - val_acc: 0.8467\n",
      "Epoch 84/150\n",
      "1350/1350 [==============================] - 0s 30us/sample - loss: 0.0090 - acc: 0.9585 - val_loss: 0.0207 - val_acc: 0.8467\n",
      "Epoch 85/150\n",
      "1350/1350 [==============================] - 0s 30us/sample - loss: 0.0090 - acc: 0.9578 - val_loss: 0.0205 - val_acc: 0.8467\n",
      "Epoch 86/150\n",
      "1350/1350 [==============================] - 0s 30us/sample - loss: 0.0089 - acc: 0.9593 - val_loss: 0.0205 - val_acc: 0.8467\n",
      "Epoch 87/150\n",
      "1350/1350 [==============================] - 0s 31us/sample - loss: 0.0088 - acc: 0.9593 - val_loss: 0.0205 - val_acc: 0.8467\n",
      "Epoch 88/150\n",
      "1350/1350 [==============================] - 0s 31us/sample - loss: 0.0087 - acc: 0.9593 - val_loss: 0.0203 - val_acc: 0.8333\n",
      "Epoch 89/150\n",
      "1350/1350 [==============================] - 0s 30us/sample - loss: 0.0086 - acc: 0.9607 - val_loss: 0.0204 - val_acc: 0.8467\n",
      "Epoch 90/150\n",
      "1350/1350 [==============================] - 0s 30us/sample - loss: 0.0085 - acc: 0.9607 - val_loss: 0.0203 - val_acc: 0.8400\n",
      "Epoch 91/150\n",
      "1350/1350 [==============================] - 0s 30us/sample - loss: 0.0085 - acc: 0.9622 - val_loss: 0.0201 - val_acc: 0.8400\n",
      "Epoch 92/150\n",
      "1350/1350 [==============================] - 0s 31us/sample - loss: 0.0084 - acc: 0.9637 - val_loss: 0.0200 - val_acc: 0.8400\n",
      "Epoch 93/150\n",
      "1350/1350 [==============================] - 0s 30us/sample - loss: 0.0083 - acc: 0.9644 - val_loss: 0.0199 - val_acc: 0.8333\n",
      "Epoch 94/150\n",
      "1350/1350 [==============================] - 0s 30us/sample - loss: 0.0082 - acc: 0.9644 - val_loss: 0.0199 - val_acc: 0.8400\n",
      "Epoch 95/150\n",
      "1350/1350 [==============================] - 0s 30us/sample - loss: 0.0081 - acc: 0.9644 - val_loss: 0.0198 - val_acc: 0.8400\n",
      "Epoch 96/150\n",
      "1350/1350 [==============================] - 0s 30us/sample - loss: 0.0081 - acc: 0.9630 - val_loss: 0.0199 - val_acc: 0.8400\n",
      "Epoch 97/150\n",
      "1350/1350 [==============================] - 0s 31us/sample - loss: 0.0080 - acc: 0.9659 - val_loss: 0.0198 - val_acc: 0.8400\n",
      "Epoch 98/150\n",
      "1350/1350 [==============================] - 0s 31us/sample - loss: 0.0079 - acc: 0.9674 - val_loss: 0.0197 - val_acc: 0.8400\n",
      "Epoch 99/150\n",
      "1350/1350 [==============================] - 0s 31us/sample - loss: 0.0078 - acc: 0.9652 - val_loss: 0.0197 - val_acc: 0.8400\n",
      "Epoch 100/150\n",
      "1350/1350 [==============================] - 0s 30us/sample - loss: 0.0078 - acc: 0.9667 - val_loss: 0.0196 - val_acc: 0.8400\n",
      "Epoch 101/150\n",
      "1350/1350 [==============================] - 0s 31us/sample - loss: 0.0077 - acc: 0.9659 - val_loss: 0.0196 - val_acc: 0.8400\n",
      "Epoch 102/150\n",
      "1350/1350 [==============================] - 0s 34us/sample - loss: 0.0076 - acc: 0.9667 - val_loss: 0.0194 - val_acc: 0.8400\n",
      "Epoch 103/150\n",
      "1350/1350 [==============================] - 0s 31us/sample - loss: 0.0076 - acc: 0.9681 - val_loss: 0.0194 - val_acc: 0.8400\n",
      "Epoch 104/150\n",
      "1350/1350 [==============================] - 0s 33us/sample - loss: 0.0075 - acc: 0.9681 - val_loss: 0.0194 - val_acc: 0.8400\n",
      "Epoch 105/150\n",
      "1350/1350 [==============================] - 0s 33us/sample - loss: 0.0074 - acc: 0.9681 - val_loss: 0.0193 - val_acc: 0.8400\n",
      "Epoch 106/150\n",
      "1350/1350 [==============================] - 0s 31us/sample - loss: 0.0074 - acc: 0.9696 - val_loss: 0.0193 - val_acc: 0.8400\n",
      "Epoch 107/150\n",
      "1350/1350 [==============================] - 0s 31us/sample - loss: 0.0073 - acc: 0.9704 - val_loss: 0.0193 - val_acc: 0.8400\n",
      "Epoch 108/150\n",
      "1350/1350 [==============================] - 0s 30us/sample - loss: 0.0072 - acc: 0.9704 - val_loss: 0.0191 - val_acc: 0.8400\n",
      "Epoch 109/150\n",
      "1350/1350 [==============================] - 0s 33us/sample - loss: 0.0072 - acc: 0.9696 - val_loss: 0.0192 - val_acc: 0.8400\n",
      "Epoch 110/150\n",
      "1350/1350 [==============================] - 0s 29us/sample - loss: 0.0071 - acc: 0.9704 - val_loss: 0.0190 - val_acc: 0.8400\n",
      "Epoch 111/150\n",
      "1350/1350 [==============================] - 0s 32us/sample - loss: 0.0071 - acc: 0.9704 - val_loss: 0.0191 - val_acc: 0.8400\n",
      "Epoch 112/150\n",
      "1350/1350 [==============================] - 0s 31us/sample - loss: 0.0070 - acc: 0.9704 - val_loss: 0.0191 - val_acc: 0.8400\n",
      "Epoch 113/150\n",
      "1350/1350 [==============================] - 0s 31us/sample - loss: 0.0070 - acc: 0.9704 - val_loss: 0.0191 - val_acc: 0.8400\n",
      "Epoch 114/150\n",
      "1350/1350 [==============================] - 0s 30us/sample - loss: 0.0069 - acc: 0.9696 - val_loss: 0.0191 - val_acc: 0.8400\n",
      "Epoch 115/150\n",
      "1350/1350 [==============================] - 0s 30us/sample - loss: 0.0069 - acc: 0.9704 - val_loss: 0.0189 - val_acc: 0.8400\n",
      "Epoch 116/150\n",
      "1350/1350 [==============================] - 0s 30us/sample - loss: 0.0068 - acc: 0.9704 - val_loss: 0.0190 - val_acc: 0.8400\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 117/150\n",
      "1350/1350 [==============================] - 0s 32us/sample - loss: 0.0067 - acc: 0.9704 - val_loss: 0.0189 - val_acc: 0.8400\n",
      "Epoch 118/150\n",
      "1350/1350 [==============================] - 0s 29us/sample - loss: 0.0067 - acc: 0.9711 - val_loss: 0.0189 - val_acc: 0.8400\n",
      "Epoch 119/150\n",
      "1350/1350 [==============================] - 0s 31us/sample - loss: 0.0066 - acc: 0.9704 - val_loss: 0.0187 - val_acc: 0.8467\n",
      "Epoch 120/150\n",
      "1350/1350 [==============================] - 0s 30us/sample - loss: 0.0066 - acc: 0.9704 - val_loss: 0.0188 - val_acc: 0.8400\n",
      "Epoch 121/150\n",
      "1350/1350 [==============================] - 0s 28us/sample - loss: 0.0065 - acc: 0.9704 - val_loss: 0.0186 - val_acc: 0.8467\n",
      "Epoch 122/150\n",
      "1350/1350 [==============================] - 0s 30us/sample - loss: 0.0065 - acc: 0.9704 - val_loss: 0.0188 - val_acc: 0.8467\n",
      "Epoch 123/150\n",
      "1350/1350 [==============================] - 0s 30us/sample - loss: 0.0064 - acc: 0.9704 - val_loss: 0.0186 - val_acc: 0.8533\n",
      "Epoch 124/150\n",
      "1350/1350 [==============================] - 0s 33us/sample - loss: 0.0064 - acc: 0.9726 - val_loss: 0.0185 - val_acc: 0.8600\n",
      "Epoch 125/150\n",
      "1350/1350 [==============================] - 0s 31us/sample - loss: 0.0063 - acc: 0.9711 - val_loss: 0.0186 - val_acc: 0.8533\n",
      "Epoch 126/150\n",
      "1350/1350 [==============================] - 0s 32us/sample - loss: 0.0063 - acc: 0.9719 - val_loss: 0.0186 - val_acc: 0.8533\n",
      "Epoch 127/150\n",
      "1350/1350 [==============================] - 0s 32us/sample - loss: 0.0062 - acc: 0.9719 - val_loss: 0.0185 - val_acc: 0.8600\n",
      "Epoch 128/150\n",
      "1350/1350 [==============================] - 0s 30us/sample - loss: 0.0062 - acc: 0.9719 - val_loss: 0.0184 - val_acc: 0.8600\n",
      "Epoch 129/150\n",
      "1350/1350 [==============================] - 0s 31us/sample - loss: 0.0062 - acc: 0.9719 - val_loss: 0.0185 - val_acc: 0.8533\n",
      "Epoch 130/150\n",
      "1350/1350 [==============================] - 0s 30us/sample - loss: 0.0061 - acc: 0.9726 - val_loss: 0.0184 - val_acc: 0.8600\n",
      "Epoch 131/150\n",
      "1350/1350 [==============================] - 0s 31us/sample - loss: 0.0061 - acc: 0.9726 - val_loss: 0.0185 - val_acc: 0.8600\n",
      "Epoch 132/150\n",
      "1350/1350 [==============================] - 0s 30us/sample - loss: 0.0060 - acc: 0.9733 - val_loss: 0.0184 - val_acc: 0.8733\n",
      "Epoch 133/150\n",
      "1350/1350 [==============================] - 0s 31us/sample - loss: 0.0060 - acc: 0.9726 - val_loss: 0.0183 - val_acc: 0.8667\n",
      "Epoch 134/150\n",
      "1350/1350 [==============================] - 0s 31us/sample - loss: 0.0059 - acc: 0.9733 - val_loss: 0.0183 - val_acc: 0.8600\n",
      "Epoch 135/150\n",
      "1350/1350 [==============================] - 0s 32us/sample - loss: 0.0059 - acc: 0.9733 - val_loss: 0.0182 - val_acc: 0.8733\n",
      "Epoch 136/150\n",
      "1350/1350 [==============================] - 0s 33us/sample - loss: 0.0059 - acc: 0.9741 - val_loss: 0.0182 - val_acc: 0.8667\n",
      "Epoch 137/150\n",
      "1350/1350 [==============================] - 0s 29us/sample - loss: 0.0058 - acc: 0.9733 - val_loss: 0.0182 - val_acc: 0.8667\n",
      "Epoch 138/150\n",
      "1350/1350 [==============================] - 0s 31us/sample - loss: 0.0058 - acc: 0.9733 - val_loss: 0.0182 - val_acc: 0.8667\n",
      "Epoch 139/150\n",
      "1350/1350 [==============================] - 0s 31us/sample - loss: 0.0057 - acc: 0.9741 - val_loss: 0.0182 - val_acc: 0.8667\n",
      "Epoch 140/150\n",
      "1350/1350 [==============================] - 0s 31us/sample - loss: 0.0057 - acc: 0.9741 - val_loss: 0.0180 - val_acc: 0.8667\n",
      "Epoch 141/150\n",
      "1350/1350 [==============================] - 0s 32us/sample - loss: 0.0056 - acc: 0.9748 - val_loss: 0.0181 - val_acc: 0.8667\n",
      "Epoch 142/150\n",
      "1350/1350 [==============================] - 0s 30us/sample - loss: 0.0056 - acc: 0.9748 - val_loss: 0.0182 - val_acc: 0.8667\n",
      "Epoch 143/150\n",
      "1350/1350 [==============================] - 0s 30us/sample - loss: 0.0056 - acc: 0.9748 - val_loss: 0.0182 - val_acc: 0.8667\n",
      "Epoch 144/150\n",
      "1350/1350 [==============================] - 0s 32us/sample - loss: 0.0055 - acc: 0.9748 - val_loss: 0.0181 - val_acc: 0.8667\n",
      "Epoch 145/150\n",
      "1350/1350 [==============================] - 0s 30us/sample - loss: 0.0055 - acc: 0.9748 - val_loss: 0.0181 - val_acc: 0.8600\n",
      "Epoch 146/150\n",
      "1350/1350 [==============================] - 0s 29us/sample - loss: 0.0054 - acc: 0.9756 - val_loss: 0.0180 - val_acc: 0.8600\n",
      "Epoch 147/150\n",
      "1350/1350 [==============================] - 0s 32us/sample - loss: 0.0054 - acc: 0.9763 - val_loss: 0.0181 - val_acc: 0.8600\n",
      "Epoch 148/150\n",
      "1350/1350 [==============================] - 0s 31us/sample - loss: 0.0054 - acc: 0.9763 - val_loss: 0.0181 - val_acc: 0.8600\n",
      "Epoch 149/150\n",
      "1350/1350 [==============================] - 0s 32us/sample - loss: 0.0053 - acc: 0.9756 - val_loss: 0.0180 - val_acc: 0.8533\n",
      "Epoch 150/150\n",
      "1350/1350 [==============================] - 0s 30us/sample - loss: 0.0053 - acc: 0.9763 - val_loss: 0.0181 - val_acc: 0.8600\n",
      "Elasped Time: 0:00:06.902309\n",
      "Training date and time : \n",
      "2020-04-25 21:08:43\n",
      "Train on 1350 samples, validate on 150 samples\n",
      "Epoch 1/160\n",
      "1350/1350 [==============================] - 0s 150us/sample - loss: 0.0690 - acc: 0.5222 - val_loss: 0.0669 - val_acc: 0.5400\n",
      "Epoch 2/160\n",
      "1350/1350 [==============================] - 0s 31us/sample - loss: 0.0669 - acc: 0.5444 - val_loss: 0.0651 - val_acc: 0.5400\n",
      "Epoch 3/160\n",
      "1350/1350 [==============================] - 0s 33us/sample - loss: 0.0647 - acc: 0.5644 - val_loss: 0.0633 - val_acc: 0.5467\n",
      "Epoch 4/160\n",
      "1350/1350 [==============================] - 0s 31us/sample - loss: 0.0626 - acc: 0.5822 - val_loss: 0.0615 - val_acc: 0.5533\n",
      "Epoch 5/160\n",
      "1350/1350 [==============================] - 0s 30us/sample - loss: 0.0604 - acc: 0.6015 - val_loss: 0.0597 - val_acc: 0.5800\n",
      "Epoch 6/160\n",
      "1350/1350 [==============================] - 0s 31us/sample - loss: 0.0583 - acc: 0.6222 - val_loss: 0.0578 - val_acc: 0.5933\n",
      "Epoch 7/160\n",
      "1350/1350 [==============================] - 0s 32us/sample - loss: 0.0562 - acc: 0.6489 - val_loss: 0.0561 - val_acc: 0.6067\n",
      "Epoch 8/160\n",
      "1350/1350 [==============================] - 0s 31us/sample - loss: 0.0540 - acc: 0.6785 - val_loss: 0.0542 - val_acc: 0.6267\n",
      "Epoch 9/160\n",
      "1350/1350 [==============================] - 0s 31us/sample - loss: 0.0519 - acc: 0.7052 - val_loss: 0.0524 - val_acc: 0.7000\n",
      "Epoch 10/160\n",
      "1350/1350 [==============================] - 0s 34us/sample - loss: 0.0498 - acc: 0.7304 - val_loss: 0.0506 - val_acc: 0.7200\n",
      "Epoch 11/160\n",
      "1350/1350 [==============================] - 0s 30us/sample - loss: 0.0477 - acc: 0.7511 - val_loss: 0.0488 - val_acc: 0.7467\n",
      "Epoch 12/160\n",
      "1350/1350 [==============================] - 0s 30us/sample - loss: 0.0457 - acc: 0.7733 - val_loss: 0.0472 - val_acc: 0.7600\n",
      "Epoch 13/160\n",
      "1350/1350 [==============================] - 0s 34us/sample - loss: 0.0437 - acc: 0.7904 - val_loss: 0.0455 - val_acc: 0.7733\n",
      "Epoch 14/160\n",
      "1350/1350 [==============================] - 0s 30us/sample - loss: 0.0418 - acc: 0.8037 - val_loss: 0.0440 - val_acc: 0.7800\n",
      "Epoch 15/160\n",
      "1350/1350 [==============================] - 0s 32us/sample - loss: 0.0401 - acc: 0.8096 - val_loss: 0.0426 - val_acc: 0.8133\n",
      "Epoch 16/160\n",
      "1350/1350 [==============================] - 0s 32us/sample - loss: 0.0384 - acc: 0.8193 - val_loss: 0.0412 - val_acc: 0.8133\n",
      "Epoch 17/160\n",
      "1350/1350 [==============================] - 0s 32us/sample - loss: 0.0369 - acc: 0.8244 - val_loss: 0.0400 - val_acc: 0.8267\n",
      "Epoch 18/160\n",
      "1350/1350 [==============================] - 0s 31us/sample - loss: 0.0354 - acc: 0.8333 - val_loss: 0.0389 - val_acc: 0.8267\n",
      "Epoch 19/160\n",
      "1350/1350 [==============================] - 0s 31us/sample - loss: 0.0341 - acc: 0.8400 - val_loss: 0.0379 - val_acc: 0.8267\n",
      "Epoch 20/160\n",
      "1350/1350 [==============================] - 0s 32us/sample - loss: 0.0328 - acc: 0.8452 - val_loss: 0.0368 - val_acc: 0.8267\n",
      "Epoch 21/160\n",
      "1350/1350 [==============================] - 0s 33us/sample - loss: 0.0317 - acc: 0.8496 - val_loss: 0.0358 - val_acc: 0.8333\n",
      "Epoch 22/160\n",
      "1350/1350 [==============================] - 0s 31us/sample - loss: 0.0306 - acc: 0.8548 - val_loss: 0.0351 - val_acc: 0.8267\n",
      "Epoch 23/160\n",
      "1350/1350 [==============================] - 0s 32us/sample - loss: 0.0296 - acc: 0.8637 - val_loss: 0.0344 - val_acc: 0.8267\n",
      "Epoch 24/160\n",
      "1350/1350 [==============================] - 0s 32us/sample - loss: 0.0286 - acc: 0.8644 - val_loss: 0.0337 - val_acc: 0.8333\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 25/160\n",
      "1350/1350 [==============================] - 0s 30us/sample - loss: 0.0278 - acc: 0.8644 - val_loss: 0.0329 - val_acc: 0.8333\n",
      "Epoch 26/160\n",
      "1350/1350 [==============================] - 0s 31us/sample - loss: 0.0269 - acc: 0.8659 - val_loss: 0.0322 - val_acc: 0.8467\n",
      "Epoch 27/160\n",
      "1350/1350 [==============================] - 0s 32us/sample - loss: 0.0262 - acc: 0.8719 - val_loss: 0.0315 - val_acc: 0.8467\n",
      "Epoch 28/160\n",
      "1350/1350 [==============================] - 0s 30us/sample - loss: 0.0255 - acc: 0.8807 - val_loss: 0.0311 - val_acc: 0.8467\n",
      "Epoch 29/160\n",
      "1350/1350 [==============================] - 0s 32us/sample - loss: 0.0247 - acc: 0.8822 - val_loss: 0.0303 - val_acc: 0.8400\n",
      "Epoch 30/160\n",
      "1350/1350 [==============================] - 0s 32us/sample - loss: 0.0241 - acc: 0.8881 - val_loss: 0.0302 - val_acc: 0.8533\n",
      "Epoch 31/160\n",
      "1350/1350 [==============================] - 0s 30us/sample - loss: 0.0235 - acc: 0.8874 - val_loss: 0.0294 - val_acc: 0.8400\n",
      "Epoch 32/160\n",
      "1350/1350 [==============================] - 0s 31us/sample - loss: 0.0229 - acc: 0.8911 - val_loss: 0.0291 - val_acc: 0.8467\n",
      "Epoch 33/160\n",
      "1350/1350 [==============================] - 0s 30us/sample - loss: 0.0223 - acc: 0.8963 - val_loss: 0.0288 - val_acc: 0.8533\n",
      "Epoch 34/160\n",
      "1350/1350 [==============================] - 0s 31us/sample - loss: 0.0218 - acc: 0.8970 - val_loss: 0.0282 - val_acc: 0.8467\n",
      "Epoch 35/160\n",
      "1350/1350 [==============================] - 0s 32us/sample - loss: 0.0213 - acc: 0.9000 - val_loss: 0.0279 - val_acc: 0.8533\n",
      "Epoch 36/160\n",
      "1350/1350 [==============================] - 0s 31us/sample - loss: 0.0208 - acc: 0.9037 - val_loss: 0.0274 - val_acc: 0.8400\n",
      "Epoch 37/160\n",
      "1350/1350 [==============================] - 0s 30us/sample - loss: 0.0204 - acc: 0.9000 - val_loss: 0.0272 - val_acc: 0.8467\n",
      "Epoch 38/160\n",
      "1350/1350 [==============================] - 0s 31us/sample - loss: 0.0200 - acc: 0.9081 - val_loss: 0.0267 - val_acc: 0.8400\n",
      "Epoch 39/160\n",
      "1350/1350 [==============================] - 0s 33us/sample - loss: 0.0196 - acc: 0.9081 - val_loss: 0.0265 - val_acc: 0.8400\n",
      "Epoch 40/160\n",
      "1350/1350 [==============================] - 0s 32us/sample - loss: 0.0192 - acc: 0.9104 - val_loss: 0.0261 - val_acc: 0.8533\n",
      "Epoch 41/160\n",
      "1350/1350 [==============================] - 0s 31us/sample - loss: 0.0188 - acc: 0.9111 - val_loss: 0.0258 - val_acc: 0.8600\n",
      "Epoch 42/160\n",
      "1350/1350 [==============================] - 0s 30us/sample - loss: 0.0185 - acc: 0.9141 - val_loss: 0.0256 - val_acc: 0.8400\n",
      "Epoch 43/160\n",
      "1350/1350 [==============================] - 0s 31us/sample - loss: 0.0181 - acc: 0.9141 - val_loss: 0.0252 - val_acc: 0.8467\n",
      "Epoch 44/160\n",
      "1350/1350 [==============================] - 0s 34us/sample - loss: 0.0178 - acc: 0.9141 - val_loss: 0.0250 - val_acc: 0.8400\n",
      "Epoch 45/160\n",
      "1350/1350 [==============================] - 0s 31us/sample - loss: 0.0175 - acc: 0.9163 - val_loss: 0.0248 - val_acc: 0.8467\n",
      "Epoch 46/160\n",
      "1350/1350 [==============================] - 0s 32us/sample - loss: 0.0172 - acc: 0.9193 - val_loss: 0.0246 - val_acc: 0.8533\n",
      "Epoch 47/160\n",
      "1350/1350 [==============================] - 0s 32us/sample - loss: 0.0169 - acc: 0.9207 - val_loss: 0.0242 - val_acc: 0.8400\n",
      "Epoch 48/160\n",
      "1350/1350 [==============================] - 0s 32us/sample - loss: 0.0166 - acc: 0.9230 - val_loss: 0.0242 - val_acc: 0.8467\n",
      "Epoch 49/160\n",
      "1350/1350 [==============================] - 0s 33us/sample - loss: 0.0163 - acc: 0.9222 - val_loss: 0.0241 - val_acc: 0.8467\n",
      "Epoch 50/160\n",
      "1350/1350 [==============================] - 0s 31us/sample - loss: 0.0161 - acc: 0.9222 - val_loss: 0.0237 - val_acc: 0.8467\n",
      "Epoch 51/160\n",
      "1350/1350 [==============================] - 0s 32us/sample - loss: 0.0158 - acc: 0.9244 - val_loss: 0.0236 - val_acc: 0.8467\n",
      "Epoch 52/160\n",
      "1350/1350 [==============================] - 0s 34us/sample - loss: 0.0156 - acc: 0.9237 - val_loss: 0.0234 - val_acc: 0.8400\n",
      "Epoch 53/160\n",
      "1350/1350 [==============================] - 0s 31us/sample - loss: 0.0153 - acc: 0.9230 - val_loss: 0.0231 - val_acc: 0.8600\n",
      "Epoch 54/160\n",
      "1350/1350 [==============================] - 0s 29us/sample - loss: 0.0151 - acc: 0.9252 - val_loss: 0.0230 - val_acc: 0.8467\n",
      "Epoch 55/160\n",
      "1350/1350 [==============================] - 0s 32us/sample - loss: 0.0149 - acc: 0.9259 - val_loss: 0.0228 - val_acc: 0.8533\n",
      "Epoch 56/160\n",
      "1350/1350 [==============================] - 0s 32us/sample - loss: 0.0147 - acc: 0.9304 - val_loss: 0.0228 - val_acc: 0.8400\n",
      "Epoch 57/160\n",
      "1350/1350 [==============================] - 0s 31us/sample - loss: 0.0145 - acc: 0.9304 - val_loss: 0.0225 - val_acc: 0.8400\n",
      "Epoch 58/160\n",
      "1350/1350 [==============================] - 0s 31us/sample - loss: 0.0143 - acc: 0.9311 - val_loss: 0.0222 - val_acc: 0.8533\n",
      "Epoch 59/160\n",
      "1350/1350 [==============================] - 0s 30us/sample - loss: 0.0140 - acc: 0.9311 - val_loss: 0.0222 - val_acc: 0.8467\n",
      "Epoch 60/160\n",
      "1350/1350 [==============================] - 0s 30us/sample - loss: 0.0138 - acc: 0.9311 - val_loss: 0.0221 - val_acc: 0.8400\n",
      "Epoch 61/160\n",
      "1350/1350 [==============================] - 0s 30us/sample - loss: 0.0137 - acc: 0.9341 - val_loss: 0.0220 - val_acc: 0.8333\n",
      "Epoch 62/160\n",
      "1350/1350 [==============================] - 0s 32us/sample - loss: 0.0135 - acc: 0.9326 - val_loss: 0.0219 - val_acc: 0.8467\n",
      "Epoch 63/160\n",
      "1350/1350 [==============================] - 0s 31us/sample - loss: 0.0133 - acc: 0.9348 - val_loss: 0.0217 - val_acc: 0.8400\n",
      "Epoch 64/160\n",
      "1350/1350 [==============================] - 0s 30us/sample - loss: 0.0132 - acc: 0.9356 - val_loss: 0.0216 - val_acc: 0.8533\n",
      "Epoch 65/160\n",
      "1350/1350 [==============================] - 0s 30us/sample - loss: 0.0129 - acc: 0.9363 - val_loss: 0.0213 - val_acc: 0.8600\n",
      "Epoch 66/160\n",
      "1350/1350 [==============================] - 0s 29us/sample - loss: 0.0128 - acc: 0.9348 - val_loss: 0.0212 - val_acc: 0.8533\n",
      "Epoch 67/160\n",
      "1350/1350 [==============================] - 0s 31us/sample - loss: 0.0127 - acc: 0.9378 - val_loss: 0.0211 - val_acc: 0.8600\n",
      "Epoch 68/160\n",
      "1350/1350 [==============================] - 0s 30us/sample - loss: 0.0125 - acc: 0.9370 - val_loss: 0.0212 - val_acc: 0.8467\n",
      "Epoch 69/160\n",
      "1350/1350 [==============================] - 0s 32us/sample - loss: 0.0124 - acc: 0.9378 - val_loss: 0.0210 - val_acc: 0.8533\n",
      "Epoch 70/160\n",
      "1350/1350 [==============================] - 0s 31us/sample - loss: 0.0122 - acc: 0.9393 - val_loss: 0.0209 - val_acc: 0.8533\n",
      "Epoch 71/160\n",
      "1350/1350 [==============================] - 0s 31us/sample - loss: 0.0120 - acc: 0.9415 - val_loss: 0.0210 - val_acc: 0.8400\n",
      "Epoch 72/160\n",
      "1350/1350 [==============================] - 0s 30us/sample - loss: 0.0119 - acc: 0.9407 - val_loss: 0.0208 - val_acc: 0.8400\n",
      "Epoch 73/160\n",
      "1350/1350 [==============================] - 0s 31us/sample - loss: 0.0118 - acc: 0.9422 - val_loss: 0.0208 - val_acc: 0.8333\n",
      "Epoch 74/160\n",
      "1350/1350 [==============================] - 0s 31us/sample - loss: 0.0116 - acc: 0.9437 - val_loss: 0.0207 - val_acc: 0.8400\n",
      "Epoch 75/160\n",
      "1350/1350 [==============================] - 0s 29us/sample - loss: 0.0115 - acc: 0.9430 - val_loss: 0.0205 - val_acc: 0.8400\n",
      "Epoch 76/160\n",
      "1350/1350 [==============================] - 0s 32us/sample - loss: 0.0113 - acc: 0.9452 - val_loss: 0.0206 - val_acc: 0.8467\n",
      "Epoch 77/160\n",
      "1350/1350 [==============================] - 0s 30us/sample - loss: 0.0112 - acc: 0.9430 - val_loss: 0.0205 - val_acc: 0.8533\n",
      "Epoch 78/160\n",
      "1350/1350 [==============================] - 0s 30us/sample - loss: 0.0111 - acc: 0.9467 - val_loss: 0.0204 - val_acc: 0.8400\n",
      "Epoch 79/160\n",
      "1350/1350 [==============================] - 0s 33us/sample - loss: 0.0110 - acc: 0.9481 - val_loss: 0.0203 - val_acc: 0.8533\n",
      "Epoch 80/160\n",
      "1350/1350 [==============================] - 0s 32us/sample - loss: 0.0108 - acc: 0.9511 - val_loss: 0.0203 - val_acc: 0.8400\n",
      "Epoch 81/160\n",
      "1350/1350 [==============================] - 0s 31us/sample - loss: 0.0107 - acc: 0.9511 - val_loss: 0.0203 - val_acc: 0.8400\n",
      "Epoch 82/160\n",
      "1350/1350 [==============================] - 0s 32us/sample - loss: 0.0106 - acc: 0.9504 - val_loss: 0.0202 - val_acc: 0.8467\n",
      "Epoch 83/160\n",
      "1350/1350 [==============================] - 0s 31us/sample - loss: 0.0105 - acc: 0.9511 - val_loss: 0.0201 - val_acc: 0.8533\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 84/160\n",
      "1350/1350 [==============================] - 0s 32us/sample - loss: 0.0104 - acc: 0.9533 - val_loss: 0.0203 - val_acc: 0.8400\n",
      "Epoch 85/160\n",
      "1350/1350 [==============================] - 0s 31us/sample - loss: 0.0103 - acc: 0.9533 - val_loss: 0.0200 - val_acc: 0.8533\n",
      "Epoch 86/160\n",
      "1350/1350 [==============================] - 0s 32us/sample - loss: 0.0101 - acc: 0.9533 - val_loss: 0.0200 - val_acc: 0.8467\n",
      "Epoch 87/160\n",
      "1350/1350 [==============================] - 0s 30us/sample - loss: 0.0100 - acc: 0.9541 - val_loss: 0.0198 - val_acc: 0.8600\n",
      "Epoch 88/160\n",
      "1350/1350 [==============================] - 0s 30us/sample - loss: 0.0099 - acc: 0.9548 - val_loss: 0.0199 - val_acc: 0.8533\n",
      "Epoch 89/160\n",
      "1350/1350 [==============================] - 0s 30us/sample - loss: 0.0098 - acc: 0.9548 - val_loss: 0.0199 - val_acc: 0.8467\n",
      "Epoch 90/160\n",
      "1350/1350 [==============================] - 0s 30us/sample - loss: 0.0097 - acc: 0.9556 - val_loss: 0.0200 - val_acc: 0.8467\n",
      "Epoch 91/160\n",
      "1350/1350 [==============================] - 0s 34us/sample - loss: 0.0096 - acc: 0.9563 - val_loss: 0.0198 - val_acc: 0.8533\n",
      "Epoch 92/160\n",
      "1350/1350 [==============================] - 0s 32us/sample - loss: 0.0095 - acc: 0.9533 - val_loss: 0.0198 - val_acc: 0.8600\n",
      "Epoch 93/160\n",
      "1350/1350 [==============================] - 0s 31us/sample - loss: 0.0094 - acc: 0.9563 - val_loss: 0.0197 - val_acc: 0.8533\n",
      "Epoch 94/160\n",
      "1350/1350 [==============================] - 0s 31us/sample - loss: 0.0093 - acc: 0.9570 - val_loss: 0.0196 - val_acc: 0.8533\n",
      "Epoch 95/160\n",
      "1350/1350 [==============================] - 0s 32us/sample - loss: 0.0092 - acc: 0.9578 - val_loss: 0.0196 - val_acc: 0.8600\n",
      "Epoch 96/160\n",
      "1350/1350 [==============================] - 0s 31us/sample - loss: 0.0091 - acc: 0.9600 - val_loss: 0.0196 - val_acc: 0.8667\n",
      "Epoch 97/160\n",
      "1350/1350 [==============================] - 0s 31us/sample - loss: 0.0091 - acc: 0.9600 - val_loss: 0.0197 - val_acc: 0.8467\n",
      "Epoch 98/160\n",
      "1350/1350 [==============================] - 0s 34us/sample - loss: 0.0090 - acc: 0.9585 - val_loss: 0.0196 - val_acc: 0.8467\n",
      "Epoch 99/160\n",
      "1350/1350 [==============================] - 0s 30us/sample - loss: 0.0089 - acc: 0.9600 - val_loss: 0.0196 - val_acc: 0.8600\n",
      "Epoch 100/160\n",
      "1350/1350 [==============================] - 0s 33us/sample - loss: 0.0088 - acc: 0.9622 - val_loss: 0.0196 - val_acc: 0.8600\n",
      "Epoch 101/160\n",
      "1350/1350 [==============================] - 0s 30us/sample - loss: 0.0087 - acc: 0.9630 - val_loss: 0.0196 - val_acc: 0.8467\n",
      "Epoch 102/160\n",
      "1350/1350 [==============================] - 0s 31us/sample - loss: 0.0086 - acc: 0.9644 - val_loss: 0.0195 - val_acc: 0.8533\n",
      "Epoch 103/160\n",
      "1350/1350 [==============================] - 0s 31us/sample - loss: 0.0085 - acc: 0.9615 - val_loss: 0.0194 - val_acc: 0.8733\n",
      "Epoch 104/160\n",
      "1350/1350 [==============================] - 0s 31us/sample - loss: 0.0084 - acc: 0.9637 - val_loss: 0.0195 - val_acc: 0.8533\n",
      "Epoch 105/160\n",
      "1350/1350 [==============================] - 0s 31us/sample - loss: 0.0084 - acc: 0.9652 - val_loss: 0.0194 - val_acc: 0.8600\n",
      "Epoch 106/160\n",
      "1350/1350 [==============================] - 0s 31us/sample - loss: 0.0083 - acc: 0.9644 - val_loss: 0.0196 - val_acc: 0.8467\n",
      "Epoch 107/160\n",
      "1350/1350 [==============================] - 0s 31us/sample - loss: 0.0082 - acc: 0.9644 - val_loss: 0.0194 - val_acc: 0.8533\n",
      "Epoch 108/160\n",
      "1350/1350 [==============================] - 0s 31us/sample - loss: 0.0081 - acc: 0.9667 - val_loss: 0.0194 - val_acc: 0.8533\n",
      "Epoch 109/160\n",
      "1350/1350 [==============================] - 0s 30us/sample - loss: 0.0081 - acc: 0.9667 - val_loss: 0.0194 - val_acc: 0.8467\n",
      "Epoch 110/160\n",
      "1350/1350 [==============================] - 0s 30us/sample - loss: 0.0080 - acc: 0.9667 - val_loss: 0.0194 - val_acc: 0.8533\n",
      "Epoch 111/160\n",
      "1350/1350 [==============================] - 0s 31us/sample - loss: 0.0079 - acc: 0.9696 - val_loss: 0.0193 - val_acc: 0.8667\n",
      "Epoch 112/160\n",
      "1350/1350 [==============================] - 0s 32us/sample - loss: 0.0078 - acc: 0.9689 - val_loss: 0.0194 - val_acc: 0.8600\n",
      "Epoch 113/160\n",
      "1350/1350 [==============================] - 0s 32us/sample - loss: 0.0078 - acc: 0.9696 - val_loss: 0.0194 - val_acc: 0.8600\n",
      "Epoch 114/160\n",
      "1350/1350 [==============================] - 0s 32us/sample - loss: 0.0077 - acc: 0.9689 - val_loss: 0.0192 - val_acc: 0.8733\n",
      "Epoch 115/160\n",
      "1350/1350 [==============================] - 0s 30us/sample - loss: 0.0076 - acc: 0.9696 - val_loss: 0.0192 - val_acc: 0.8733\n",
      "Epoch 116/160\n",
      "1350/1350 [==============================] - 0s 29us/sample - loss: 0.0075 - acc: 0.9696 - val_loss: 0.0192 - val_acc: 0.8600\n",
      "Epoch 117/160\n",
      "1350/1350 [==============================] - 0s 30us/sample - loss: 0.0075 - acc: 0.9704 - val_loss: 0.0194 - val_acc: 0.8533\n",
      "Epoch 118/160\n",
      "1350/1350 [==============================] - 0s 31us/sample - loss: 0.0074 - acc: 0.9711 - val_loss: 0.0193 - val_acc: 0.8600\n",
      "Epoch 119/160\n",
      "1350/1350 [==============================] - 0s 30us/sample - loss: 0.0073 - acc: 0.9696 - val_loss: 0.0194 - val_acc: 0.8600\n",
      "Epoch 120/160\n",
      "1350/1350 [==============================] - 0s 32us/sample - loss: 0.0073 - acc: 0.9711 - val_loss: 0.0192 - val_acc: 0.8600\n",
      "Epoch 121/160\n",
      "1350/1350 [==============================] - 0s 32us/sample - loss: 0.0072 - acc: 0.9689 - val_loss: 0.0193 - val_acc: 0.8600\n",
      "Epoch 122/160\n",
      "1350/1350 [==============================] - 0s 31us/sample - loss: 0.0071 - acc: 0.9704 - val_loss: 0.0192 - val_acc: 0.8667\n",
      "Epoch 123/160\n",
      "1350/1350 [==============================] - 0s 32us/sample - loss: 0.0071 - acc: 0.9704 - val_loss: 0.0193 - val_acc: 0.8600\n",
      "Epoch 124/160\n",
      "1350/1350 [==============================] - 0s 31us/sample - loss: 0.0070 - acc: 0.9719 - val_loss: 0.0191 - val_acc: 0.8733\n",
      "Epoch 125/160\n",
      "1350/1350 [==============================] - 0s 30us/sample - loss: 0.0070 - acc: 0.9711 - val_loss: 0.0191 - val_acc: 0.8733\n",
      "Epoch 126/160\n",
      "1350/1350 [==============================] - 0s 31us/sample - loss: 0.0069 - acc: 0.9719 - val_loss: 0.0192 - val_acc: 0.8600\n",
      "Epoch 127/160\n",
      "1350/1350 [==============================] - 0s 32us/sample - loss: 0.0068 - acc: 0.9719 - val_loss: 0.0191 - val_acc: 0.8733\n",
      "Epoch 128/160\n",
      "1350/1350 [==============================] - 0s 33us/sample - loss: 0.0068 - acc: 0.9711 - val_loss: 0.0191 - val_acc: 0.8600\n",
      "Epoch 129/160\n",
      "1350/1350 [==============================] - 0s 33us/sample - loss: 0.0067 - acc: 0.9711 - val_loss: 0.0192 - val_acc: 0.8600\n",
      "Epoch 130/160\n",
      "1350/1350 [==============================] - 0s 32us/sample - loss: 0.0067 - acc: 0.9726 - val_loss: 0.0191 - val_acc: 0.8667\n",
      "Epoch 131/160\n",
      "1350/1350 [==============================] - 0s 31us/sample - loss: 0.0066 - acc: 0.9726 - val_loss: 0.0191 - val_acc: 0.8733\n",
      "Epoch 132/160\n",
      "1350/1350 [==============================] - 0s 32us/sample - loss: 0.0065 - acc: 0.9726 - val_loss: 0.0191 - val_acc: 0.8733\n",
      "Epoch 133/160\n",
      "1350/1350 [==============================] - 0s 32us/sample - loss: 0.0065 - acc: 0.9726 - val_loss: 0.0191 - val_acc: 0.8667\n",
      "Epoch 134/160\n",
      "1350/1350 [==============================] - 0s 30us/sample - loss: 0.0065 - acc: 0.9733 - val_loss: 0.0191 - val_acc: 0.8600\n",
      "Epoch 135/160\n",
      "1350/1350 [==============================] - 0s 31us/sample - loss: 0.0064 - acc: 0.9733 - val_loss: 0.0190 - val_acc: 0.8733\n",
      "Epoch 136/160\n",
      "1350/1350 [==============================] - 0s 30us/sample - loss: 0.0064 - acc: 0.9726 - val_loss: 0.0191 - val_acc: 0.8600\n",
      "Epoch 137/160\n",
      "1350/1350 [==============================] - 0s 33us/sample - loss: 0.0063 - acc: 0.9733 - val_loss: 0.0192 - val_acc: 0.8733\n",
      "Epoch 138/160\n",
      "1350/1350 [==============================] - 0s 31us/sample - loss: 0.0062 - acc: 0.9741 - val_loss: 0.0192 - val_acc: 0.8600\n",
      "Epoch 139/160\n",
      "1350/1350 [==============================] - 0s 31us/sample - loss: 0.0062 - acc: 0.9733 - val_loss: 0.0191 - val_acc: 0.8733\n",
      "Epoch 140/160\n",
      "1350/1350 [==============================] - 0s 32us/sample - loss: 0.0061 - acc: 0.9748 - val_loss: 0.0191 - val_acc: 0.8667\n",
      "Epoch 141/160\n",
      "1350/1350 [==============================] - 0s 30us/sample - loss: 0.0061 - acc: 0.9741 - val_loss: 0.0191 - val_acc: 0.8733\n",
      "Epoch 142/160\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1350/1350 [==============================] - 0s 30us/sample - loss: 0.0060 - acc: 0.9748 - val_loss: 0.0190 - val_acc: 0.8733\n",
      "Epoch 143/160\n",
      "1350/1350 [==============================] - 0s 31us/sample - loss: 0.0060 - acc: 0.9748 - val_loss: 0.0191 - val_acc: 0.8533\n",
      "Epoch 144/160\n",
      "1350/1350 [==============================] - 0s 32us/sample - loss: 0.0059 - acc: 0.9756 - val_loss: 0.0190 - val_acc: 0.8733\n",
      "Epoch 145/160\n",
      "1350/1350 [==============================] - 0s 30us/sample - loss: 0.0059 - acc: 0.9748 - val_loss: 0.0191 - val_acc: 0.8733\n",
      "Epoch 146/160\n",
      "1350/1350 [==============================] - 0s 31us/sample - loss: 0.0058 - acc: 0.9756 - val_loss: 0.0190 - val_acc: 0.8800\n",
      "Epoch 147/160\n",
      "1350/1350 [==============================] - 0s 32us/sample - loss: 0.0058 - acc: 0.9763 - val_loss: 0.0191 - val_acc: 0.8667\n",
      "Epoch 148/160\n",
      "1350/1350 [==============================] - 0s 29us/sample - loss: 0.0058 - acc: 0.9763 - val_loss: 0.0191 - val_acc: 0.8733\n",
      "Epoch 149/160\n",
      "1350/1350 [==============================] - 0s 31us/sample - loss: 0.0057 - acc: 0.9756 - val_loss: 0.0191 - val_acc: 0.8800\n",
      "Epoch 150/160\n",
      "1350/1350 [==============================] - 0s 30us/sample - loss: 0.0057 - acc: 0.9756 - val_loss: 0.0191 - val_acc: 0.8667\n",
      "Epoch 151/160\n",
      "1350/1350 [==============================] - 0s 33us/sample - loss: 0.0056 - acc: 0.9763 - val_loss: 0.0191 - val_acc: 0.8800\n",
      "Epoch 152/160\n",
      "1350/1350 [==============================] - 0s 28us/sample - loss: 0.0056 - acc: 0.9770 - val_loss: 0.0190 - val_acc: 0.8800\n",
      "Epoch 153/160\n",
      "1350/1350 [==============================] - 0s 30us/sample - loss: 0.0055 - acc: 0.9763 - val_loss: 0.0191 - val_acc: 0.8733\n",
      "Epoch 154/160\n",
      "1350/1350 [==============================] - 0s 30us/sample - loss: 0.0055 - acc: 0.9763 - val_loss: 0.0190 - val_acc: 0.8733\n",
      "Epoch 155/160\n",
      "1350/1350 [==============================] - 0s 30us/sample - loss: 0.0055 - acc: 0.9770 - val_loss: 0.0191 - val_acc: 0.8600\n",
      "Epoch 156/160\n",
      "1350/1350 [==============================] - 0s 30us/sample - loss: 0.0054 - acc: 0.9770 - val_loss: 0.0190 - val_acc: 0.8800\n",
      "Epoch 157/160\n",
      "1350/1350 [==============================] - 0s 31us/sample - loss: 0.0054 - acc: 0.9770 - val_loss: 0.0191 - val_acc: 0.8667\n",
      "Epoch 158/160\n",
      "1350/1350 [==============================] - 0s 30us/sample - loss: 0.0054 - acc: 0.9763 - val_loss: 0.0190 - val_acc: 0.8667\n",
      "Epoch 159/160\n",
      "1350/1350 [==============================] - 0s 30us/sample - loss: 0.0053 - acc: 0.9763 - val_loss: 0.0190 - val_acc: 0.8667\n",
      "Epoch 160/160\n",
      "1350/1350 [==============================] - 0s 30us/sample - loss: 0.0053 - acc: 0.9770 - val_loss: 0.0189 - val_acc: 0.8800\n",
      "Elasped Time: 0:00:07.383605\n",
      "Training date and time : \n",
      "2020-04-25 21:08:51\n",
      "Train on 1350 samples, validate on 150 samples\n",
      "Epoch 1/170\n",
      "1350/1350 [==============================] - 0s 152us/sample - loss: 0.0686 - acc: 0.5281 - val_loss: 0.0617 - val_acc: 0.5733\n",
      "Epoch 2/170\n",
      "1350/1350 [==============================] - 0s 33us/sample - loss: 0.0665 - acc: 0.5415 - val_loss: 0.0594 - val_acc: 0.5800\n",
      "Epoch 3/170\n",
      "1350/1350 [==============================] - 0s 30us/sample - loss: 0.0642 - acc: 0.5585 - val_loss: 0.0573 - val_acc: 0.5867\n",
      "Epoch 4/170\n",
      "1350/1350 [==============================] - 0s 32us/sample - loss: 0.0620 - acc: 0.5719 - val_loss: 0.0550 - val_acc: 0.6067\n",
      "Epoch 5/170\n",
      "1350/1350 [==============================] - 0s 31us/sample - loss: 0.0598 - acc: 0.5970 - val_loss: 0.0529 - val_acc: 0.6267\n",
      "Epoch 6/170\n",
      "1350/1350 [==============================] - 0s 30us/sample - loss: 0.0576 - acc: 0.6119 - val_loss: 0.0507 - val_acc: 0.6333\n",
      "Epoch 7/170\n",
      "1350/1350 [==============================] - 0s 32us/sample - loss: 0.0555 - acc: 0.6296 - val_loss: 0.0487 - val_acc: 0.6800\n",
      "Epoch 8/170\n",
      "1350/1350 [==============================] - 0s 32us/sample - loss: 0.0535 - acc: 0.6511 - val_loss: 0.0465 - val_acc: 0.7067\n",
      "Epoch 9/170\n",
      "1350/1350 [==============================] - 0s 31us/sample - loss: 0.0514 - acc: 0.6889 - val_loss: 0.0446 - val_acc: 0.7400\n",
      "Epoch 10/170\n",
      "1350/1350 [==============================] - 0s 32us/sample - loss: 0.0494 - acc: 0.7185 - val_loss: 0.0424 - val_acc: 0.7933\n",
      "Epoch 11/170\n",
      "1350/1350 [==============================] - 0s 32us/sample - loss: 0.0474 - acc: 0.7370 - val_loss: 0.0404 - val_acc: 0.8133\n",
      "Epoch 12/170\n",
      "1350/1350 [==============================] - 0s 30us/sample - loss: 0.0455 - acc: 0.7556 - val_loss: 0.0387 - val_acc: 0.8067\n",
      "Epoch 13/170\n",
      "1350/1350 [==============================] - 0s 30us/sample - loss: 0.0437 - acc: 0.7748 - val_loss: 0.0368 - val_acc: 0.8333\n",
      "Epoch 14/170\n",
      "1350/1350 [==============================] - 0s 31us/sample - loss: 0.0419 - acc: 0.7785 - val_loss: 0.0349 - val_acc: 0.8467\n",
      "Epoch 15/170\n",
      "1350/1350 [==============================] - 0s 32us/sample - loss: 0.0402 - acc: 0.7919 - val_loss: 0.0334 - val_acc: 0.8667\n",
      "Epoch 16/170\n",
      "1350/1350 [==============================] - 0s 30us/sample - loss: 0.0386 - acc: 0.7978 - val_loss: 0.0318 - val_acc: 0.8733\n",
      "Epoch 17/170\n",
      "1350/1350 [==============================] - 0s 30us/sample - loss: 0.0371 - acc: 0.8081 - val_loss: 0.0302 - val_acc: 0.8733\n",
      "Epoch 18/170\n",
      "1350/1350 [==============================] - 0s 32us/sample - loss: 0.0357 - acc: 0.8119 - val_loss: 0.0290 - val_acc: 0.8867\n",
      "Epoch 19/170\n",
      "1350/1350 [==============================] - 0s 33us/sample - loss: 0.0343 - acc: 0.8163 - val_loss: 0.0278 - val_acc: 0.8733\n",
      "Epoch 20/170\n",
      "1350/1350 [==============================] - 0s 31us/sample - loss: 0.0331 - acc: 0.8237 - val_loss: 0.0268 - val_acc: 0.9067\n",
      "Epoch 21/170\n",
      "1350/1350 [==============================] - 0s 31us/sample - loss: 0.0320 - acc: 0.8348 - val_loss: 0.0259 - val_acc: 0.9000\n",
      "Epoch 22/170\n",
      "1350/1350 [==============================] - 0s 30us/sample - loss: 0.0309 - acc: 0.8378 - val_loss: 0.0251 - val_acc: 0.9000\n",
      "Epoch 23/170\n",
      "1350/1350 [==============================] - 0s 29us/sample - loss: 0.0300 - acc: 0.8444 - val_loss: 0.0244 - val_acc: 0.9000\n",
      "Epoch 24/170\n",
      "1350/1350 [==============================] - 0s 31us/sample - loss: 0.0291 - acc: 0.8481 - val_loss: 0.0237 - val_acc: 0.8933\n",
      "Epoch 25/170\n",
      "1350/1350 [==============================] - 0s 33us/sample - loss: 0.0283 - acc: 0.8489 - val_loss: 0.0228 - val_acc: 0.9067\n",
      "Epoch 26/170\n",
      "1350/1350 [==============================] - 0s 30us/sample - loss: 0.0274 - acc: 0.8548 - val_loss: 0.0221 - val_acc: 0.9067\n",
      "Epoch 27/170\n",
      "1350/1350 [==============================] - 0s 32us/sample - loss: 0.0267 - acc: 0.8585 - val_loss: 0.0215 - val_acc: 0.9000\n",
      "Epoch 28/170\n",
      "1350/1350 [==============================] - 0s 30us/sample - loss: 0.0261 - acc: 0.8593 - val_loss: 0.0212 - val_acc: 0.9000\n",
      "Epoch 29/170\n",
      "1350/1350 [==============================] - 0s 31us/sample - loss: 0.0254 - acc: 0.8644 - val_loss: 0.0206 - val_acc: 0.9067\n",
      "Epoch 30/170\n",
      "1350/1350 [==============================] - 0s 30us/sample - loss: 0.0248 - acc: 0.8637 - val_loss: 0.0199 - val_acc: 0.9000\n",
      "Epoch 31/170\n",
      "1350/1350 [==============================] - 0s 34us/sample - loss: 0.0242 - acc: 0.8696 - val_loss: 0.0196 - val_acc: 0.9133\n",
      "Epoch 32/170\n",
      "1350/1350 [==============================] - 0s 29us/sample - loss: 0.0237 - acc: 0.8733 - val_loss: 0.0193 - val_acc: 0.9067\n",
      "Epoch 33/170\n",
      "1350/1350 [==============================] - 0s 32us/sample - loss: 0.0231 - acc: 0.8741 - val_loss: 0.0192 - val_acc: 0.9133\n",
      "Epoch 34/170\n",
      "1350/1350 [==============================] - 0s 30us/sample - loss: 0.0227 - acc: 0.8763 - val_loss: 0.0188 - val_acc: 0.9067\n",
      "Epoch 35/170\n",
      "1350/1350 [==============================] - 0s 31us/sample - loss: 0.0222 - acc: 0.8793 - val_loss: 0.0184 - val_acc: 0.9067\n",
      "Epoch 36/170\n",
      "1350/1350 [==============================] - 0s 31us/sample - loss: 0.0218 - acc: 0.8830 - val_loss: 0.0180 - val_acc: 0.9067\n",
      "Epoch 37/170\n",
      "1350/1350 [==============================] - 0s 30us/sample - loss: 0.0213 - acc: 0.8859 - val_loss: 0.0177 - val_acc: 0.9067\n",
      "Epoch 38/170\n",
      "1350/1350 [==============================] - 0s 31us/sample - loss: 0.0209 - acc: 0.8911 - val_loss: 0.0174 - val_acc: 0.9067\n",
      "Epoch 39/170\n",
      "1350/1350 [==============================] - 0s 31us/sample - loss: 0.0206 - acc: 0.8904 - val_loss: 0.0172 - val_acc: 0.9133\n",
      "Epoch 40/170\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1350/1350 [==============================] - 0s 30us/sample - loss: 0.0202 - acc: 0.8970 - val_loss: 0.0170 - val_acc: 0.9133\n",
      "Epoch 41/170\n",
      "1350/1350 [==============================] - 0s 31us/sample - loss: 0.0199 - acc: 0.8970 - val_loss: 0.0167 - val_acc: 0.9067\n",
      "Epoch 42/170\n",
      "1350/1350 [==============================] - 0s 33us/sample - loss: 0.0195 - acc: 0.8978 - val_loss: 0.0162 - val_acc: 0.9200\n",
      "Epoch 43/170\n",
      "1350/1350 [==============================] - 0s 32us/sample - loss: 0.0191 - acc: 0.9015 - val_loss: 0.0163 - val_acc: 0.9200\n",
      "Epoch 44/170\n",
      "1350/1350 [==============================] - 0s 31us/sample - loss: 0.0188 - acc: 0.8985 - val_loss: 0.0160 - val_acc: 0.9200\n",
      "Epoch 45/170\n",
      "1350/1350 [==============================] - 0s 32us/sample - loss: 0.0185 - acc: 0.9037 - val_loss: 0.0159 - val_acc: 0.9267\n",
      "Epoch 46/170\n",
      "1350/1350 [==============================] - 0s 31us/sample - loss: 0.0182 - acc: 0.9044 - val_loss: 0.0156 - val_acc: 0.9200\n",
      "Epoch 47/170\n",
      "1350/1350 [==============================] - 0s 31us/sample - loss: 0.0180 - acc: 0.9059 - val_loss: 0.0155 - val_acc: 0.9200\n",
      "Epoch 48/170\n",
      "1350/1350 [==============================] - 0s 32us/sample - loss: 0.0176 - acc: 0.9096 - val_loss: 0.0155 - val_acc: 0.9267\n",
      "Epoch 49/170\n",
      "1350/1350 [==============================] - 0s 30us/sample - loss: 0.0173 - acc: 0.9133 - val_loss: 0.0150 - val_acc: 0.9200\n",
      "Epoch 50/170\n",
      "1350/1350 [==============================] - 0s 31us/sample - loss: 0.0171 - acc: 0.9096 - val_loss: 0.0148 - val_acc: 0.9200\n",
      "Epoch 51/170\n",
      "1350/1350 [==============================] - 0s 31us/sample - loss: 0.0168 - acc: 0.9141 - val_loss: 0.0149 - val_acc: 0.9267\n",
      "Epoch 52/170\n",
      "1350/1350 [==============================] - 0s 30us/sample - loss: 0.0166 - acc: 0.9185 - val_loss: 0.0146 - val_acc: 0.9200\n",
      "Epoch 53/170\n",
      "1350/1350 [==============================] - 0s 31us/sample - loss: 0.0163 - acc: 0.9170 - val_loss: 0.0146 - val_acc: 0.9333\n",
      "Epoch 54/170\n",
      "1350/1350 [==============================] - 0s 31us/sample - loss: 0.0161 - acc: 0.9170 - val_loss: 0.0144 - val_acc: 0.9200\n",
      "Epoch 55/170\n",
      "1350/1350 [==============================] - 0s 31us/sample - loss: 0.0159 - acc: 0.9193 - val_loss: 0.0144 - val_acc: 0.9333\n",
      "Epoch 56/170\n",
      "1350/1350 [==============================] - 0s 32us/sample - loss: 0.0157 - acc: 0.9170 - val_loss: 0.0141 - val_acc: 0.9267\n",
      "Epoch 57/170\n",
      "1350/1350 [==============================] - 0s 29us/sample - loss: 0.0154 - acc: 0.9222 - val_loss: 0.0140 - val_acc: 0.9267\n",
      "Epoch 58/170\n",
      "1350/1350 [==============================] - 0s 32us/sample - loss: 0.0152 - acc: 0.9207 - val_loss: 0.0140 - val_acc: 0.9200\n",
      "Epoch 59/170\n",
      "1350/1350 [==============================] - 0s 30us/sample - loss: 0.0150 - acc: 0.9215 - val_loss: 0.0137 - val_acc: 0.9200\n",
      "Epoch 60/170\n",
      "1350/1350 [==============================] - 0s 31us/sample - loss: 0.0148 - acc: 0.9222 - val_loss: 0.0138 - val_acc: 0.9333\n",
      "Epoch 61/170\n",
      "1350/1350 [==============================] - 0s 31us/sample - loss: 0.0146 - acc: 0.9230 - val_loss: 0.0136 - val_acc: 0.9267\n",
      "Epoch 62/170\n",
      "1350/1350 [==============================] - 0s 31us/sample - loss: 0.0144 - acc: 0.9244 - val_loss: 0.0135 - val_acc: 0.9267\n",
      "Epoch 63/170\n",
      "1350/1350 [==============================] - 0s 29us/sample - loss: 0.0142 - acc: 0.9237 - val_loss: 0.0132 - val_acc: 0.9267\n",
      "Epoch 64/170\n",
      "1350/1350 [==============================] - 0s 32us/sample - loss: 0.0140 - acc: 0.9267 - val_loss: 0.0133 - val_acc: 0.9267\n",
      "Epoch 65/170\n",
      "1350/1350 [==============================] - 0s 31us/sample - loss: 0.0139 - acc: 0.9289 - val_loss: 0.0132 - val_acc: 0.9267\n",
      "Epoch 66/170\n",
      "1350/1350 [==============================] - 0s 32us/sample - loss: 0.0137 - acc: 0.9289 - val_loss: 0.0132 - val_acc: 0.9267\n",
      "Epoch 67/170\n",
      "1350/1350 [==============================] - 0s 32us/sample - loss: 0.0135 - acc: 0.9281 - val_loss: 0.0131 - val_acc: 0.9267\n",
      "Epoch 68/170\n",
      "1350/1350 [==============================] - 0s 32us/sample - loss: 0.0133 - acc: 0.9311 - val_loss: 0.0130 - val_acc: 0.9333\n",
      "Epoch 69/170\n",
      "1350/1350 [==============================] - 0s 30us/sample - loss: 0.0132 - acc: 0.9319 - val_loss: 0.0130 - val_acc: 0.9267\n",
      "Epoch 70/170\n",
      "1350/1350 [==============================] - 0s 32us/sample - loss: 0.0130 - acc: 0.9370 - val_loss: 0.0129 - val_acc: 0.9267\n",
      "Epoch 71/170\n",
      "1350/1350 [==============================] - 0s 32us/sample - loss: 0.0128 - acc: 0.9370 - val_loss: 0.0128 - val_acc: 0.9267\n",
      "Epoch 72/170\n",
      "1350/1350 [==============================] - 0s 32us/sample - loss: 0.0127 - acc: 0.9363 - val_loss: 0.0127 - val_acc: 0.9267\n",
      "Epoch 73/170\n",
      "1350/1350 [==============================] - 0s 31us/sample - loss: 0.0125 - acc: 0.9400 - val_loss: 0.0127 - val_acc: 0.9267\n",
      "Epoch 74/170\n",
      "1350/1350 [==============================] - 0s 32us/sample - loss: 0.0124 - acc: 0.9430 - val_loss: 0.0125 - val_acc: 0.9267\n",
      "Epoch 75/170\n",
      "1350/1350 [==============================] - 0s 31us/sample - loss: 0.0122 - acc: 0.9400 - val_loss: 0.0125 - val_acc: 0.9267\n",
      "Epoch 76/170\n",
      "1350/1350 [==============================] - 0s 32us/sample - loss: 0.0121 - acc: 0.9437 - val_loss: 0.0125 - val_acc: 0.9267\n",
      "Epoch 77/170\n",
      "1350/1350 [==============================] - 0s 31us/sample - loss: 0.0119 - acc: 0.9459 - val_loss: 0.0124 - val_acc: 0.9267\n",
      "Epoch 78/170\n",
      "1350/1350 [==============================] - 0s 30us/sample - loss: 0.0118 - acc: 0.9459 - val_loss: 0.0123 - val_acc: 0.9267\n",
      "Epoch 79/170\n",
      "1350/1350 [==============================] - 0s 30us/sample - loss: 0.0116 - acc: 0.9444 - val_loss: 0.0124 - val_acc: 0.9267\n",
      "Epoch 80/170\n",
      "1350/1350 [==============================] - 0s 30us/sample - loss: 0.0115 - acc: 0.9489 - val_loss: 0.0121 - val_acc: 0.9267\n",
      "Epoch 81/170\n",
      "1350/1350 [==============================] - 0s 32us/sample - loss: 0.0113 - acc: 0.9459 - val_loss: 0.0123 - val_acc: 0.9267\n",
      "Epoch 82/170\n",
      "1350/1350 [==============================] - 0s 31us/sample - loss: 0.0112 - acc: 0.9474 - val_loss: 0.0120 - val_acc: 0.9267\n",
      "Epoch 83/170\n",
      "1350/1350 [==============================] - 0s 31us/sample - loss: 0.0111 - acc: 0.9504 - val_loss: 0.0120 - val_acc: 0.9267\n",
      "Epoch 84/170\n",
      "1350/1350 [==============================] - 0s 32us/sample - loss: 0.0110 - acc: 0.9489 - val_loss: 0.0120 - val_acc: 0.9267\n",
      "Epoch 85/170\n",
      "1350/1350 [==============================] - 0s 32us/sample - loss: 0.0109 - acc: 0.9489 - val_loss: 0.0118 - val_acc: 0.9267\n",
      "Epoch 86/170\n",
      "1350/1350 [==============================] - 0s 33us/sample - loss: 0.0107 - acc: 0.9511 - val_loss: 0.0119 - val_acc: 0.9267\n",
      "Epoch 87/170\n",
      "1350/1350 [==============================] - 0s 31us/sample - loss: 0.0106 - acc: 0.9511 - val_loss: 0.0118 - val_acc: 0.9267\n",
      "Epoch 88/170\n",
      "1350/1350 [==============================] - 0s 31us/sample - loss: 0.0105 - acc: 0.9504 - val_loss: 0.0118 - val_acc: 0.9267\n",
      "Epoch 89/170\n",
      "1350/1350 [==============================] - 0s 33us/sample - loss: 0.0104 - acc: 0.9548 - val_loss: 0.0118 - val_acc: 0.9267\n",
      "Epoch 90/170\n",
      "1350/1350 [==============================] - 0s 32us/sample - loss: 0.0103 - acc: 0.9533 - val_loss: 0.0116 - val_acc: 0.9267\n",
      "Epoch 91/170\n",
      "1350/1350 [==============================] - 0s 32us/sample - loss: 0.0102 - acc: 0.9548 - val_loss: 0.0116 - val_acc: 0.9267\n",
      "Epoch 92/170\n",
      "1350/1350 [==============================] - 0s 30us/sample - loss: 0.0101 - acc: 0.9533 - val_loss: 0.0117 - val_acc: 0.9267\n",
      "Epoch 93/170\n",
      "1350/1350 [==============================] - 0s 32us/sample - loss: 0.0099 - acc: 0.9556 - val_loss: 0.0115 - val_acc: 0.9267\n",
      "Epoch 94/170\n",
      "1350/1350 [==============================] - 0s 31us/sample - loss: 0.0098 - acc: 0.9556 - val_loss: 0.0115 - val_acc: 0.9267\n",
      "Epoch 95/170\n",
      "1350/1350 [==============================] - 0s 33us/sample - loss: 0.0097 - acc: 0.9563 - val_loss: 0.0116 - val_acc: 0.9267\n",
      "Epoch 96/170\n",
      "1350/1350 [==============================] - 0s 31us/sample - loss: 0.0096 - acc: 0.9570 - val_loss: 0.0114 - val_acc: 0.9267\n",
      "Epoch 97/170\n",
      "1350/1350 [==============================] - 0s 33us/sample - loss: 0.0095 - acc: 0.9578 - val_loss: 0.0116 - val_acc: 0.9267\n",
      "Epoch 98/170\n",
      "1350/1350 [==============================] - 0s 32us/sample - loss: 0.0094 - acc: 0.9578 - val_loss: 0.0114 - val_acc: 0.9267\n",
      "Epoch 99/170\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1350/1350 [==============================] - 0s 31us/sample - loss: 0.0093 - acc: 0.9593 - val_loss: 0.0113 - val_acc: 0.9267\n",
      "Epoch 100/170\n",
      "1350/1350 [==============================] - 0s 31us/sample - loss: 0.0092 - acc: 0.9600 - val_loss: 0.0113 - val_acc: 0.9267\n",
      "Epoch 101/170\n",
      "1350/1350 [==============================] - 0s 30us/sample - loss: 0.0091 - acc: 0.9593 - val_loss: 0.0113 - val_acc: 0.9267\n",
      "Epoch 102/170\n",
      "1350/1350 [==============================] - 0s 31us/sample - loss: 0.0090 - acc: 0.9600 - val_loss: 0.0112 - val_acc: 0.9267\n",
      "Epoch 103/170\n",
      "1350/1350 [==============================] - 0s 31us/sample - loss: 0.0090 - acc: 0.9607 - val_loss: 0.0112 - val_acc: 0.9267\n",
      "Epoch 104/170\n",
      "1350/1350 [==============================] - 0s 31us/sample - loss: 0.0089 - acc: 0.9615 - val_loss: 0.0112 - val_acc: 0.9267\n",
      "Epoch 105/170\n",
      "1350/1350 [==============================] - 0s 31us/sample - loss: 0.0088 - acc: 0.9615 - val_loss: 0.0112 - val_acc: 0.9267\n",
      "Epoch 106/170\n",
      "1350/1350 [==============================] - 0s 31us/sample - loss: 0.0087 - acc: 0.9615 - val_loss: 0.0112 - val_acc: 0.9267\n",
      "Epoch 107/170\n",
      "1350/1350 [==============================] - 0s 30us/sample - loss: 0.0086 - acc: 0.9607 - val_loss: 0.0111 - val_acc: 0.9267\n",
      "Epoch 108/170\n",
      "1350/1350 [==============================] - 0s 31us/sample - loss: 0.0085 - acc: 0.9615 - val_loss: 0.0110 - val_acc: 0.9267\n",
      "Epoch 109/170\n",
      "1350/1350 [==============================] - 0s 31us/sample - loss: 0.0084 - acc: 0.9637 - val_loss: 0.0110 - val_acc: 0.9267\n",
      "Epoch 110/170\n",
      "1350/1350 [==============================] - 0s 32us/sample - loss: 0.0084 - acc: 0.9630 - val_loss: 0.0111 - val_acc: 0.9267\n",
      "Epoch 111/170\n",
      "1350/1350 [==============================] - 0s 31us/sample - loss: 0.0083 - acc: 0.9630 - val_loss: 0.0109 - val_acc: 0.9267\n",
      "Epoch 112/170\n",
      "1350/1350 [==============================] - 0s 30us/sample - loss: 0.0082 - acc: 0.9644 - val_loss: 0.0110 - val_acc: 0.9267\n",
      "Epoch 113/170\n",
      "1350/1350 [==============================] - 0s 31us/sample - loss: 0.0081 - acc: 0.9637 - val_loss: 0.0109 - val_acc: 0.9267\n",
      "Epoch 114/170\n",
      "1350/1350 [==============================] - 0s 31us/sample - loss: 0.0081 - acc: 0.9667 - val_loss: 0.0110 - val_acc: 0.9267\n",
      "Epoch 115/170\n",
      "1350/1350 [==============================] - 0s 32us/sample - loss: 0.0079 - acc: 0.9667 - val_loss: 0.0108 - val_acc: 0.9267\n",
      "Epoch 116/170\n",
      "1350/1350 [==============================] - 0s 32us/sample - loss: 0.0079 - acc: 0.9652 - val_loss: 0.0108 - val_acc: 0.9267\n",
      "Epoch 117/170\n",
      "1350/1350 [==============================] - 0s 32us/sample - loss: 0.0078 - acc: 0.9674 - val_loss: 0.0108 - val_acc: 0.9267\n",
      "Epoch 118/170\n",
      "1350/1350 [==============================] - 0s 31us/sample - loss: 0.0078 - acc: 0.9674 - val_loss: 0.0108 - val_acc: 0.9267\n",
      "Epoch 119/170\n",
      "1350/1350 [==============================] - 0s 30us/sample - loss: 0.0077 - acc: 0.9681 - val_loss: 0.0107 - val_acc: 0.9267\n",
      "Epoch 120/170\n",
      "1350/1350 [==============================] - 0s 31us/sample - loss: 0.0076 - acc: 0.9689 - val_loss: 0.0106 - val_acc: 0.9267\n",
      "Epoch 121/170\n",
      "1350/1350 [==============================] - 0s 31us/sample - loss: 0.0075 - acc: 0.9689 - val_loss: 0.0106 - val_acc: 0.9267\n",
      "Epoch 122/170\n",
      "1350/1350 [==============================] - 0s 30us/sample - loss: 0.0075 - acc: 0.9696 - val_loss: 0.0107 - val_acc: 0.9267\n",
      "Epoch 123/170\n",
      "1350/1350 [==============================] - 0s 33us/sample - loss: 0.0074 - acc: 0.9704 - val_loss: 0.0107 - val_acc: 0.9267\n",
      "Epoch 124/170\n",
      "1350/1350 [==============================] - 0s 31us/sample - loss: 0.0073 - acc: 0.9711 - val_loss: 0.0106 - val_acc: 0.9267\n",
      "Epoch 125/170\n",
      "1350/1350 [==============================] - 0s 31us/sample - loss: 0.0072 - acc: 0.9704 - val_loss: 0.0106 - val_acc: 0.9267\n",
      "Epoch 126/170\n",
      "1350/1350 [==============================] - 0s 31us/sample - loss: 0.0072 - acc: 0.9711 - val_loss: 0.0106 - val_acc: 0.9267\n",
      "Epoch 127/170\n",
      "1350/1350 [==============================] - 0s 30us/sample - loss: 0.0071 - acc: 0.9711 - val_loss: 0.0106 - val_acc: 0.9267\n",
      "Epoch 128/170\n",
      "1350/1350 [==============================] - 0s 31us/sample - loss: 0.0071 - acc: 0.9711 - val_loss: 0.0106 - val_acc: 0.9267\n",
      "Epoch 129/170\n",
      "1350/1350 [==============================] - 0s 30us/sample - loss: 0.0070 - acc: 0.9711 - val_loss: 0.0106 - val_acc: 0.9333\n",
      "Epoch 130/170\n",
      "1350/1350 [==============================] - 0s 30us/sample - loss: 0.0070 - acc: 0.9704 - val_loss: 0.0105 - val_acc: 0.9267\n",
      "Epoch 131/170\n",
      "1350/1350 [==============================] - 0s 31us/sample - loss: 0.0069 - acc: 0.9726 - val_loss: 0.0104 - val_acc: 0.9333\n",
      "Epoch 132/170\n",
      "1350/1350 [==============================] - 0s 31us/sample - loss: 0.0068 - acc: 0.9719 - val_loss: 0.0104 - val_acc: 0.9333\n",
      "Epoch 133/170\n",
      "1350/1350 [==============================] - 0s 32us/sample - loss: 0.0068 - acc: 0.9741 - val_loss: 0.0105 - val_acc: 0.9267\n",
      "Epoch 134/170\n",
      "1350/1350 [==============================] - 0s 30us/sample - loss: 0.0067 - acc: 0.9733 - val_loss: 0.0105 - val_acc: 0.9267\n",
      "Epoch 135/170\n",
      "1350/1350 [==============================] - 0s 31us/sample - loss: 0.0067 - acc: 0.9748 - val_loss: 0.0105 - val_acc: 0.9333\n",
      "Epoch 136/170\n",
      "1350/1350 [==============================] - 0s 31us/sample - loss: 0.0066 - acc: 0.9741 - val_loss: 0.0104 - val_acc: 0.9333\n",
      "Epoch 137/170\n",
      "1350/1350 [==============================] - 0s 31us/sample - loss: 0.0065 - acc: 0.9756 - val_loss: 0.0104 - val_acc: 0.9333\n",
      "Epoch 138/170\n",
      "1350/1350 [==============================] - 0s 31us/sample - loss: 0.0065 - acc: 0.9756 - val_loss: 0.0103 - val_acc: 0.9333\n",
      "Epoch 139/170\n",
      "1350/1350 [==============================] - 0s 31us/sample - loss: 0.0064 - acc: 0.9770 - val_loss: 0.0103 - val_acc: 0.9333\n",
      "Epoch 140/170\n",
      "1350/1350 [==============================] - 0s 32us/sample - loss: 0.0064 - acc: 0.9770 - val_loss: 0.0104 - val_acc: 0.9333\n",
      "Epoch 141/170\n",
      "1350/1350 [==============================] - 0s 32us/sample - loss: 0.0063 - acc: 0.9778 - val_loss: 0.0103 - val_acc: 0.9333\n",
      "Epoch 142/170\n",
      "1350/1350 [==============================] - 0s 32us/sample - loss: 0.0063 - acc: 0.9770 - val_loss: 0.0103 - val_acc: 0.9333\n",
      "Epoch 143/170\n",
      "1350/1350 [==============================] - 0s 29us/sample - loss: 0.0062 - acc: 0.9785 - val_loss: 0.0104 - val_acc: 0.9333\n",
      "Epoch 144/170\n",
      "1350/1350 [==============================] - 0s 30us/sample - loss: 0.0062 - acc: 0.9770 - val_loss: 0.0103 - val_acc: 0.9333\n",
      "Epoch 145/170\n",
      "1350/1350 [==============================] - 0s 30us/sample - loss: 0.0061 - acc: 0.9785 - val_loss: 0.0102 - val_acc: 0.9333\n",
      "Epoch 146/170\n",
      "1350/1350 [==============================] - 0s 31us/sample - loss: 0.0061 - acc: 0.9770 - val_loss: 0.0103 - val_acc: 0.9333\n",
      "Epoch 147/170\n",
      "1350/1350 [==============================] - 0s 30us/sample - loss: 0.0060 - acc: 0.9785 - val_loss: 0.0102 - val_acc: 0.9333\n",
      "Epoch 148/170\n",
      "1350/1350 [==============================] - 0s 32us/sample - loss: 0.0060 - acc: 0.9785 - val_loss: 0.0101 - val_acc: 0.9400\n",
      "Epoch 149/170\n",
      "1350/1350 [==============================] - 0s 31us/sample - loss: 0.0059 - acc: 0.9785 - val_loss: 0.0102 - val_acc: 0.9333\n",
      "Epoch 150/170\n",
      "1350/1350 [==============================] - 0s 30us/sample - loss: 0.0059 - acc: 0.9785 - val_loss: 0.0102 - val_acc: 0.9333\n",
      "Epoch 151/170\n",
      "1350/1350 [==============================] - 0s 32us/sample - loss: 0.0058 - acc: 0.9785 - val_loss: 0.0102 - val_acc: 0.9333\n",
      "Epoch 152/170\n",
      "1350/1350 [==============================] - 0s 31us/sample - loss: 0.0058 - acc: 0.9800 - val_loss: 0.0102 - val_acc: 0.9333\n",
      "Epoch 153/170\n",
      "1350/1350 [==============================] - 0s 33us/sample - loss: 0.0057 - acc: 0.9793 - val_loss: 0.0102 - val_acc: 0.9333\n",
      "Epoch 154/170\n",
      "1350/1350 [==============================] - 0s 30us/sample - loss: 0.0057 - acc: 0.9800 - val_loss: 0.0101 - val_acc: 0.9333\n",
      "Epoch 155/170\n",
      "1350/1350 [==============================] - 0s 30us/sample - loss: 0.0056 - acc: 0.9800 - val_loss: 0.0102 - val_acc: 0.9333\n",
      "Epoch 156/170\n",
      "1350/1350 [==============================] - 0s 30us/sample - loss: 0.0056 - acc: 0.9807 - val_loss: 0.0101 - val_acc: 0.9333\n",
      "Epoch 157/170\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1350/1350 [==============================] - 0s 30us/sample - loss: 0.0056 - acc: 0.9800 - val_loss: 0.0101 - val_acc: 0.9333\n",
      "Epoch 158/170\n",
      "1350/1350 [==============================] - 0s 32us/sample - loss: 0.0055 - acc: 0.9807 - val_loss: 0.0101 - val_acc: 0.9333\n",
      "Epoch 159/170\n",
      "1350/1350 [==============================] - 0s 34us/sample - loss: 0.0055 - acc: 0.9800 - val_loss: 0.0101 - val_acc: 0.9333\n",
      "Epoch 160/170\n",
      "1350/1350 [==============================] - 0s 31us/sample - loss: 0.0054 - acc: 0.9815 - val_loss: 0.0101 - val_acc: 0.9333\n",
      "Epoch 161/170\n",
      "1350/1350 [==============================] - 0s 33us/sample - loss: 0.0054 - acc: 0.9815 - val_loss: 0.0101 - val_acc: 0.9333\n",
      "Epoch 162/170\n",
      "1350/1350 [==============================] - 0s 32us/sample - loss: 0.0053 - acc: 0.9815 - val_loss: 0.0101 - val_acc: 0.9333\n",
      "Epoch 163/170\n",
      "1350/1350 [==============================] - 0s 32us/sample - loss: 0.0053 - acc: 0.9815 - val_loss: 0.0101 - val_acc: 0.9333\n",
      "Epoch 164/170\n",
      "1350/1350 [==============================] - 0s 30us/sample - loss: 0.0053 - acc: 0.9807 - val_loss: 0.0101 - val_acc: 0.9333\n",
      "Epoch 165/170\n",
      "1350/1350 [==============================] - 0s 29us/sample - loss: 0.0052 - acc: 0.9815 - val_loss: 0.0100 - val_acc: 0.9333\n",
      "Epoch 166/170\n",
      "1350/1350 [==============================] - 0s 31us/sample - loss: 0.0052 - acc: 0.9822 - val_loss: 0.0101 - val_acc: 0.9333\n",
      "Epoch 167/170\n",
      "1350/1350 [==============================] - 0s 31us/sample - loss: 0.0051 - acc: 0.9815 - val_loss: 0.0099 - val_acc: 0.9333\n",
      "Epoch 168/170\n",
      "1350/1350 [==============================] - 0s 31us/sample - loss: 0.0051 - acc: 0.9822 - val_loss: 0.0100 - val_acc: 0.9333\n",
      "Epoch 169/170\n",
      "1350/1350 [==============================] - 0s 31us/sample - loss: 0.0051 - acc: 0.9815 - val_loss: 0.0100 - val_acc: 0.9333\n",
      "Epoch 170/170\n",
      "1350/1350 [==============================] - 0s 31us/sample - loss: 0.0050 - acc: 0.9822 - val_loss: 0.0099 - val_acc: 0.9333\n",
      "Elasped Time: 0:00:07.816630\n",
      "Training date and time : \n",
      "2020-04-25 21:08:59\n",
      "Train on 1350 samples, validate on 150 samples\n",
      "Epoch 1/180\n",
      "1350/1350 [==============================] - 0s 158us/sample - loss: 0.0685 - acc: 0.5415 - val_loss: 0.0662 - val_acc: 0.5333\n",
      "Epoch 2/180\n",
      "1350/1350 [==============================] - 0s 31us/sample - loss: 0.0662 - acc: 0.5489 - val_loss: 0.0641 - val_acc: 0.5467\n",
      "Epoch 3/180\n",
      "1350/1350 [==============================] - 0s 31us/sample - loss: 0.0638 - acc: 0.5674 - val_loss: 0.0620 - val_acc: 0.5733\n",
      "Epoch 4/180\n",
      "1350/1350 [==============================] - 0s 31us/sample - loss: 0.0614 - acc: 0.5911 - val_loss: 0.0599 - val_acc: 0.5667\n",
      "Epoch 5/180\n",
      "1350/1350 [==============================] - 0s 33us/sample - loss: 0.0591 - acc: 0.6156 - val_loss: 0.0579 - val_acc: 0.5800\n",
      "Epoch 6/180\n",
      "1350/1350 [==============================] - 0s 32us/sample - loss: 0.0568 - acc: 0.6393 - val_loss: 0.0559 - val_acc: 0.6000\n",
      "Epoch 7/180\n",
      "1350/1350 [==============================] - 0s 32us/sample - loss: 0.0544 - acc: 0.6630 - val_loss: 0.0539 - val_acc: 0.6333\n",
      "Epoch 8/180\n",
      "1350/1350 [==============================] - 0s 32us/sample - loss: 0.0521 - acc: 0.7015 - val_loss: 0.0520 - val_acc: 0.6533\n",
      "Epoch 9/180\n",
      "1350/1350 [==============================] - 0s 32us/sample - loss: 0.0499 - acc: 0.7200 - val_loss: 0.0503 - val_acc: 0.6733\n",
      "Epoch 10/180\n",
      "1350/1350 [==============================] - 0s 30us/sample - loss: 0.0477 - acc: 0.7533 - val_loss: 0.0485 - val_acc: 0.6800\n",
      "Epoch 11/180\n",
      "1350/1350 [==============================] - 0s 31us/sample - loss: 0.0456 - acc: 0.7785 - val_loss: 0.0469 - val_acc: 0.7200\n",
      "Epoch 12/180\n",
      "1350/1350 [==============================] - 0s 33us/sample - loss: 0.0435 - acc: 0.7993 - val_loss: 0.0452 - val_acc: 0.7400\n",
      "Epoch 13/180\n",
      "1350/1350 [==============================] - 0s 31us/sample - loss: 0.0415 - acc: 0.8170 - val_loss: 0.0437 - val_acc: 0.7667\n",
      "Epoch 14/180\n",
      "1350/1350 [==============================] - 0s 32us/sample - loss: 0.0396 - acc: 0.8333 - val_loss: 0.0423 - val_acc: 0.7667\n",
      "Epoch 15/180\n",
      "1350/1350 [==============================] - 0s 31us/sample - loss: 0.0378 - acc: 0.8422 - val_loss: 0.0410 - val_acc: 0.7800\n",
      "Epoch 16/180\n",
      "1350/1350 [==============================] - 0s 31us/sample - loss: 0.0362 - acc: 0.8511 - val_loss: 0.0397 - val_acc: 0.7933\n",
      "Epoch 17/180\n",
      "1350/1350 [==============================] - 0s 31us/sample - loss: 0.0346 - acc: 0.8622 - val_loss: 0.0384 - val_acc: 0.8000\n",
      "Epoch 18/180\n",
      "1350/1350 [==============================] - 0s 31us/sample - loss: 0.0331 - acc: 0.8622 - val_loss: 0.0374 - val_acc: 0.8067\n",
      "Epoch 19/180\n",
      "1350/1350 [==============================] - 0s 30us/sample - loss: 0.0317 - acc: 0.8711 - val_loss: 0.0364 - val_acc: 0.8067\n",
      "Epoch 20/180\n",
      "1350/1350 [==============================] - 0s 31us/sample - loss: 0.0305 - acc: 0.8800 - val_loss: 0.0355 - val_acc: 0.8000\n",
      "Epoch 21/180\n",
      "1350/1350 [==============================] - 0s 32us/sample - loss: 0.0293 - acc: 0.8785 - val_loss: 0.0345 - val_acc: 0.8000\n",
      "Epoch 22/180\n",
      "1350/1350 [==============================] - 0s 33us/sample - loss: 0.0282 - acc: 0.8844 - val_loss: 0.0338 - val_acc: 0.7933\n",
      "Epoch 23/180\n",
      "1350/1350 [==============================] - 0s 32us/sample - loss: 0.0272 - acc: 0.8874 - val_loss: 0.0330 - val_acc: 0.8000\n",
      "Epoch 24/180\n",
      "1350/1350 [==============================] - 0s 33us/sample - loss: 0.0263 - acc: 0.8904 - val_loss: 0.0324 - val_acc: 0.8000\n",
      "Epoch 25/180\n",
      "1350/1350 [==============================] - 0s 32us/sample - loss: 0.0254 - acc: 0.8948 - val_loss: 0.0317 - val_acc: 0.7933\n",
      "Epoch 26/180\n",
      "1350/1350 [==============================] - 0s 30us/sample - loss: 0.0246 - acc: 0.8956 - val_loss: 0.0315 - val_acc: 0.8133\n",
      "Epoch 27/180\n",
      "1350/1350 [==============================] - 0s 30us/sample - loss: 0.0239 - acc: 0.8970 - val_loss: 0.0307 - val_acc: 0.8133\n",
      "Epoch 28/180\n",
      "1350/1350 [==============================] - 0s 30us/sample - loss: 0.0232 - acc: 0.8993 - val_loss: 0.0301 - val_acc: 0.8200\n",
      "Epoch 29/180\n",
      "1350/1350 [==============================] - 0s 31us/sample - loss: 0.0226 - acc: 0.9007 - val_loss: 0.0297 - val_acc: 0.8200\n",
      "Epoch 30/180\n",
      "1350/1350 [==============================] - 0s 33us/sample - loss: 0.0220 - acc: 0.9015 - val_loss: 0.0293 - val_acc: 0.8133\n",
      "Epoch 31/180\n",
      "1350/1350 [==============================] - 0s 33us/sample - loss: 0.0214 - acc: 0.9052 - val_loss: 0.0287 - val_acc: 0.8200\n",
      "Epoch 32/180\n",
      "1350/1350 [==============================] - 0s 31us/sample - loss: 0.0209 - acc: 0.9067 - val_loss: 0.0282 - val_acc: 0.8200\n",
      "Epoch 33/180\n",
      "1350/1350 [==============================] - 0s 32us/sample - loss: 0.0204 - acc: 0.9059 - val_loss: 0.0278 - val_acc: 0.8133\n",
      "Epoch 34/180\n",
      "1350/1350 [==============================] - 0s 31us/sample - loss: 0.0199 - acc: 0.9096 - val_loss: 0.0276 - val_acc: 0.8200\n",
      "Epoch 35/180\n",
      "1350/1350 [==============================] - 0s 29us/sample - loss: 0.0195 - acc: 0.9096 - val_loss: 0.0272 - val_acc: 0.8333\n",
      "Epoch 36/180\n",
      "1350/1350 [==============================] - 0s 31us/sample - loss: 0.0191 - acc: 0.9126 - val_loss: 0.0269 - val_acc: 0.8267\n",
      "Epoch 37/180\n",
      "1350/1350 [==============================] - 0s 32us/sample - loss: 0.0187 - acc: 0.9148 - val_loss: 0.0267 - val_acc: 0.8333\n",
      "Epoch 38/180\n",
      "1350/1350 [==============================] - 0s 32us/sample - loss: 0.0183 - acc: 0.9148 - val_loss: 0.0264 - val_acc: 0.8400\n",
      "Epoch 39/180\n",
      "1350/1350 [==============================] - 0s 30us/sample - loss: 0.0180 - acc: 0.9163 - val_loss: 0.0262 - val_acc: 0.8400\n",
      "Epoch 40/180\n",
      "1350/1350 [==============================] - 0s 31us/sample - loss: 0.0176 - acc: 0.9148 - val_loss: 0.0258 - val_acc: 0.8400\n",
      "Epoch 41/180\n",
      "1350/1350 [==============================] - 0s 32us/sample - loss: 0.0173 - acc: 0.9193 - val_loss: 0.0254 - val_acc: 0.8400\n",
      "Epoch 42/180\n",
      "1350/1350 [==============================] - 0s 31us/sample - loss: 0.0169 - acc: 0.9222 - val_loss: 0.0253 - val_acc: 0.8400\n",
      "Epoch 43/180\n",
      "1350/1350 [==============================] - 0s 31us/sample - loss: 0.0166 - acc: 0.9193 - val_loss: 0.0250 - val_acc: 0.8400\n",
      "Epoch 44/180\n",
      "1350/1350 [==============================] - 0s 32us/sample - loss: 0.0163 - acc: 0.9230 - val_loss: 0.0247 - val_acc: 0.8467\n",
      "Epoch 45/180\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1350/1350 [==============================] - 0s 33us/sample - loss: 0.0160 - acc: 0.9230 - val_loss: 0.0246 - val_acc: 0.8400\n",
      "Epoch 46/180\n",
      "1350/1350 [==============================] - 0s 30us/sample - loss: 0.0158 - acc: 0.9259 - val_loss: 0.0241 - val_acc: 0.8467\n",
      "Epoch 47/180\n",
      "1350/1350 [==============================] - 0s 32us/sample - loss: 0.0155 - acc: 0.9274 - val_loss: 0.0242 - val_acc: 0.8400\n",
      "Epoch 48/180\n",
      "1350/1350 [==============================] - 0s 30us/sample - loss: 0.0153 - acc: 0.9304 - val_loss: 0.0238 - val_acc: 0.8400\n",
      "Epoch 49/180\n",
      "1350/1350 [==============================] - 0s 31us/sample - loss: 0.0150 - acc: 0.9281 - val_loss: 0.0237 - val_acc: 0.8467\n",
      "Epoch 50/180\n",
      "1350/1350 [==============================] - 0s 31us/sample - loss: 0.0148 - acc: 0.9296 - val_loss: 0.0235 - val_acc: 0.8467\n",
      "Epoch 51/180\n",
      "1350/1350 [==============================] - 0s 30us/sample - loss: 0.0146 - acc: 0.9311 - val_loss: 0.0235 - val_acc: 0.8533\n",
      "Epoch 52/180\n",
      "1350/1350 [==============================] - 0s 33us/sample - loss: 0.0144 - acc: 0.9319 - val_loss: 0.0233 - val_acc: 0.8533\n",
      "Epoch 53/180\n",
      "1350/1350 [==============================] - 0s 30us/sample - loss: 0.0142 - acc: 0.9326 - val_loss: 0.0233 - val_acc: 0.8533\n",
      "Epoch 54/180\n",
      "1350/1350 [==============================] - 0s 28us/sample - loss: 0.0140 - acc: 0.9348 - val_loss: 0.0229 - val_acc: 0.8533\n",
      "Epoch 55/180\n",
      "1350/1350 [==============================] - 0s 32us/sample - loss: 0.0138 - acc: 0.9348 - val_loss: 0.0228 - val_acc: 0.8533\n",
      "Epoch 56/180\n",
      "1350/1350 [==============================] - 0s 33us/sample - loss: 0.0135 - acc: 0.9348 - val_loss: 0.0227 - val_acc: 0.8600\n",
      "Epoch 57/180\n",
      "1350/1350 [==============================] - 0s 32us/sample - loss: 0.0134 - acc: 0.9407 - val_loss: 0.0223 - val_acc: 0.8667\n",
      "Epoch 58/180\n",
      "1350/1350 [==============================] - 0s 32us/sample - loss: 0.0132 - acc: 0.9356 - val_loss: 0.0224 - val_acc: 0.8600\n",
      "Epoch 59/180\n",
      "1350/1350 [==============================] - 0s 30us/sample - loss: 0.0130 - acc: 0.9393 - val_loss: 0.0221 - val_acc: 0.8667\n",
      "Epoch 60/180\n",
      "1350/1350 [==============================] - 0s 32us/sample - loss: 0.0128 - acc: 0.9393 - val_loss: 0.0219 - val_acc: 0.8733\n",
      "Epoch 61/180\n",
      "1350/1350 [==============================] - 0s 33us/sample - loss: 0.0127 - acc: 0.9400 - val_loss: 0.0218 - val_acc: 0.8733\n",
      "Epoch 62/180\n",
      "1350/1350 [==============================] - 0s 34us/sample - loss: 0.0125 - acc: 0.9422 - val_loss: 0.0216 - val_acc: 0.8733\n",
      "Epoch 63/180\n",
      "1350/1350 [==============================] - 0s 31us/sample - loss: 0.0124 - acc: 0.9407 - val_loss: 0.0215 - val_acc: 0.8733\n",
      "Epoch 64/180\n",
      "1350/1350 [==============================] - 0s 33us/sample - loss: 0.0122 - acc: 0.9422 - val_loss: 0.0213 - val_acc: 0.8800\n",
      "Epoch 65/180\n",
      "1350/1350 [==============================] - 0s 30us/sample - loss: 0.0121 - acc: 0.9430 - val_loss: 0.0212 - val_acc: 0.8800\n",
      "Epoch 66/180\n",
      "1350/1350 [==============================] - 0s 31us/sample - loss: 0.0119 - acc: 0.9437 - val_loss: 0.0211 - val_acc: 0.8800\n",
      "Epoch 67/180\n",
      "1350/1350 [==============================] - 0s 31us/sample - loss: 0.0118 - acc: 0.9437 - val_loss: 0.0210 - val_acc: 0.8800\n",
      "Epoch 68/180\n",
      "1350/1350 [==============================] - 0s 31us/sample - loss: 0.0117 - acc: 0.9437 - val_loss: 0.0211 - val_acc: 0.8733\n",
      "Epoch 69/180\n",
      "1350/1350 [==============================] - 0s 34us/sample - loss: 0.0115 - acc: 0.9430 - val_loss: 0.0211 - val_acc: 0.8733\n",
      "Epoch 70/180\n",
      "1350/1350 [==============================] - 0s 32us/sample - loss: 0.0114 - acc: 0.9467 - val_loss: 0.0209 - val_acc: 0.8800\n",
      "Epoch 71/180\n",
      "1350/1350 [==============================] - 0s 32us/sample - loss: 0.0113 - acc: 0.9437 - val_loss: 0.0206 - val_acc: 0.8800\n",
      "Epoch 72/180\n",
      "1350/1350 [==============================] - 0s 31us/sample - loss: 0.0111 - acc: 0.9452 - val_loss: 0.0207 - val_acc: 0.8800\n",
      "Epoch 73/180\n",
      "1350/1350 [==============================] - 0s 32us/sample - loss: 0.0110 - acc: 0.9467 - val_loss: 0.0206 - val_acc: 0.8800\n",
      "Epoch 74/180\n",
      "1350/1350 [==============================] - 0s 31us/sample - loss: 0.0109 - acc: 0.9481 - val_loss: 0.0204 - val_acc: 0.8800\n",
      "Epoch 75/180\n",
      "1350/1350 [==============================] - 0s 32us/sample - loss: 0.0108 - acc: 0.9496 - val_loss: 0.0204 - val_acc: 0.8800\n",
      "Epoch 76/180\n",
      "1350/1350 [==============================] - 0s 31us/sample - loss: 0.0107 - acc: 0.9481 - val_loss: 0.0202 - val_acc: 0.8800\n",
      "Epoch 77/180\n",
      "1350/1350 [==============================] - 0s 30us/sample - loss: 0.0106 - acc: 0.9504 - val_loss: 0.0201 - val_acc: 0.8800\n",
      "Epoch 78/180\n",
      "1350/1350 [==============================] - 0s 31us/sample - loss: 0.0105 - acc: 0.9496 - val_loss: 0.0202 - val_acc: 0.8800\n",
      "Epoch 79/180\n",
      "1350/1350 [==============================] - 0s 31us/sample - loss: 0.0103 - acc: 0.9511 - val_loss: 0.0200 - val_acc: 0.8800\n",
      "Epoch 80/180\n",
      "1350/1350 [==============================] - 0s 32us/sample - loss: 0.0102 - acc: 0.9519 - val_loss: 0.0201 - val_acc: 0.8800\n",
      "Epoch 81/180\n",
      "1350/1350 [==============================] - 0s 32us/sample - loss: 0.0101 - acc: 0.9519 - val_loss: 0.0201 - val_acc: 0.8867\n",
      "Epoch 82/180\n",
      "1350/1350 [==============================] - 0s 31us/sample - loss: 0.0100 - acc: 0.9519 - val_loss: 0.0197 - val_acc: 0.8800\n",
      "Epoch 83/180\n",
      "1350/1350 [==============================] - 0s 31us/sample - loss: 0.0099 - acc: 0.9526 - val_loss: 0.0199 - val_acc: 0.8867\n",
      "Epoch 84/180\n",
      "1350/1350 [==============================] - 0s 31us/sample - loss: 0.0098 - acc: 0.9541 - val_loss: 0.0195 - val_acc: 0.8800\n",
      "Epoch 85/180\n",
      "1350/1350 [==============================] - 0s 30us/sample - loss: 0.0098 - acc: 0.9526 - val_loss: 0.0195 - val_acc: 0.8800\n",
      "Epoch 86/180\n",
      "1350/1350 [==============================] - 0s 31us/sample - loss: 0.0096 - acc: 0.9533 - val_loss: 0.0196 - val_acc: 0.8867\n",
      "Epoch 87/180\n",
      "1350/1350 [==============================] - 0s 32us/sample - loss: 0.0095 - acc: 0.9548 - val_loss: 0.0196 - val_acc: 0.8867\n",
      "Epoch 88/180\n",
      "1350/1350 [==============================] - 0s 33us/sample - loss: 0.0095 - acc: 0.9556 - val_loss: 0.0193 - val_acc: 0.8800\n",
      "Epoch 89/180\n",
      "1350/1350 [==============================] - 0s 31us/sample - loss: 0.0094 - acc: 0.9541 - val_loss: 0.0193 - val_acc: 0.8800\n",
      "Epoch 90/180\n",
      "1350/1350 [==============================] - 0s 32us/sample - loss: 0.0093 - acc: 0.9556 - val_loss: 0.0193 - val_acc: 0.8800\n",
      "Epoch 91/180\n",
      "1350/1350 [==============================] - 0s 33us/sample - loss: 0.0092 - acc: 0.9556 - val_loss: 0.0191 - val_acc: 0.8800\n",
      "Epoch 92/180\n",
      "1350/1350 [==============================] - 0s 30us/sample - loss: 0.0091 - acc: 0.9556 - val_loss: 0.0189 - val_acc: 0.8867\n",
      "Epoch 93/180\n",
      "1350/1350 [==============================] - 0s 31us/sample - loss: 0.0090 - acc: 0.9563 - val_loss: 0.0190 - val_acc: 0.8867\n",
      "Epoch 94/180\n",
      "1350/1350 [==============================] - 0s 30us/sample - loss: 0.0089 - acc: 0.9593 - val_loss: 0.0192 - val_acc: 0.9000\n",
      "Epoch 95/180\n",
      "1350/1350 [==============================] - 0s 32us/sample - loss: 0.0089 - acc: 0.9585 - val_loss: 0.0189 - val_acc: 0.8867\n",
      "Epoch 96/180\n",
      "1350/1350 [==============================] - 0s 31us/sample - loss: 0.0088 - acc: 0.9570 - val_loss: 0.0188 - val_acc: 0.8867\n",
      "Epoch 97/180\n",
      "1350/1350 [==============================] - 0s 30us/sample - loss: 0.0087 - acc: 0.9585 - val_loss: 0.0188 - val_acc: 0.9000\n",
      "Epoch 98/180\n",
      "1350/1350 [==============================] - 0s 31us/sample - loss: 0.0086 - acc: 0.9607 - val_loss: 0.0187 - val_acc: 0.8933\n",
      "Epoch 99/180\n",
      "1350/1350 [==============================] - 0s 31us/sample - loss: 0.0086 - acc: 0.9607 - val_loss: 0.0187 - val_acc: 0.8933\n",
      "Epoch 100/180\n",
      "1350/1350 [==============================] - 0s 33us/sample - loss: 0.0085 - acc: 0.9607 - val_loss: 0.0186 - val_acc: 0.8867\n",
      "Epoch 101/180\n",
      "1350/1350 [==============================] - 0s 32us/sample - loss: 0.0084 - acc: 0.9622 - val_loss: 0.0187 - val_acc: 0.9000\n",
      "Epoch 102/180\n",
      "1350/1350 [==============================] - 0s 32us/sample - loss: 0.0083 - acc: 0.9615 - val_loss: 0.0186 - val_acc: 0.8933\n",
      "Epoch 103/180\n",
      "1350/1350 [==============================] - 0s 32us/sample - loss: 0.0083 - acc: 0.9630 - val_loss: 0.0184 - val_acc: 0.9067\n",
      "Epoch 104/180\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1350/1350 [==============================] - 0s 31us/sample - loss: 0.0082 - acc: 0.9622 - val_loss: 0.0185 - val_acc: 0.9000\n",
      "Epoch 105/180\n",
      "1350/1350 [==============================] - 0s 31us/sample - loss: 0.0081 - acc: 0.9615 - val_loss: 0.0183 - val_acc: 0.9000\n",
      "Epoch 106/180\n",
      "1350/1350 [==============================] - 0s 31us/sample - loss: 0.0081 - acc: 0.9630 - val_loss: 0.0184 - val_acc: 0.8933\n",
      "Epoch 107/180\n",
      "1350/1350 [==============================] - 0s 30us/sample - loss: 0.0080 - acc: 0.9615 - val_loss: 0.0184 - val_acc: 0.9000\n",
      "Epoch 108/180\n",
      "1350/1350 [==============================] - 0s 30us/sample - loss: 0.0080 - acc: 0.9630 - val_loss: 0.0183 - val_acc: 0.9000\n",
      "Epoch 109/180\n",
      "1350/1350 [==============================] - 0s 31us/sample - loss: 0.0079 - acc: 0.9644 - val_loss: 0.0183 - val_acc: 0.9000\n",
      "Epoch 110/180\n",
      "1350/1350 [==============================] - 0s 31us/sample - loss: 0.0078 - acc: 0.9622 - val_loss: 0.0182 - val_acc: 0.9000\n",
      "Epoch 111/180\n",
      "1350/1350 [==============================] - 0s 32us/sample - loss: 0.0077 - acc: 0.9630 - val_loss: 0.0182 - val_acc: 0.9000\n",
      "Epoch 112/180\n",
      "1350/1350 [==============================] - 0s 31us/sample - loss: 0.0077 - acc: 0.9644 - val_loss: 0.0181 - val_acc: 0.9067\n",
      "Epoch 113/180\n",
      "1350/1350 [==============================] - 0s 33us/sample - loss: 0.0076 - acc: 0.9637 - val_loss: 0.0182 - val_acc: 0.9067\n",
      "Epoch 114/180\n",
      "1350/1350 [==============================] - 0s 32us/sample - loss: 0.0076 - acc: 0.9644 - val_loss: 0.0180 - val_acc: 0.9067\n",
      "Epoch 115/180\n",
      "1350/1350 [==============================] - 0s 32us/sample - loss: 0.0075 - acc: 0.9667 - val_loss: 0.0181 - val_acc: 0.9000\n",
      "Epoch 116/180\n",
      "1350/1350 [==============================] - 0s 33us/sample - loss: 0.0075 - acc: 0.9652 - val_loss: 0.0181 - val_acc: 0.9000\n",
      "Epoch 117/180\n",
      "1350/1350 [==============================] - 0s 31us/sample - loss: 0.0074 - acc: 0.9644 - val_loss: 0.0180 - val_acc: 0.9067\n",
      "Epoch 118/180\n",
      "1350/1350 [==============================] - 0s 32us/sample - loss: 0.0073 - acc: 0.9674 - val_loss: 0.0178 - val_acc: 0.9067\n",
      "Epoch 119/180\n",
      "1350/1350 [==============================] - 0s 31us/sample - loss: 0.0073 - acc: 0.9659 - val_loss: 0.0180 - val_acc: 0.9067\n",
      "Epoch 120/180\n",
      "1350/1350 [==============================] - 0s 32us/sample - loss: 0.0072 - acc: 0.9652 - val_loss: 0.0179 - val_acc: 0.9067\n",
      "Epoch 121/180\n",
      "1350/1350 [==============================] - 0s 30us/sample - loss: 0.0072 - acc: 0.9652 - val_loss: 0.0178 - val_acc: 0.9067\n",
      "Epoch 122/180\n",
      "1350/1350 [==============================] - 0s 30us/sample - loss: 0.0071 - acc: 0.9667 - val_loss: 0.0179 - val_acc: 0.9067\n",
      "Epoch 123/180\n",
      "1350/1350 [==============================] - 0s 31us/sample - loss: 0.0071 - acc: 0.9689 - val_loss: 0.0179 - val_acc: 0.9067\n",
      "Epoch 124/180\n",
      "1350/1350 [==============================] - 0s 31us/sample - loss: 0.0070 - acc: 0.9689 - val_loss: 0.0179 - val_acc: 0.9000\n",
      "Epoch 125/180\n",
      "1350/1350 [==============================] - 0s 30us/sample - loss: 0.0070 - acc: 0.9681 - val_loss: 0.0179 - val_acc: 0.9067\n",
      "Epoch 126/180\n",
      "1350/1350 [==============================] - 0s 32us/sample - loss: 0.0069 - acc: 0.9681 - val_loss: 0.0178 - val_acc: 0.9067\n",
      "Epoch 127/180\n",
      "1350/1350 [==============================] - 0s 30us/sample - loss: 0.0069 - acc: 0.9696 - val_loss: 0.0178 - val_acc: 0.9067\n",
      "Epoch 128/180\n",
      "1350/1350 [==============================] - 0s 31us/sample - loss: 0.0068 - acc: 0.9704 - val_loss: 0.0178 - val_acc: 0.9067\n",
      "Epoch 129/180\n",
      "1350/1350 [==============================] - 0s 30us/sample - loss: 0.0068 - acc: 0.9689 - val_loss: 0.0177 - val_acc: 0.9067\n",
      "Epoch 130/180\n",
      "1350/1350 [==============================] - 0s 31us/sample - loss: 0.0067 - acc: 0.9689 - val_loss: 0.0177 - val_acc: 0.9067\n",
      "Epoch 131/180\n",
      "1350/1350 [==============================] - 0s 33us/sample - loss: 0.0067 - acc: 0.9704 - val_loss: 0.0176 - val_acc: 0.9067\n",
      "Epoch 132/180\n",
      "1350/1350 [==============================] - 0s 31us/sample - loss: 0.0066 - acc: 0.9689 - val_loss: 0.0176 - val_acc: 0.9067\n",
      "Epoch 133/180\n",
      "1350/1350 [==============================] - 0s 31us/sample - loss: 0.0066 - acc: 0.9696 - val_loss: 0.0177 - val_acc: 0.9067\n",
      "Epoch 134/180\n",
      "1350/1350 [==============================] - 0s 30us/sample - loss: 0.0065 - acc: 0.9704 - val_loss: 0.0176 - val_acc: 0.9067\n",
      "Epoch 135/180\n",
      "1350/1350 [==============================] - 0s 29us/sample - loss: 0.0065 - acc: 0.9696 - val_loss: 0.0176 - val_acc: 0.9067\n",
      "Epoch 136/180\n",
      "1350/1350 [==============================] - 0s 30us/sample - loss: 0.0064 - acc: 0.9719 - val_loss: 0.0176 - val_acc: 0.9067\n",
      "Epoch 137/180\n",
      "1350/1350 [==============================] - 0s 31us/sample - loss: 0.0064 - acc: 0.9704 - val_loss: 0.0177 - val_acc: 0.9067\n",
      "Epoch 138/180\n",
      "1350/1350 [==============================] - 0s 31us/sample - loss: 0.0063 - acc: 0.9711 - val_loss: 0.0176 - val_acc: 0.9067\n",
      "Epoch 139/180\n",
      "1350/1350 [==============================] - 0s 32us/sample - loss: 0.0063 - acc: 0.9726 - val_loss: 0.0178 - val_acc: 0.9067\n",
      "Epoch 140/180\n",
      "1350/1350 [==============================] - 0s 33us/sample - loss: 0.0063 - acc: 0.9733 - val_loss: 0.0176 - val_acc: 0.9067\n",
      "Epoch 141/180\n",
      "1350/1350 [==============================] - 0s 31us/sample - loss: 0.0062 - acc: 0.9711 - val_loss: 0.0176 - val_acc: 0.9067\n",
      "Epoch 142/180\n",
      "1350/1350 [==============================] - 0s 29us/sample - loss: 0.0062 - acc: 0.9711 - val_loss: 0.0177 - val_acc: 0.9067\n",
      "Epoch 143/180\n",
      "1350/1350 [==============================] - 0s 33us/sample - loss: 0.0061 - acc: 0.9741 - val_loss: 0.0176 - val_acc: 0.9067\n",
      "Epoch 144/180\n",
      "1350/1350 [==============================] - 0s 31us/sample - loss: 0.0061 - acc: 0.9741 - val_loss: 0.0177 - val_acc: 0.9067\n",
      "Epoch 145/180\n",
      "1350/1350 [==============================] - 0s 30us/sample - loss: 0.0060 - acc: 0.9741 - val_loss: 0.0175 - val_acc: 0.9067\n",
      "Epoch 146/180\n",
      "1350/1350 [==============================] - 0s 31us/sample - loss: 0.0060 - acc: 0.9756 - val_loss: 0.0174 - val_acc: 0.9067\n",
      "Epoch 147/180\n",
      "1350/1350 [==============================] - 0s 32us/sample - loss: 0.0060 - acc: 0.9733 - val_loss: 0.0175 - val_acc: 0.9067\n",
      "Epoch 148/180\n",
      "1350/1350 [==============================] - 0s 31us/sample - loss: 0.0059 - acc: 0.9756 - val_loss: 0.0175 - val_acc: 0.9067\n",
      "Epoch 149/180\n",
      "1350/1350 [==============================] - 0s 31us/sample - loss: 0.0059 - acc: 0.9748 - val_loss: 0.0176 - val_acc: 0.9067\n",
      "Epoch 150/180\n",
      "1350/1350 [==============================] - 0s 31us/sample - loss: 0.0058 - acc: 0.9756 - val_loss: 0.0175 - val_acc: 0.9067\n",
      "Epoch 151/180\n",
      "1350/1350 [==============================] - 0s 30us/sample - loss: 0.0058 - acc: 0.9756 - val_loss: 0.0174 - val_acc: 0.9067\n",
      "Epoch 152/180\n",
      "1350/1350 [==============================] - 0s 29us/sample - loss: 0.0058 - acc: 0.9763 - val_loss: 0.0175 - val_acc: 0.9067\n",
      "Epoch 153/180\n",
      "1350/1350 [==============================] - 0s 31us/sample - loss: 0.0057 - acc: 0.9756 - val_loss: 0.0176 - val_acc: 0.9067\n",
      "Epoch 154/180\n",
      "1350/1350 [==============================] - 0s 31us/sample - loss: 0.0057 - acc: 0.9763 - val_loss: 0.0174 - val_acc: 0.9067\n",
      "Epoch 155/180\n",
      "1350/1350 [==============================] - 0s 32us/sample - loss: 0.0056 - acc: 0.9763 - val_loss: 0.0175 - val_acc: 0.9067\n",
      "Epoch 156/180\n",
      "1350/1350 [==============================] - 0s 29us/sample - loss: 0.0056 - acc: 0.9756 - val_loss: 0.0175 - val_acc: 0.9067\n",
      "Epoch 157/180\n",
      "1350/1350 [==============================] - 0s 30us/sample - loss: 0.0056 - acc: 0.9763 - val_loss: 0.0176 - val_acc: 0.9067\n",
      "Epoch 158/180\n",
      "1350/1350 [==============================] - 0s 30us/sample - loss: 0.0055 - acc: 0.9763 - val_loss: 0.0175 - val_acc: 0.9067\n",
      "Epoch 159/180\n",
      "1350/1350 [==============================] - 0s 31us/sample - loss: 0.0055 - acc: 0.9756 - val_loss: 0.0175 - val_acc: 0.9067\n",
      "Epoch 160/180\n",
      "1350/1350 [==============================] - 0s 31us/sample - loss: 0.0054 - acc: 0.9763 - val_loss: 0.0175 - val_acc: 0.9067\n",
      "Epoch 161/180\n",
      "1350/1350 [==============================] - 0s 30us/sample - loss: 0.0054 - acc: 0.9770 - val_loss: 0.0175 - val_acc: 0.9067\n",
      "Epoch 162/180\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1350/1350 [==============================] - 0s 33us/sample - loss: 0.0054 - acc: 0.9770 - val_loss: 0.0174 - val_acc: 0.9067\n",
      "Epoch 163/180\n",
      "1350/1350 [==============================] - 0s 32us/sample - loss: 0.0053 - acc: 0.9770 - val_loss: 0.0175 - val_acc: 0.9067\n",
      "Epoch 164/180\n",
      "1350/1350 [==============================] - 0s 30us/sample - loss: 0.0053 - acc: 0.9770 - val_loss: 0.0174 - val_acc: 0.9067\n",
      "Epoch 165/180\n",
      "1350/1350 [==============================] - 0s 31us/sample - loss: 0.0053 - acc: 0.9770 - val_loss: 0.0174 - val_acc: 0.9067\n",
      "Epoch 166/180\n",
      "1350/1350 [==============================] - 0s 30us/sample - loss: 0.0052 - acc: 0.9770 - val_loss: 0.0173 - val_acc: 0.9067\n",
      "Epoch 167/180\n",
      "1350/1350 [==============================] - 0s 32us/sample - loss: 0.0052 - acc: 0.9778 - val_loss: 0.0174 - val_acc: 0.9067\n",
      "Epoch 168/180\n",
      "1350/1350 [==============================] - 0s 31us/sample - loss: 0.0052 - acc: 0.9778 - val_loss: 0.0175 - val_acc: 0.9067\n",
      "Epoch 169/180\n",
      "1350/1350 [==============================] - 0s 33us/sample - loss: 0.0051 - acc: 0.9778 - val_loss: 0.0173 - val_acc: 0.9067\n",
      "Epoch 170/180\n",
      "1350/1350 [==============================] - 0s 32us/sample - loss: 0.0051 - acc: 0.9770 - val_loss: 0.0174 - val_acc: 0.9067\n",
      "Epoch 171/180\n",
      "1350/1350 [==============================] - 0s 32us/sample - loss: 0.0051 - acc: 0.9770 - val_loss: 0.0175 - val_acc: 0.9067\n",
      "Epoch 172/180\n",
      "1350/1350 [==============================] - 0s 29us/sample - loss: 0.0050 - acc: 0.9778 - val_loss: 0.0176 - val_acc: 0.9067\n",
      "Epoch 173/180\n",
      "1350/1350 [==============================] - 0s 31us/sample - loss: 0.0050 - acc: 0.9770 - val_loss: 0.0175 - val_acc: 0.9067\n",
      "Epoch 174/180\n",
      "1350/1350 [==============================] - 0s 32us/sample - loss: 0.0050 - acc: 0.9770 - val_loss: 0.0175 - val_acc: 0.9067\n",
      "Epoch 175/180\n",
      "1350/1350 [==============================] - 0s 31us/sample - loss: 0.0049 - acc: 0.9778 - val_loss: 0.0174 - val_acc: 0.9067\n",
      "Epoch 176/180\n",
      "1350/1350 [==============================] - 0s 31us/sample - loss: 0.0049 - acc: 0.9778 - val_loss: 0.0174 - val_acc: 0.9067\n",
      "Epoch 177/180\n",
      "1350/1350 [==============================] - 0s 30us/sample - loss: 0.0049 - acc: 0.9778 - val_loss: 0.0175 - val_acc: 0.9000\n",
      "Epoch 178/180\n",
      "1350/1350 [==============================] - 0s 30us/sample - loss: 0.0049 - acc: 0.9770 - val_loss: 0.0174 - val_acc: 0.9000\n",
      "Epoch 179/180\n",
      "1350/1350 [==============================] - 0s 31us/sample - loss: 0.0048 - acc: 0.9778 - val_loss: 0.0174 - val_acc: 0.9000\n",
      "Epoch 180/180\n",
      "1350/1350 [==============================] - 0s 33us/sample - loss: 0.0048 - acc: 0.9778 - val_loss: 0.0175 - val_acc: 0.9000\n",
      "Elasped Time: 0:00:08.301623\n",
      "Training date and time : \n",
      "2020-04-25 21:09:07\n",
      "Train on 1350 samples, validate on 150 samples\n",
      "Epoch 1/190\n",
      "1350/1350 [==============================] - 0s 159us/sample - loss: 0.0685 - acc: 0.5348 - val_loss: 0.0664 - val_acc: 0.5133\n",
      "Epoch 2/190\n",
      "1350/1350 [==============================] - 0s 32us/sample - loss: 0.0662 - acc: 0.5585 - val_loss: 0.0644 - val_acc: 0.5067\n",
      "Epoch 3/190\n",
      "1350/1350 [==============================] - 0s 31us/sample - loss: 0.0639 - acc: 0.5704 - val_loss: 0.0625 - val_acc: 0.5333\n",
      "Epoch 4/190\n",
      "1350/1350 [==============================] - 0s 32us/sample - loss: 0.0615 - acc: 0.5867 - val_loss: 0.0605 - val_acc: 0.5467\n",
      "Epoch 5/190\n",
      "1350/1350 [==============================] - 0s 33us/sample - loss: 0.0591 - acc: 0.6141 - val_loss: 0.0585 - val_acc: 0.5800\n",
      "Epoch 6/190\n",
      "1350/1350 [==============================] - 0s 30us/sample - loss: 0.0567 - acc: 0.6363 - val_loss: 0.0565 - val_acc: 0.6000\n",
      "Epoch 7/190\n",
      "1350/1350 [==============================] - 0s 31us/sample - loss: 0.0542 - acc: 0.6681 - val_loss: 0.0545 - val_acc: 0.6133\n",
      "Epoch 8/190\n",
      "1350/1350 [==============================] - 0s 30us/sample - loss: 0.0518 - acc: 0.6970 - val_loss: 0.0525 - val_acc: 0.6533\n",
      "Epoch 9/190\n",
      "1350/1350 [==============================] - 0s 32us/sample - loss: 0.0494 - acc: 0.7163 - val_loss: 0.0506 - val_acc: 0.6800\n",
      "Epoch 10/190\n",
      "1350/1350 [==============================] - 0s 30us/sample - loss: 0.0472 - acc: 0.7356 - val_loss: 0.0488 - val_acc: 0.6867\n",
      "Epoch 11/190\n",
      "1350/1350 [==============================] - 0s 30us/sample - loss: 0.0450 - acc: 0.7548 - val_loss: 0.0470 - val_acc: 0.7000\n",
      "Epoch 12/190\n",
      "1350/1350 [==============================] - 0s 31us/sample - loss: 0.0429 - acc: 0.7837 - val_loss: 0.0454 - val_acc: 0.7200\n",
      "Epoch 13/190\n",
      "1350/1350 [==============================] - 0s 32us/sample - loss: 0.0409 - acc: 0.8119 - val_loss: 0.0437 - val_acc: 0.7333\n",
      "Epoch 14/190\n",
      "1350/1350 [==============================] - 0s 32us/sample - loss: 0.0389 - acc: 0.8252 - val_loss: 0.0422 - val_acc: 0.7533\n",
      "Epoch 15/190\n",
      "1350/1350 [==============================] - 0s 31us/sample - loss: 0.0371 - acc: 0.8356 - val_loss: 0.0406 - val_acc: 0.7667\n",
      "Epoch 16/190\n",
      "1350/1350 [==============================] - 0s 31us/sample - loss: 0.0353 - acc: 0.8452 - val_loss: 0.0392 - val_acc: 0.7933\n",
      "Epoch 17/190\n",
      "1350/1350 [==============================] - 0s 30us/sample - loss: 0.0337 - acc: 0.8511 - val_loss: 0.0379 - val_acc: 0.8133\n",
      "Epoch 18/190\n",
      "1350/1350 [==============================] - 0s 31us/sample - loss: 0.0322 - acc: 0.8607 - val_loss: 0.0367 - val_acc: 0.8200\n",
      "Epoch 19/190\n",
      "1350/1350 [==============================] - 0s 32us/sample - loss: 0.0308 - acc: 0.8630 - val_loss: 0.0357 - val_acc: 0.8133\n",
      "Epoch 20/190\n",
      "1350/1350 [==============================] - 0s 32us/sample - loss: 0.0295 - acc: 0.8689 - val_loss: 0.0349 - val_acc: 0.8200\n",
      "Epoch 21/190\n",
      "1350/1350 [==============================] - 0s 31us/sample - loss: 0.0283 - acc: 0.8696 - val_loss: 0.0341 - val_acc: 0.8067\n",
      "Epoch 22/190\n",
      "1350/1350 [==============================] - 0s 32us/sample - loss: 0.0272 - acc: 0.8756 - val_loss: 0.0332 - val_acc: 0.8200\n",
      "Epoch 23/190\n",
      "1350/1350 [==============================] - 0s 33us/sample - loss: 0.0262 - acc: 0.8793 - val_loss: 0.0326 - val_acc: 0.8200\n",
      "Epoch 24/190\n",
      "1350/1350 [==============================] - 0s 31us/sample - loss: 0.0254 - acc: 0.8830 - val_loss: 0.0319 - val_acc: 0.8133\n",
      "Epoch 25/190\n",
      "1350/1350 [==============================] - 0s 31us/sample - loss: 0.0245 - acc: 0.8830 - val_loss: 0.0314 - val_acc: 0.8200\n",
      "Epoch 26/190\n",
      "1350/1350 [==============================] - 0s 32us/sample - loss: 0.0238 - acc: 0.8844 - val_loss: 0.0310 - val_acc: 0.8200\n",
      "Epoch 27/190\n",
      "1350/1350 [==============================] - 0s 31us/sample - loss: 0.0230 - acc: 0.8889 - val_loss: 0.0303 - val_acc: 0.8200\n",
      "Epoch 28/190\n",
      "1350/1350 [==============================] - 0s 31us/sample - loss: 0.0223 - acc: 0.8911 - val_loss: 0.0300 - val_acc: 0.8200\n",
      "Epoch 29/190\n",
      "1350/1350 [==============================] - 0s 31us/sample - loss: 0.0217 - acc: 0.8911 - val_loss: 0.0297 - val_acc: 0.8267\n",
      "Epoch 30/190\n",
      "1350/1350 [==============================] - 0s 32us/sample - loss: 0.0211 - acc: 0.8970 - val_loss: 0.0292 - val_acc: 0.8333\n",
      "Epoch 31/190\n",
      "1350/1350 [==============================] - 0s 32us/sample - loss: 0.0206 - acc: 0.8956 - val_loss: 0.0288 - val_acc: 0.8200\n",
      "Epoch 32/190\n",
      "1350/1350 [==============================] - 0s 30us/sample - loss: 0.0200 - acc: 0.8970 - val_loss: 0.0287 - val_acc: 0.8200\n",
      "Epoch 33/190\n",
      "1350/1350 [==============================] - 0s 32us/sample - loss: 0.0196 - acc: 0.8985 - val_loss: 0.0284 - val_acc: 0.8267\n",
      "Epoch 34/190\n",
      "1350/1350 [==============================] - 0s 32us/sample - loss: 0.0191 - acc: 0.9022 - val_loss: 0.0280 - val_acc: 0.8200\n",
      "Epoch 35/190\n",
      "1350/1350 [==============================] - 0s 33us/sample - loss: 0.0187 - acc: 0.9037 - val_loss: 0.0278 - val_acc: 0.8200\n",
      "Epoch 36/190\n",
      "1350/1350 [==============================] - 0s 32us/sample - loss: 0.0182 - acc: 0.9104 - val_loss: 0.0275 - val_acc: 0.8267\n",
      "Epoch 37/190\n",
      "1350/1350 [==============================] - 0s 30us/sample - loss: 0.0178 - acc: 0.9074 - val_loss: 0.0273 - val_acc: 0.8333\n",
      "Epoch 38/190\n",
      "1350/1350 [==============================] - 0s 30us/sample - loss: 0.0175 - acc: 0.9111 - val_loss: 0.0271 - val_acc: 0.8267\n",
      "Epoch 39/190\n",
      "1350/1350 [==============================] - 0s 32us/sample - loss: 0.0172 - acc: 0.9126 - val_loss: 0.0268 - val_acc: 0.8267\n",
      "Epoch 40/190\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1350/1350 [==============================] - 0s 30us/sample - loss: 0.0168 - acc: 0.9148 - val_loss: 0.0266 - val_acc: 0.8333\n",
      "Epoch 41/190\n",
      "1350/1350 [==============================] - 0s 30us/sample - loss: 0.0165 - acc: 0.9156 - val_loss: 0.0266 - val_acc: 0.8200\n",
      "Epoch 42/190\n",
      "1350/1350 [==============================] - 0s 31us/sample - loss: 0.0162 - acc: 0.9207 - val_loss: 0.0264 - val_acc: 0.8267\n",
      "Epoch 43/190\n",
      "1350/1350 [==============================] - 0s 30us/sample - loss: 0.0159 - acc: 0.9178 - val_loss: 0.0262 - val_acc: 0.8200\n",
      "Epoch 44/190\n",
      "1350/1350 [==============================] - 0s 31us/sample - loss: 0.0156 - acc: 0.9207 - val_loss: 0.0259 - val_acc: 0.8333\n",
      "Epoch 45/190\n",
      "1350/1350 [==============================] - 0s 30us/sample - loss: 0.0153 - acc: 0.9215 - val_loss: 0.0259 - val_acc: 0.8267\n",
      "Epoch 46/190\n",
      "1350/1350 [==============================] - 0s 31us/sample - loss: 0.0150 - acc: 0.9222 - val_loss: 0.0257 - val_acc: 0.8333\n",
      "Epoch 47/190\n",
      "1350/1350 [==============================] - 0s 31us/sample - loss: 0.0148 - acc: 0.9252 - val_loss: 0.0256 - val_acc: 0.8333\n",
      "Epoch 48/190\n",
      "1350/1350 [==============================] - 0s 29us/sample - loss: 0.0146 - acc: 0.9237 - val_loss: 0.0256 - val_acc: 0.8333\n",
      "Epoch 49/190\n",
      "1350/1350 [==============================] - 0s 32us/sample - loss: 0.0143 - acc: 0.9267 - val_loss: 0.0253 - val_acc: 0.8400\n",
      "Epoch 50/190\n",
      "1350/1350 [==============================] - 0s 31us/sample - loss: 0.0141 - acc: 0.9259 - val_loss: 0.0251 - val_acc: 0.8267\n",
      "Epoch 51/190\n",
      "1350/1350 [==============================] - 0s 32us/sample - loss: 0.0139 - acc: 0.9281 - val_loss: 0.0251 - val_acc: 0.8400\n",
      "Epoch 52/190\n",
      "1350/1350 [==============================] - 0s 31us/sample - loss: 0.0137 - acc: 0.9296 - val_loss: 0.0248 - val_acc: 0.8400\n",
      "Epoch 53/190\n",
      "1350/1350 [==============================] - 0s 31us/sample - loss: 0.0135 - acc: 0.9296 - val_loss: 0.0247 - val_acc: 0.8400\n",
      "Epoch 54/190\n",
      "1350/1350 [==============================] - 0s 32us/sample - loss: 0.0133 - acc: 0.9296 - val_loss: 0.0247 - val_acc: 0.8400\n",
      "Epoch 55/190\n",
      "1350/1350 [==============================] - 0s 31us/sample - loss: 0.0131 - acc: 0.9311 - val_loss: 0.0246 - val_acc: 0.8400\n",
      "Epoch 56/190\n",
      "1350/1350 [==============================] - 0s 32us/sample - loss: 0.0129 - acc: 0.9326 - val_loss: 0.0245 - val_acc: 0.8400\n",
      "Epoch 57/190\n",
      "1350/1350 [==============================] - 0s 32us/sample - loss: 0.0127 - acc: 0.9319 - val_loss: 0.0242 - val_acc: 0.8400\n",
      "Epoch 58/190\n",
      "1350/1350 [==============================] - 0s 32us/sample - loss: 0.0126 - acc: 0.9311 - val_loss: 0.0242 - val_acc: 0.8333\n",
      "Epoch 59/190\n",
      "1350/1350 [==============================] - 0s 31us/sample - loss: 0.0124 - acc: 0.9326 - val_loss: 0.0241 - val_acc: 0.8400\n",
      "Epoch 60/190\n",
      "1350/1350 [==============================] - 0s 31us/sample - loss: 0.0122 - acc: 0.9356 - val_loss: 0.0241 - val_acc: 0.8400\n",
      "Epoch 61/190\n",
      "1350/1350 [==============================] - 0s 31us/sample - loss: 0.0121 - acc: 0.9341 - val_loss: 0.0238 - val_acc: 0.8400\n",
      "Epoch 62/190\n",
      "1350/1350 [==============================] - 0s 30us/sample - loss: 0.0119 - acc: 0.9370 - val_loss: 0.0240 - val_acc: 0.8333\n",
      "Epoch 63/190\n",
      "1350/1350 [==============================] - 0s 31us/sample - loss: 0.0118 - acc: 0.9378 - val_loss: 0.0239 - val_acc: 0.8400\n",
      "Epoch 64/190\n",
      "1350/1350 [==============================] - 0s 32us/sample - loss: 0.0116 - acc: 0.9385 - val_loss: 0.0237 - val_acc: 0.8400\n",
      "Epoch 65/190\n",
      "1350/1350 [==============================] - 0s 31us/sample - loss: 0.0115 - acc: 0.9407 - val_loss: 0.0235 - val_acc: 0.8400\n",
      "Epoch 66/190\n",
      "1350/1350 [==============================] - 0s 32us/sample - loss: 0.0113 - acc: 0.9422 - val_loss: 0.0235 - val_acc: 0.8400\n",
      "Epoch 67/190\n",
      "1350/1350 [==============================] - 0s 31us/sample - loss: 0.0112 - acc: 0.9400 - val_loss: 0.0234 - val_acc: 0.8400\n",
      "Epoch 68/190\n",
      "1350/1350 [==============================] - 0s 34us/sample - loss: 0.0111 - acc: 0.9400 - val_loss: 0.0232 - val_acc: 0.8400\n",
      "Epoch 69/190\n",
      "1350/1350 [==============================] - 0s 32us/sample - loss: 0.0110 - acc: 0.9422 - val_loss: 0.0232 - val_acc: 0.8400\n",
      "Epoch 70/190\n",
      "1350/1350 [==============================] - 0s 33us/sample - loss: 0.0109 - acc: 0.9422 - val_loss: 0.0233 - val_acc: 0.8400\n",
      "Epoch 71/190\n",
      "1350/1350 [==============================] - 0s 33us/sample - loss: 0.0107 - acc: 0.9437 - val_loss: 0.0230 - val_acc: 0.8400\n",
      "Epoch 72/190\n",
      "1350/1350 [==============================] - 0s 31us/sample - loss: 0.0106 - acc: 0.9444 - val_loss: 0.0231 - val_acc: 0.8400\n",
      "Epoch 73/190\n",
      "1350/1350 [==============================] - 0s 31us/sample - loss: 0.0105 - acc: 0.9452 - val_loss: 0.0230 - val_acc: 0.8400\n",
      "Epoch 74/190\n",
      "1350/1350 [==============================] - 0s 31us/sample - loss: 0.0104 - acc: 0.9474 - val_loss: 0.0230 - val_acc: 0.8400\n",
      "Epoch 75/190\n",
      "1350/1350 [==============================] - 0s 34us/sample - loss: 0.0102 - acc: 0.9467 - val_loss: 0.0228 - val_acc: 0.8533\n",
      "Epoch 76/190\n",
      "1350/1350 [==============================] - 0s 30us/sample - loss: 0.0101 - acc: 0.9481 - val_loss: 0.0227 - val_acc: 0.8467\n",
      "Epoch 77/190\n",
      "1350/1350 [==============================] - 0s 33us/sample - loss: 0.0101 - acc: 0.9474 - val_loss: 0.0226 - val_acc: 0.8467\n",
      "Epoch 78/190\n",
      "1350/1350 [==============================] - 0s 29us/sample - loss: 0.0099 - acc: 0.9504 - val_loss: 0.0226 - val_acc: 0.8533\n",
      "Epoch 79/190\n",
      "1350/1350 [==============================] - 0s 30us/sample - loss: 0.0099 - acc: 0.9496 - val_loss: 0.0226 - val_acc: 0.8533\n",
      "Epoch 80/190\n",
      "1350/1350 [==============================] - 0s 30us/sample - loss: 0.0097 - acc: 0.9496 - val_loss: 0.0224 - val_acc: 0.8533\n",
      "Epoch 81/190\n",
      "1350/1350 [==============================] - 0s 33us/sample - loss: 0.0097 - acc: 0.9489 - val_loss: 0.0224 - val_acc: 0.8533\n",
      "Epoch 82/190\n",
      "1350/1350 [==============================] - 0s 31us/sample - loss: 0.0096 - acc: 0.9504 - val_loss: 0.0224 - val_acc: 0.8600\n",
      "Epoch 83/190\n",
      "1350/1350 [==============================] - 0s 32us/sample - loss: 0.0095 - acc: 0.9511 - val_loss: 0.0222 - val_acc: 0.8600\n",
      "Epoch 84/190\n",
      "1350/1350 [==============================] - 0s 31us/sample - loss: 0.0094 - acc: 0.9496 - val_loss: 0.0222 - val_acc: 0.8600\n",
      "Epoch 85/190\n",
      "1350/1350 [==============================] - 0s 30us/sample - loss: 0.0093 - acc: 0.9519 - val_loss: 0.0221 - val_acc: 0.8600\n",
      "Epoch 86/190\n",
      "1350/1350 [==============================] - 0s 33us/sample - loss: 0.0092 - acc: 0.9511 - val_loss: 0.0222 - val_acc: 0.8667\n",
      "Epoch 87/190\n",
      "1350/1350 [==============================] - 0s 31us/sample - loss: 0.0091 - acc: 0.9519 - val_loss: 0.0220 - val_acc: 0.8600\n",
      "Epoch 88/190\n",
      "1350/1350 [==============================] - 0s 31us/sample - loss: 0.0090 - acc: 0.9533 - val_loss: 0.0219 - val_acc: 0.8667\n",
      "Epoch 89/190\n",
      "1350/1350 [==============================] - 0s 31us/sample - loss: 0.0089 - acc: 0.9511 - val_loss: 0.0219 - val_acc: 0.8667\n",
      "Epoch 90/190\n",
      "1350/1350 [==============================] - 0s 31us/sample - loss: 0.0089 - acc: 0.9541 - val_loss: 0.0220 - val_acc: 0.8667\n",
      "Epoch 91/190\n",
      "1350/1350 [==============================] - 0s 31us/sample - loss: 0.0088 - acc: 0.9556 - val_loss: 0.0217 - val_acc: 0.8667\n",
      "Epoch 92/190\n",
      "1350/1350 [==============================] - 0s 31us/sample - loss: 0.0087 - acc: 0.9556 - val_loss: 0.0216 - val_acc: 0.8600\n",
      "Epoch 93/190\n",
      "1350/1350 [==============================] - 0s 32us/sample - loss: 0.0086 - acc: 0.9548 - val_loss: 0.0217 - val_acc: 0.8667\n",
      "Epoch 94/190\n",
      "1350/1350 [==============================] - 0s 32us/sample - loss: 0.0086 - acc: 0.9563 - val_loss: 0.0217 - val_acc: 0.8667\n",
      "Epoch 95/190\n",
      "1350/1350 [==============================] - 0s 32us/sample - loss: 0.0085 - acc: 0.9563 - val_loss: 0.0217 - val_acc: 0.8667\n",
      "Epoch 96/190\n",
      "1350/1350 [==============================] - 0s 31us/sample - loss: 0.0084 - acc: 0.9563 - val_loss: 0.0217 - val_acc: 0.8667\n",
      "Epoch 97/190\n",
      "1350/1350 [==============================] - 0s 30us/sample - loss: 0.0083 - acc: 0.9578 - val_loss: 0.0218 - val_acc: 0.8667\n",
      "Epoch 98/190\n",
      "1350/1350 [==============================] - 0s 32us/sample - loss: 0.0083 - acc: 0.9578 - val_loss: 0.0216 - val_acc: 0.8667\n",
      "Epoch 99/190\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1350/1350 [==============================] - 0s 31us/sample - loss: 0.0082 - acc: 0.9578 - val_loss: 0.0214 - val_acc: 0.8667\n",
      "Epoch 100/190\n",
      "1350/1350 [==============================] - 0s 30us/sample - loss: 0.0081 - acc: 0.9593 - val_loss: 0.0215 - val_acc: 0.8667\n",
      "Epoch 101/190\n",
      "1350/1350 [==============================] - 0s 32us/sample - loss: 0.0080 - acc: 0.9607 - val_loss: 0.0213 - val_acc: 0.8667\n",
      "Epoch 102/190\n",
      "1350/1350 [==============================] - 0s 31us/sample - loss: 0.0080 - acc: 0.9593 - val_loss: 0.0215 - val_acc: 0.8667\n",
      "Epoch 103/190\n",
      "1350/1350 [==============================] - 0s 31us/sample - loss: 0.0079 - acc: 0.9600 - val_loss: 0.0214 - val_acc: 0.8667\n",
      "Epoch 104/190\n",
      "1350/1350 [==============================] - 0s 32us/sample - loss: 0.0079 - acc: 0.9600 - val_loss: 0.0215 - val_acc: 0.8667\n",
      "Epoch 105/190\n",
      "1350/1350 [==============================] - 0s 31us/sample - loss: 0.0078 - acc: 0.9615 - val_loss: 0.0215 - val_acc: 0.8667\n",
      "Epoch 106/190\n",
      "1350/1350 [==============================] - 0s 30us/sample - loss: 0.0077 - acc: 0.9622 - val_loss: 0.0213 - val_acc: 0.8667\n",
      "Epoch 107/190\n",
      "1350/1350 [==============================] - 0s 31us/sample - loss: 0.0076 - acc: 0.9622 - val_loss: 0.0211 - val_acc: 0.8667\n",
      "Epoch 108/190\n",
      "1350/1350 [==============================] - 0s 32us/sample - loss: 0.0076 - acc: 0.9615 - val_loss: 0.0212 - val_acc: 0.8667\n",
      "Epoch 109/190\n",
      "1350/1350 [==============================] - 0s 32us/sample - loss: 0.0075 - acc: 0.9615 - val_loss: 0.0211 - val_acc: 0.8667\n",
      "Epoch 110/190\n",
      "1350/1350 [==============================] - 0s 31us/sample - loss: 0.0075 - acc: 0.9630 - val_loss: 0.0210 - val_acc: 0.8667\n",
      "Epoch 111/190\n",
      "1350/1350 [==============================] - 0s 31us/sample - loss: 0.0074 - acc: 0.9630 - val_loss: 0.0210 - val_acc: 0.8667\n",
      "Epoch 112/190\n",
      "1350/1350 [==============================] - 0s 31us/sample - loss: 0.0073 - acc: 0.9622 - val_loss: 0.0211 - val_acc: 0.8667\n",
      "Epoch 113/190\n",
      "1350/1350 [==============================] - 0s 33us/sample - loss: 0.0073 - acc: 0.9630 - val_loss: 0.0210 - val_acc: 0.8667\n",
      "Epoch 114/190\n",
      "1350/1350 [==============================] - 0s 31us/sample - loss: 0.0072 - acc: 0.9637 - val_loss: 0.0211 - val_acc: 0.8667\n",
      "Epoch 115/190\n",
      "1350/1350 [==============================] - 0s 31us/sample - loss: 0.0072 - acc: 0.9630 - val_loss: 0.0210 - val_acc: 0.8667\n",
      "Epoch 116/190\n",
      "1350/1350 [==============================] - 0s 30us/sample - loss: 0.0071 - acc: 0.9644 - val_loss: 0.0208 - val_acc: 0.8667\n",
      "Epoch 117/190\n",
      "1350/1350 [==============================] - 0s 32us/sample - loss: 0.0071 - acc: 0.9644 - val_loss: 0.0209 - val_acc: 0.8667\n",
      "Epoch 118/190\n",
      "1350/1350 [==============================] - 0s 31us/sample - loss: 0.0070 - acc: 0.9644 - val_loss: 0.0208 - val_acc: 0.8667\n",
      "Epoch 119/190\n",
      "1350/1350 [==============================] - 0s 32us/sample - loss: 0.0070 - acc: 0.9644 - val_loss: 0.0206 - val_acc: 0.8667\n",
      "Epoch 120/190\n",
      "1350/1350 [==============================] - 0s 31us/sample - loss: 0.0069 - acc: 0.9659 - val_loss: 0.0206 - val_acc: 0.8667\n",
      "Epoch 121/190\n",
      "1350/1350 [==============================] - 0s 30us/sample - loss: 0.0069 - acc: 0.9652 - val_loss: 0.0205 - val_acc: 0.8667\n",
      "Epoch 122/190\n",
      "1350/1350 [==============================] - 0s 31us/sample - loss: 0.0068 - acc: 0.9652 - val_loss: 0.0207 - val_acc: 0.8667\n",
      "Epoch 123/190\n",
      "1350/1350 [==============================] - 0s 30us/sample - loss: 0.0067 - acc: 0.9667 - val_loss: 0.0207 - val_acc: 0.8667\n",
      "Epoch 124/190\n",
      "1350/1350 [==============================] - 0s 32us/sample - loss: 0.0067 - acc: 0.9659 - val_loss: 0.0205 - val_acc: 0.8667\n",
      "Epoch 125/190\n",
      "1350/1350 [==============================] - 0s 31us/sample - loss: 0.0066 - acc: 0.9667 - val_loss: 0.0206 - val_acc: 0.8667\n",
      "Epoch 126/190\n",
      "1350/1350 [==============================] - 0s 31us/sample - loss: 0.0066 - acc: 0.9667 - val_loss: 0.0205 - val_acc: 0.8667\n",
      "Epoch 127/190\n",
      "1350/1350 [==============================] - 0s 33us/sample - loss: 0.0065 - acc: 0.9681 - val_loss: 0.0204 - val_acc: 0.8667\n",
      "Epoch 128/190\n",
      "1350/1350 [==============================] - 0s 31us/sample - loss: 0.0065 - acc: 0.9681 - val_loss: 0.0204 - val_acc: 0.8667\n",
      "Epoch 129/190\n",
      "1350/1350 [==============================] - 0s 32us/sample - loss: 0.0065 - acc: 0.9681 - val_loss: 0.0205 - val_acc: 0.8667\n",
      "Epoch 130/190\n",
      "1350/1350 [==============================] - 0s 32us/sample - loss: 0.0064 - acc: 0.9681 - val_loss: 0.0203 - val_acc: 0.8667\n",
      "Epoch 131/190\n",
      "1350/1350 [==============================] - 0s 34us/sample - loss: 0.0063 - acc: 0.9681 - val_loss: 0.0205 - val_acc: 0.8667\n",
      "Epoch 132/190\n",
      "1350/1350 [==============================] - 0s 31us/sample - loss: 0.0063 - acc: 0.9681 - val_loss: 0.0203 - val_acc: 0.8667\n",
      "Epoch 133/190\n",
      "1350/1350 [==============================] - 0s 31us/sample - loss: 0.0062 - acc: 0.9681 - val_loss: 0.0205 - val_acc: 0.8667\n",
      "Epoch 134/190\n",
      "1350/1350 [==============================] - 0s 33us/sample - loss: 0.0062 - acc: 0.9681 - val_loss: 0.0202 - val_acc: 0.8667\n",
      "Epoch 135/190\n",
      "1350/1350 [==============================] - 0s 31us/sample - loss: 0.0062 - acc: 0.9681 - val_loss: 0.0201 - val_acc: 0.8667\n",
      "Epoch 136/190\n",
      "1350/1350 [==============================] - 0s 32us/sample - loss: 0.0061 - acc: 0.9689 - val_loss: 0.0201 - val_acc: 0.8667\n",
      "Epoch 137/190\n",
      "1350/1350 [==============================] - 0s 31us/sample - loss: 0.0061 - acc: 0.9696 - val_loss: 0.0203 - val_acc: 0.8667\n",
      "Epoch 138/190\n",
      "1350/1350 [==============================] - 0s 32us/sample - loss: 0.0060 - acc: 0.9719 - val_loss: 0.0201 - val_acc: 0.8667\n",
      "Epoch 139/190\n",
      "1350/1350 [==============================] - 0s 30us/sample - loss: 0.0060 - acc: 0.9696 - val_loss: 0.0201 - val_acc: 0.8667\n",
      "Epoch 140/190\n",
      "1350/1350 [==============================] - 0s 30us/sample - loss: 0.0059 - acc: 0.9704 - val_loss: 0.0199 - val_acc: 0.8667\n",
      "Epoch 141/190\n",
      "1350/1350 [==============================] - 0s 31us/sample - loss: 0.0059 - acc: 0.9704 - val_loss: 0.0201 - val_acc: 0.8667\n",
      "Epoch 142/190\n",
      "1350/1350 [==============================] - 0s 30us/sample - loss: 0.0058 - acc: 0.9711 - val_loss: 0.0200 - val_acc: 0.8667\n",
      "Epoch 143/190\n",
      "1350/1350 [==============================] - 0s 30us/sample - loss: 0.0058 - acc: 0.9704 - val_loss: 0.0200 - val_acc: 0.8667\n",
      "Epoch 144/190\n",
      "1350/1350 [==============================] - 0s 32us/sample - loss: 0.0058 - acc: 0.9741 - val_loss: 0.0199 - val_acc: 0.8667\n",
      "Epoch 145/190\n",
      "1350/1350 [==============================] - 0s 32us/sample - loss: 0.0057 - acc: 0.9726 - val_loss: 0.0199 - val_acc: 0.8667\n",
      "Epoch 146/190\n",
      "1350/1350 [==============================] - 0s 32us/sample - loss: 0.0057 - acc: 0.9733 - val_loss: 0.0198 - val_acc: 0.8667\n",
      "Epoch 147/190\n",
      "1350/1350 [==============================] - 0s 32us/sample - loss: 0.0056 - acc: 0.9726 - val_loss: 0.0200 - val_acc: 0.8667\n",
      "Epoch 148/190\n",
      "1350/1350 [==============================] - 0s 30us/sample - loss: 0.0056 - acc: 0.9726 - val_loss: 0.0198 - val_acc: 0.8667\n",
      "Epoch 149/190\n",
      "1350/1350 [==============================] - 0s 30us/sample - loss: 0.0056 - acc: 0.9748 - val_loss: 0.0197 - val_acc: 0.8667\n",
      "Epoch 150/190\n",
      "1350/1350 [==============================] - 0s 31us/sample - loss: 0.0055 - acc: 0.9741 - val_loss: 0.0197 - val_acc: 0.8667\n",
      "Epoch 151/190\n",
      "1350/1350 [==============================] - 0s 33us/sample - loss: 0.0055 - acc: 0.9741 - val_loss: 0.0198 - val_acc: 0.8667\n",
      "Epoch 152/190\n",
      "1350/1350 [==============================] - 0s 30us/sample - loss: 0.0054 - acc: 0.9756 - val_loss: 0.0197 - val_acc: 0.8733\n",
      "Epoch 153/190\n",
      "1350/1350 [==============================] - 0s 32us/sample - loss: 0.0054 - acc: 0.9770 - val_loss: 0.0196 - val_acc: 0.8667\n",
      "Epoch 154/190\n",
      "1350/1350 [==============================] - 0s 29us/sample - loss: 0.0053 - acc: 0.9748 - val_loss: 0.0196 - val_acc: 0.8667\n",
      "Epoch 155/190\n",
      "1350/1350 [==============================] - 0s 31us/sample - loss: 0.0053 - acc: 0.9778 - val_loss: 0.0196 - val_acc: 0.8667\n",
      "Epoch 156/190\n",
      "1350/1350 [==============================] - 0s 30us/sample - loss: 0.0053 - acc: 0.9785 - val_loss: 0.0195 - val_acc: 0.8733\n",
      "Epoch 157/190\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1350/1350 [==============================] - 0s 31us/sample - loss: 0.0052 - acc: 0.9785 - val_loss: 0.0195 - val_acc: 0.8733\n",
      "Epoch 158/190\n",
      "1350/1350 [==============================] - 0s 32us/sample - loss: 0.0052 - acc: 0.9785 - val_loss: 0.0196 - val_acc: 0.8733\n",
      "Epoch 159/190\n",
      "1350/1350 [==============================] - 0s 30us/sample - loss: 0.0052 - acc: 0.9770 - val_loss: 0.0196 - val_acc: 0.8733\n",
      "Epoch 160/190\n",
      "1350/1350 [==============================] - 0s 31us/sample - loss: 0.0051 - acc: 0.9793 - val_loss: 0.0194 - val_acc: 0.8733\n",
      "Epoch 161/190\n",
      "1350/1350 [==============================] - 0s 34us/sample - loss: 0.0051 - acc: 0.9785 - val_loss: 0.0194 - val_acc: 0.8733\n",
      "Epoch 162/190\n",
      "1350/1350 [==============================] - 0s 32us/sample - loss: 0.0050 - acc: 0.9793 - val_loss: 0.0194 - val_acc: 0.8800\n",
      "Epoch 163/190\n",
      "1350/1350 [==============================] - 0s 33us/sample - loss: 0.0050 - acc: 0.9800 - val_loss: 0.0192 - val_acc: 0.8800\n",
      "Epoch 164/190\n",
      "1350/1350 [==============================] - 0s 30us/sample - loss: 0.0050 - acc: 0.9800 - val_loss: 0.0194 - val_acc: 0.8800\n",
      "Epoch 165/190\n",
      "1350/1350 [==============================] - 0s 31us/sample - loss: 0.0049 - acc: 0.9807 - val_loss: 0.0193 - val_acc: 0.8800\n",
      "Epoch 166/190\n",
      "1350/1350 [==============================] - 0s 30us/sample - loss: 0.0049 - acc: 0.9807 - val_loss: 0.0193 - val_acc: 0.8800\n",
      "Epoch 167/190\n",
      "1350/1350 [==============================] - 0s 29us/sample - loss: 0.0049 - acc: 0.9800 - val_loss: 0.0194 - val_acc: 0.8800\n",
      "Epoch 168/190\n",
      "1350/1350 [==============================] - 0s 32us/sample - loss: 0.0048 - acc: 0.9807 - val_loss: 0.0194 - val_acc: 0.8800\n",
      "Epoch 169/190\n",
      "1350/1350 [==============================] - 0s 30us/sample - loss: 0.0048 - acc: 0.9815 - val_loss: 0.0191 - val_acc: 0.8800\n",
      "Epoch 170/190\n",
      "1350/1350 [==============================] - 0s 32us/sample - loss: 0.0048 - acc: 0.9822 - val_loss: 0.0192 - val_acc: 0.8800\n",
      "Epoch 171/190\n",
      "1350/1350 [==============================] - 0s 31us/sample - loss: 0.0047 - acc: 0.9830 - val_loss: 0.0192 - val_acc: 0.8800\n",
      "Epoch 172/190\n",
      "1350/1350 [==============================] - 0s 31us/sample - loss: 0.0047 - acc: 0.9830 - val_loss: 0.0192 - val_acc: 0.8800\n",
      "Epoch 173/190\n",
      "1350/1350 [==============================] - 0s 30us/sample - loss: 0.0047 - acc: 0.9822 - val_loss: 0.0192 - val_acc: 0.8800\n",
      "Epoch 174/190\n",
      "1350/1350 [==============================] - 0s 31us/sample - loss: 0.0046 - acc: 0.9830 - val_loss: 0.0192 - val_acc: 0.8800\n",
      "Epoch 175/190\n",
      "1350/1350 [==============================] - 0s 32us/sample - loss: 0.0046 - acc: 0.9837 - val_loss: 0.0191 - val_acc: 0.8800\n",
      "Epoch 176/190\n",
      "1350/1350 [==============================] - 0s 32us/sample - loss: 0.0046 - acc: 0.9837 - val_loss: 0.0192 - val_acc: 0.8800\n",
      "Epoch 177/190\n",
      "1350/1350 [==============================] - 0s 29us/sample - loss: 0.0045 - acc: 0.9837 - val_loss: 0.0190 - val_acc: 0.8800\n",
      "Epoch 178/190\n",
      "1350/1350 [==============================] - 0s 30us/sample - loss: 0.0045 - acc: 0.9830 - val_loss: 0.0190 - val_acc: 0.8800\n",
      "Epoch 179/190\n",
      "1350/1350 [==============================] - 0s 30us/sample - loss: 0.0045 - acc: 0.9830 - val_loss: 0.0192 - val_acc: 0.8800\n",
      "Epoch 180/190\n",
      "1350/1350 [==============================] - 0s 31us/sample - loss: 0.0044 - acc: 0.9837 - val_loss: 0.0191 - val_acc: 0.8800\n",
      "Epoch 181/190\n",
      "1350/1350 [==============================] - 0s 30us/sample - loss: 0.0044 - acc: 0.9837 - val_loss: 0.0189 - val_acc: 0.8800\n",
      "Epoch 182/190\n",
      "1350/1350 [==============================] - 0s 31us/sample - loss: 0.0044 - acc: 0.9837 - val_loss: 0.0190 - val_acc: 0.8800\n",
      "Epoch 183/190\n",
      "1350/1350 [==============================] - 0s 30us/sample - loss: 0.0044 - acc: 0.9837 - val_loss: 0.0190 - val_acc: 0.8800\n",
      "Epoch 184/190\n",
      "1350/1350 [==============================] - 0s 31us/sample - loss: 0.0043 - acc: 0.9844 - val_loss: 0.0189 - val_acc: 0.8800\n",
      "Epoch 185/190\n",
      "1350/1350 [==============================] - 0s 31us/sample - loss: 0.0043 - acc: 0.9837 - val_loss: 0.0188 - val_acc: 0.8800\n",
      "Epoch 186/190\n",
      "1350/1350 [==============================] - 0s 32us/sample - loss: 0.0043 - acc: 0.9852 - val_loss: 0.0188 - val_acc: 0.8800\n",
      "Epoch 187/190\n",
      "1350/1350 [==============================] - 0s 32us/sample - loss: 0.0042 - acc: 0.9844 - val_loss: 0.0189 - val_acc: 0.8800\n",
      "Epoch 188/190\n",
      "1350/1350 [==============================] - 0s 32us/sample - loss: 0.0042 - acc: 0.9852 - val_loss: 0.0188 - val_acc: 0.8800\n",
      "Epoch 189/190\n",
      "1350/1350 [==============================] - 0s 32us/sample - loss: 0.0042 - acc: 0.9852 - val_loss: 0.0188 - val_acc: 0.8800\n",
      "Epoch 190/190\n",
      "1350/1350 [==============================] - 0s 31us/sample - loss: 0.0041 - acc: 0.9852 - val_loss: 0.0188 - val_acc: 0.8800\n",
      "Elasped Time: 0:00:08.725237\n",
      "Training date and time : \n",
      "2020-04-25 21:09:16\n",
      "Train on 1350 samples, validate on 150 samples\n",
      "Epoch 1/200\n",
      "1350/1350 [==============================] - 0s 163us/sample - loss: 0.0676 - acc: 0.5326 - val_loss: 0.0728 - val_acc: 0.4333\n",
      "Epoch 2/200\n",
      "1350/1350 [==============================] - 0s 32us/sample - loss: 0.0656 - acc: 0.5644 - val_loss: 0.0714 - val_acc: 0.4467\n",
      "Epoch 3/200\n",
      "1350/1350 [==============================] - 0s 33us/sample - loss: 0.0635 - acc: 0.5933 - val_loss: 0.0699 - val_acc: 0.4667\n",
      "Epoch 4/200\n",
      "1350/1350 [==============================] - 0s 32us/sample - loss: 0.0614 - acc: 0.6074 - val_loss: 0.0685 - val_acc: 0.5000\n",
      "Epoch 5/200\n",
      "1350/1350 [==============================] - 0s 31us/sample - loss: 0.0593 - acc: 0.6267 - val_loss: 0.0670 - val_acc: 0.5400\n",
      "Epoch 6/200\n",
      "1350/1350 [==============================] - 0s 30us/sample - loss: 0.0573 - acc: 0.6585 - val_loss: 0.0655 - val_acc: 0.5600\n",
      "Epoch 7/200\n",
      "1350/1350 [==============================] - 0s 28us/sample - loss: 0.0552 - acc: 0.6733 - val_loss: 0.0640 - val_acc: 0.5800\n",
      "Epoch 8/200\n",
      "1350/1350 [==============================] - 0s 31us/sample - loss: 0.0532 - acc: 0.6904 - val_loss: 0.0625 - val_acc: 0.5867\n",
      "Epoch 9/200\n",
      "1350/1350 [==============================] - 0s 30us/sample - loss: 0.0513 - acc: 0.7111 - val_loss: 0.0610 - val_acc: 0.6200\n",
      "Epoch 10/200\n",
      "1350/1350 [==============================] - 0s 31us/sample - loss: 0.0493 - acc: 0.7333 - val_loss: 0.0595 - val_acc: 0.6333\n",
      "Epoch 11/200\n",
      "1350/1350 [==============================] - 0s 31us/sample - loss: 0.0473 - acc: 0.7459 - val_loss: 0.0580 - val_acc: 0.6400\n",
      "Epoch 12/200\n",
      "1350/1350 [==============================] - 0s 30us/sample - loss: 0.0455 - acc: 0.7681 - val_loss: 0.0564 - val_acc: 0.6533\n",
      "Epoch 13/200\n",
      "1350/1350 [==============================] - 0s 32us/sample - loss: 0.0436 - acc: 0.7815 - val_loss: 0.0550 - val_acc: 0.6933\n",
      "Epoch 14/200\n",
      "1350/1350 [==============================] - 0s 34us/sample - loss: 0.0419 - acc: 0.7933 - val_loss: 0.0535 - val_acc: 0.7000\n",
      "Epoch 15/200\n",
      "1350/1350 [==============================] - 0s 32us/sample - loss: 0.0402 - acc: 0.8081 - val_loss: 0.0521 - val_acc: 0.7133\n",
      "Epoch 16/200\n",
      "1350/1350 [==============================] - 0s 30us/sample - loss: 0.0387 - acc: 0.8148 - val_loss: 0.0509 - val_acc: 0.7400\n",
      "Epoch 17/200\n",
      "1350/1350 [==============================] - 0s 31us/sample - loss: 0.0372 - acc: 0.8193 - val_loss: 0.0497 - val_acc: 0.7533\n",
      "Epoch 18/200\n",
      "1350/1350 [==============================] - 0s 32us/sample - loss: 0.0358 - acc: 0.8274 - val_loss: 0.0484 - val_acc: 0.7467\n",
      "Epoch 19/200\n",
      "1350/1350 [==============================] - 0s 30us/sample - loss: 0.0346 - acc: 0.8363 - val_loss: 0.0472 - val_acc: 0.7600\n",
      "Epoch 20/200\n",
      "1350/1350 [==============================] - 0s 30us/sample - loss: 0.0334 - acc: 0.8363 - val_loss: 0.0463 - val_acc: 0.7733\n",
      "Epoch 21/200\n",
      "1350/1350 [==============================] - 0s 30us/sample - loss: 0.0323 - acc: 0.8378 - val_loss: 0.0454 - val_acc: 0.7533\n",
      "Epoch 22/200\n",
      "1350/1350 [==============================] - 0s 33us/sample - loss: 0.0313 - acc: 0.8452 - val_loss: 0.0443 - val_acc: 0.7667\n",
      "Epoch 23/200\n",
      "1350/1350 [==============================] - 0s 34us/sample - loss: 0.0303 - acc: 0.8459 - val_loss: 0.0435 - val_acc: 0.7600\n",
      "Epoch 24/200\n",
      "1350/1350 [==============================] - 0s 33us/sample - loss: 0.0294 - acc: 0.8556 - val_loss: 0.0427 - val_acc: 0.7733\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 25/200\n",
      "1350/1350 [==============================] - 0s 32us/sample - loss: 0.0287 - acc: 0.8585 - val_loss: 0.0420 - val_acc: 0.7667\n",
      "Epoch 26/200\n",
      "1350/1350 [==============================] - 0s 31us/sample - loss: 0.0279 - acc: 0.8585 - val_loss: 0.0413 - val_acc: 0.7667\n",
      "Epoch 27/200\n",
      "1350/1350 [==============================] - 0s 31us/sample - loss: 0.0272 - acc: 0.8600 - val_loss: 0.0405 - val_acc: 0.7733\n",
      "Epoch 28/200\n",
      "1350/1350 [==============================] - 0s 31us/sample - loss: 0.0265 - acc: 0.8637 - val_loss: 0.0399 - val_acc: 0.7800\n",
      "Epoch 29/200\n",
      "1350/1350 [==============================] - 0s 30us/sample - loss: 0.0259 - acc: 0.8652 - val_loss: 0.0393 - val_acc: 0.7800\n",
      "Epoch 30/200\n",
      "1350/1350 [==============================] - 0s 33us/sample - loss: 0.0253 - acc: 0.8652 - val_loss: 0.0387 - val_acc: 0.7867\n",
      "Epoch 31/200\n",
      "1350/1350 [==============================] - 0s 30us/sample - loss: 0.0248 - acc: 0.8689 - val_loss: 0.0382 - val_acc: 0.7867\n",
      "Epoch 32/200\n",
      "1350/1350 [==============================] - 0s 32us/sample - loss: 0.0242 - acc: 0.8719 - val_loss: 0.0378 - val_acc: 0.7800\n",
      "Epoch 33/200\n",
      "1350/1350 [==============================] - 0s 30us/sample - loss: 0.0238 - acc: 0.8711 - val_loss: 0.0373 - val_acc: 0.7800\n",
      "Epoch 34/200\n",
      "1350/1350 [==============================] - 0s 31us/sample - loss: 0.0233 - acc: 0.8770 - val_loss: 0.0368 - val_acc: 0.7800\n",
      "Epoch 35/200\n",
      "1350/1350 [==============================] - 0s 32us/sample - loss: 0.0228 - acc: 0.8807 - val_loss: 0.0364 - val_acc: 0.7867\n",
      "Epoch 36/200\n",
      "1350/1350 [==============================] - 0s 31us/sample - loss: 0.0224 - acc: 0.8800 - val_loss: 0.0359 - val_acc: 0.7867\n",
      "Epoch 37/200\n",
      "1350/1350 [==============================] - 0s 31us/sample - loss: 0.0220 - acc: 0.8800 - val_loss: 0.0356 - val_acc: 0.7867\n",
      "Epoch 38/200\n",
      "1350/1350 [==============================] - 0s 32us/sample - loss: 0.0216 - acc: 0.8867 - val_loss: 0.0352 - val_acc: 0.7867\n",
      "Epoch 39/200\n",
      "1350/1350 [==============================] - 0s 33us/sample - loss: 0.0212 - acc: 0.8844 - val_loss: 0.0349 - val_acc: 0.7867\n",
      "Epoch 40/200\n",
      "1350/1350 [==============================] - 0s 30us/sample - loss: 0.0209 - acc: 0.8889 - val_loss: 0.0346 - val_acc: 0.7867\n",
      "Epoch 41/200\n",
      "1350/1350 [==============================] - 0s 32us/sample - loss: 0.0205 - acc: 0.8889 - val_loss: 0.0343 - val_acc: 0.7867\n",
      "Epoch 42/200\n",
      "1350/1350 [==============================] - 0s 32us/sample - loss: 0.0202 - acc: 0.8889 - val_loss: 0.0340 - val_acc: 0.7867\n",
      "Epoch 43/200\n",
      "1350/1350 [==============================] - 0s 31us/sample - loss: 0.0199 - acc: 0.8933 - val_loss: 0.0336 - val_acc: 0.7933\n",
      "Epoch 44/200\n",
      "1350/1350 [==============================] - 0s 32us/sample - loss: 0.0196 - acc: 0.8956 - val_loss: 0.0334 - val_acc: 0.7933\n",
      "Epoch 45/200\n",
      "1350/1350 [==============================] - 0s 31us/sample - loss: 0.0193 - acc: 0.8956 - val_loss: 0.0331 - val_acc: 0.8000\n",
      "Epoch 46/200\n",
      "1350/1350 [==============================] - 0s 31us/sample - loss: 0.0190 - acc: 0.8978 - val_loss: 0.0330 - val_acc: 0.7867\n",
      "Epoch 47/200\n",
      "1350/1350 [==============================] - 0s 31us/sample - loss: 0.0187 - acc: 0.8963 - val_loss: 0.0328 - val_acc: 0.7933\n",
      "Epoch 48/200\n",
      "1350/1350 [==============================] - 0s 30us/sample - loss: 0.0184 - acc: 0.9022 - val_loss: 0.0325 - val_acc: 0.7933\n",
      "Epoch 49/200\n",
      "1350/1350 [==============================] - 0s 31us/sample - loss: 0.0182 - acc: 0.9037 - val_loss: 0.0324 - val_acc: 0.7933\n",
      "Epoch 50/200\n",
      "1350/1350 [==============================] - 0s 32us/sample - loss: 0.0179 - acc: 0.9052 - val_loss: 0.0321 - val_acc: 0.7933\n",
      "Epoch 51/200\n",
      "1350/1350 [==============================] - 0s 31us/sample - loss: 0.0177 - acc: 0.9030 - val_loss: 0.0320 - val_acc: 0.7933\n",
      "Epoch 52/200\n",
      "1350/1350 [==============================] - 0s 31us/sample - loss: 0.0175 - acc: 0.9104 - val_loss: 0.0316 - val_acc: 0.7933\n",
      "Epoch 53/200\n",
      "1350/1350 [==============================] - 0s 33us/sample - loss: 0.0172 - acc: 0.9074 - val_loss: 0.0317 - val_acc: 0.8000\n",
      "Epoch 54/200\n",
      "1350/1350 [==============================] - 0s 32us/sample - loss: 0.0170 - acc: 0.9119 - val_loss: 0.0314 - val_acc: 0.7933\n",
      "Epoch 55/200\n",
      "1350/1350 [==============================] - 0s 32us/sample - loss: 0.0168 - acc: 0.9096 - val_loss: 0.0313 - val_acc: 0.7933\n",
      "Epoch 56/200\n",
      "1350/1350 [==============================] - 0s 31us/sample - loss: 0.0166 - acc: 0.9119 - val_loss: 0.0311 - val_acc: 0.8000\n",
      "Epoch 57/200\n",
      "1350/1350 [==============================] - 0s 32us/sample - loss: 0.0164 - acc: 0.9133 - val_loss: 0.0310 - val_acc: 0.7933\n",
      "Epoch 58/200\n",
      "1350/1350 [==============================] - 0s 33us/sample - loss: 0.0162 - acc: 0.9141 - val_loss: 0.0307 - val_acc: 0.8000\n",
      "Epoch 59/200\n",
      "1350/1350 [==============================] - 0s 32us/sample - loss: 0.0160 - acc: 0.9163 - val_loss: 0.0308 - val_acc: 0.7933\n",
      "Epoch 60/200\n",
      "1350/1350 [==============================] - 0s 31us/sample - loss: 0.0158 - acc: 0.9156 - val_loss: 0.0307 - val_acc: 0.7933\n",
      "Epoch 61/200\n",
      "1350/1350 [==============================] - 0s 31us/sample - loss: 0.0156 - acc: 0.9170 - val_loss: 0.0305 - val_acc: 0.7933\n",
      "Epoch 62/200\n",
      "1350/1350 [==============================] - 0s 31us/sample - loss: 0.0154 - acc: 0.9207 - val_loss: 0.0302 - val_acc: 0.7933\n",
      "Epoch 63/200\n",
      "1350/1350 [==============================] - 0s 31us/sample - loss: 0.0152 - acc: 0.9200 - val_loss: 0.0302 - val_acc: 0.8000\n",
      "Epoch 64/200\n",
      "1350/1350 [==============================] - 0s 30us/sample - loss: 0.0150 - acc: 0.9207 - val_loss: 0.0300 - val_acc: 0.7933\n",
      "Epoch 65/200\n",
      "1350/1350 [==============================] - 0s 34us/sample - loss: 0.0148 - acc: 0.9230 - val_loss: 0.0300 - val_acc: 0.7867\n",
      "Epoch 66/200\n",
      "1350/1350 [==============================] - 0s 30us/sample - loss: 0.0147 - acc: 0.9207 - val_loss: 0.0297 - val_acc: 0.8000\n",
      "Epoch 67/200\n",
      "1350/1350 [==============================] - 0s 33us/sample - loss: 0.0145 - acc: 0.9215 - val_loss: 0.0297 - val_acc: 0.8000\n",
      "Epoch 68/200\n",
      "1350/1350 [==============================] - 0s 32us/sample - loss: 0.0143 - acc: 0.9230 - val_loss: 0.0296 - val_acc: 0.8000\n",
      "Epoch 69/200\n",
      "1350/1350 [==============================] - 0s 31us/sample - loss: 0.0141 - acc: 0.9259 - val_loss: 0.0295 - val_acc: 0.8067\n",
      "Epoch 70/200\n",
      "1350/1350 [==============================] - 0s 33us/sample - loss: 0.0140 - acc: 0.9259 - val_loss: 0.0292 - val_acc: 0.8067\n",
      "Epoch 71/200\n",
      "1350/1350 [==============================] - 0s 34us/sample - loss: 0.0138 - acc: 0.9267 - val_loss: 0.0294 - val_acc: 0.8000\n",
      "Epoch 72/200\n",
      "1350/1350 [==============================] - 0s 31us/sample - loss: 0.0137 - acc: 0.9281 - val_loss: 0.0294 - val_acc: 0.8000\n",
      "Epoch 73/200\n",
      "1350/1350 [==============================] - 0s 32us/sample - loss: 0.0136 - acc: 0.9296 - val_loss: 0.0293 - val_acc: 0.8000\n",
      "Epoch 74/200\n",
      "1350/1350 [==============================] - 0s 32us/sample - loss: 0.0134 - acc: 0.9289 - val_loss: 0.0291 - val_acc: 0.8000\n",
      "Epoch 75/200\n",
      "1350/1350 [==============================] - 0s 32us/sample - loss: 0.0132 - acc: 0.9296 - val_loss: 0.0290 - val_acc: 0.8067\n",
      "Epoch 76/200\n",
      "1350/1350 [==============================] - 0s 34us/sample - loss: 0.0131 - acc: 0.9311 - val_loss: 0.0290 - val_acc: 0.8000\n",
      "Epoch 77/200\n",
      "1350/1350 [==============================] - 0s 32us/sample - loss: 0.0129 - acc: 0.9326 - val_loss: 0.0290 - val_acc: 0.7933\n",
      "Epoch 78/200\n",
      "1350/1350 [==============================] - 0s 29us/sample - loss: 0.0128 - acc: 0.9348 - val_loss: 0.0289 - val_acc: 0.8000\n",
      "Epoch 79/200\n",
      "1350/1350 [==============================] - 0s 32us/sample - loss: 0.0127 - acc: 0.9326 - val_loss: 0.0287 - val_acc: 0.8000\n",
      "Epoch 80/200\n",
      "1350/1350 [==============================] - 0s 30us/sample - loss: 0.0125 - acc: 0.9348 - val_loss: 0.0287 - val_acc: 0.8000\n",
      "Epoch 81/200\n",
      "1350/1350 [==============================] - 0s 31us/sample - loss: 0.0124 - acc: 0.9363 - val_loss: 0.0287 - val_acc: 0.8000\n",
      "Epoch 82/200\n",
      "1350/1350 [==============================] - 0s 32us/sample - loss: 0.0123 - acc: 0.9356 - val_loss: 0.0285 - val_acc: 0.8000\n",
      "Epoch 83/200\n",
      "1350/1350 [==============================] - 0s 33us/sample - loss: 0.0121 - acc: 0.9370 - val_loss: 0.0285 - val_acc: 0.8000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 84/200\n",
      "1350/1350 [==============================] - 0s 34us/sample - loss: 0.0120 - acc: 0.9393 - val_loss: 0.0283 - val_acc: 0.8000\n",
      "Epoch 85/200\n",
      "1350/1350 [==============================] - 0s 31us/sample - loss: 0.0119 - acc: 0.9385 - val_loss: 0.0283 - val_acc: 0.8000\n",
      "Epoch 86/200\n",
      "1350/1350 [==============================] - 0s 31us/sample - loss: 0.0118 - acc: 0.9393 - val_loss: 0.0282 - val_acc: 0.8000\n",
      "Epoch 87/200\n",
      "1350/1350 [==============================] - 0s 31us/sample - loss: 0.0116 - acc: 0.9422 - val_loss: 0.0285 - val_acc: 0.8000\n",
      "Epoch 88/200\n",
      "1350/1350 [==============================] - 0s 31us/sample - loss: 0.0115 - acc: 0.9393 - val_loss: 0.0281 - val_acc: 0.8000\n",
      "Epoch 89/200\n",
      "1350/1350 [==============================] - 0s 30us/sample - loss: 0.0114 - acc: 0.9422 - val_loss: 0.0283 - val_acc: 0.8000\n",
      "Epoch 90/200\n",
      "1350/1350 [==============================] - 0s 29us/sample - loss: 0.0113 - acc: 0.9430 - val_loss: 0.0280 - val_acc: 0.8000\n",
      "Epoch 91/200\n",
      "1350/1350 [==============================] - 0s 31us/sample - loss: 0.0112 - acc: 0.9430 - val_loss: 0.0280 - val_acc: 0.8000\n",
      "Epoch 92/200\n",
      "1350/1350 [==============================] - 0s 32us/sample - loss: 0.0110 - acc: 0.9467 - val_loss: 0.0281 - val_acc: 0.7933\n",
      "Epoch 93/200\n",
      "1350/1350 [==============================] - 0s 33us/sample - loss: 0.0110 - acc: 0.9467 - val_loss: 0.0280 - val_acc: 0.8000\n",
      "Epoch 94/200\n",
      "1350/1350 [==============================] - 0s 31us/sample - loss: 0.0108 - acc: 0.9467 - val_loss: 0.0278 - val_acc: 0.8000\n",
      "Epoch 95/200\n",
      "1350/1350 [==============================] - 0s 31us/sample - loss: 0.0107 - acc: 0.9474 - val_loss: 0.0281 - val_acc: 0.7933\n",
      "Epoch 96/200\n",
      "1350/1350 [==============================] - 0s 31us/sample - loss: 0.0106 - acc: 0.9504 - val_loss: 0.0277 - val_acc: 0.8000\n",
      "Epoch 97/200\n",
      "1350/1350 [==============================] - 0s 33us/sample - loss: 0.0105 - acc: 0.9489 - val_loss: 0.0278 - val_acc: 0.7933\n",
      "Epoch 98/200\n",
      "1350/1350 [==============================] - 0s 31us/sample - loss: 0.0104 - acc: 0.9519 - val_loss: 0.0276 - val_acc: 0.8000\n",
      "Epoch 99/200\n",
      "1350/1350 [==============================] - 0s 31us/sample - loss: 0.0103 - acc: 0.9519 - val_loss: 0.0278 - val_acc: 0.7933\n",
      "Epoch 100/200\n",
      "1350/1350 [==============================] - 0s 33us/sample - loss: 0.0102 - acc: 0.9533 - val_loss: 0.0277 - val_acc: 0.8067\n",
      "Epoch 101/200\n",
      "1350/1350 [==============================] - 0s 31us/sample - loss: 0.0101 - acc: 0.9519 - val_loss: 0.0275 - val_acc: 0.7933\n",
      "Epoch 102/200\n",
      "1350/1350 [==============================] - 0s 32us/sample - loss: 0.0100 - acc: 0.9548 - val_loss: 0.0276 - val_acc: 0.7933\n",
      "Epoch 103/200\n",
      "1350/1350 [==============================] - 0s 31us/sample - loss: 0.0099 - acc: 0.9533 - val_loss: 0.0274 - val_acc: 0.8000\n",
      "Epoch 104/200\n",
      "1350/1350 [==============================] - 0s 32us/sample - loss: 0.0098 - acc: 0.9548 - val_loss: 0.0276 - val_acc: 0.7933\n",
      "Epoch 105/200\n",
      "1350/1350 [==============================] - 0s 34us/sample - loss: 0.0098 - acc: 0.9548 - val_loss: 0.0274 - val_acc: 0.8067\n",
      "Epoch 106/200\n",
      "1350/1350 [==============================] - 0s 32us/sample - loss: 0.0097 - acc: 0.9548 - val_loss: 0.0274 - val_acc: 0.8000\n",
      "Epoch 107/200\n",
      "1350/1350 [==============================] - 0s 32us/sample - loss: 0.0096 - acc: 0.9548 - val_loss: 0.0276 - val_acc: 0.7933\n",
      "Epoch 108/200\n",
      "1350/1350 [==============================] - 0s 34us/sample - loss: 0.0095 - acc: 0.9548 - val_loss: 0.0274 - val_acc: 0.7933\n",
      "Epoch 109/200\n",
      "1350/1350 [==============================] - 0s 33us/sample - loss: 0.0094 - acc: 0.9556 - val_loss: 0.0274 - val_acc: 0.8000\n",
      "Epoch 110/200\n",
      "1350/1350 [==============================] - 0s 31us/sample - loss: 0.0094 - acc: 0.9563 - val_loss: 0.0272 - val_acc: 0.8000\n",
      "Epoch 111/200\n",
      "1350/1350 [==============================] - 0s 33us/sample - loss: 0.0092 - acc: 0.9570 - val_loss: 0.0271 - val_acc: 0.8000\n",
      "Epoch 112/200\n",
      "1350/1350 [==============================] - 0s 32us/sample - loss: 0.0092 - acc: 0.9570 - val_loss: 0.0271 - val_acc: 0.7933\n",
      "Epoch 113/200\n",
      "1350/1350 [==============================] - 0s 31us/sample - loss: 0.0091 - acc: 0.9578 - val_loss: 0.0274 - val_acc: 0.8000\n",
      "Epoch 114/200\n",
      "1350/1350 [==============================] - 0s 30us/sample - loss: 0.0090 - acc: 0.9570 - val_loss: 0.0273 - val_acc: 0.8000\n",
      "Epoch 115/200\n",
      "1350/1350 [==============================] - 0s 31us/sample - loss: 0.0089 - acc: 0.9585 - val_loss: 0.0271 - val_acc: 0.8067\n",
      "Epoch 116/200\n",
      "1350/1350 [==============================] - 0s 32us/sample - loss: 0.0089 - acc: 0.9593 - val_loss: 0.0272 - val_acc: 0.8000\n",
      "Epoch 117/200\n",
      "1350/1350 [==============================] - 0s 30us/sample - loss: 0.0088 - acc: 0.9600 - val_loss: 0.0270 - val_acc: 0.8067\n",
      "Epoch 118/200\n",
      "1350/1350 [==============================] - 0s 31us/sample - loss: 0.0087 - acc: 0.9600 - val_loss: 0.0271 - val_acc: 0.8000\n",
      "Epoch 119/200\n",
      "1350/1350 [==============================] - 0s 30us/sample - loss: 0.0087 - acc: 0.9607 - val_loss: 0.0271 - val_acc: 0.8000\n",
      "Epoch 120/200\n",
      "1350/1350 [==============================] - 0s 31us/sample - loss: 0.0086 - acc: 0.9630 - val_loss: 0.0270 - val_acc: 0.8000\n",
      "Epoch 121/200\n",
      "1350/1350 [==============================] - 0s 30us/sample - loss: 0.0085 - acc: 0.9615 - val_loss: 0.0271 - val_acc: 0.8133\n",
      "Epoch 122/200\n",
      "1350/1350 [==============================] - 0s 32us/sample - loss: 0.0084 - acc: 0.9607 - val_loss: 0.0273 - val_acc: 0.8000\n",
      "Epoch 123/200\n",
      "1350/1350 [==============================] - 0s 30us/sample - loss: 0.0084 - acc: 0.9615 - val_loss: 0.0269 - val_acc: 0.8067\n",
      "Epoch 124/200\n",
      "1350/1350 [==============================] - 0s 33us/sample - loss: 0.0083 - acc: 0.9622 - val_loss: 0.0270 - val_acc: 0.8000\n",
      "Epoch 125/200\n",
      "1350/1350 [==============================] - 0s 32us/sample - loss: 0.0082 - acc: 0.9615 - val_loss: 0.0269 - val_acc: 0.8067\n",
      "Epoch 126/200\n",
      "1350/1350 [==============================] - 0s 31us/sample - loss: 0.0082 - acc: 0.9622 - val_loss: 0.0269 - val_acc: 0.8067\n",
      "Epoch 127/200\n",
      "1350/1350 [==============================] - 0s 30us/sample - loss: 0.0081 - acc: 0.9622 - val_loss: 0.0270 - val_acc: 0.8000\n",
      "Epoch 128/200\n",
      "1350/1350 [==============================] - 0s 31us/sample - loss: 0.0081 - acc: 0.9630 - val_loss: 0.0269 - val_acc: 0.8067\n",
      "Epoch 129/200\n",
      "1350/1350 [==============================] - 0s 31us/sample - loss: 0.0080 - acc: 0.9622 - val_loss: 0.0269 - val_acc: 0.8067\n",
      "Epoch 130/200\n",
      "1350/1350 [==============================] - 0s 31us/sample - loss: 0.0079 - acc: 0.9630 - val_loss: 0.0269 - val_acc: 0.8067\n",
      "Epoch 131/200\n",
      "1350/1350 [==============================] - 0s 30us/sample - loss: 0.0079 - acc: 0.9637 - val_loss: 0.0271 - val_acc: 0.8067\n",
      "Epoch 132/200\n",
      "1350/1350 [==============================] - 0s 31us/sample - loss: 0.0078 - acc: 0.9637 - val_loss: 0.0270 - val_acc: 0.8067\n",
      "Epoch 133/200\n",
      "1350/1350 [==============================] - 0s 33us/sample - loss: 0.0078 - acc: 0.9630 - val_loss: 0.0268 - val_acc: 0.8067\n",
      "Epoch 134/200\n",
      "1350/1350 [==============================] - 0s 29us/sample - loss: 0.0077 - acc: 0.9637 - val_loss: 0.0268 - val_acc: 0.8067\n",
      "Epoch 135/200\n",
      "1350/1350 [==============================] - 0s 33us/sample - loss: 0.0076 - acc: 0.9652 - val_loss: 0.0266 - val_acc: 0.8133\n",
      "Epoch 136/200\n",
      "1350/1350 [==============================] - 0s 31us/sample - loss: 0.0076 - acc: 0.9644 - val_loss: 0.0267 - val_acc: 0.8133\n",
      "Epoch 137/200\n",
      "1350/1350 [==============================] - 0s 33us/sample - loss: 0.0075 - acc: 0.9637 - val_loss: 0.0269 - val_acc: 0.8000\n",
      "Epoch 138/200\n",
      "1350/1350 [==============================] - 0s 32us/sample - loss: 0.0075 - acc: 0.9652 - val_loss: 0.0266 - val_acc: 0.8133\n",
      "Epoch 139/200\n",
      "1350/1350 [==============================] - 0s 32us/sample - loss: 0.0074 - acc: 0.9644 - val_loss: 0.0268 - val_acc: 0.8067\n",
      "Epoch 140/200\n",
      "1350/1350 [==============================] - 0s 32us/sample - loss: 0.0074 - acc: 0.9637 - val_loss: 0.0269 - val_acc: 0.8067\n",
      "Epoch 141/200\n",
      "1350/1350 [==============================] - 0s 33us/sample - loss: 0.0073 - acc: 0.9644 - val_loss: 0.0268 - val_acc: 0.8000\n",
      "Epoch 142/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1350/1350 [==============================] - 0s 30us/sample - loss: 0.0072 - acc: 0.9659 - val_loss: 0.0266 - val_acc: 0.8067\n",
      "Epoch 143/200\n",
      "1350/1350 [==============================] - 0s 32us/sample - loss: 0.0072 - acc: 0.9652 - val_loss: 0.0267 - val_acc: 0.8133\n",
      "Epoch 144/200\n",
      "1350/1350 [==============================] - 0s 31us/sample - loss: 0.0072 - acc: 0.9652 - val_loss: 0.0268 - val_acc: 0.8067\n",
      "Epoch 145/200\n",
      "1350/1350 [==============================] - 0s 31us/sample - loss: 0.0071 - acc: 0.9667 - val_loss: 0.0269 - val_acc: 0.8067\n",
      "Epoch 146/200\n",
      "1350/1350 [==============================] - 0s 32us/sample - loss: 0.0071 - acc: 0.9667 - val_loss: 0.0266 - val_acc: 0.8067\n",
      "Epoch 147/200\n",
      "1350/1350 [==============================] - 0s 32us/sample - loss: 0.0070 - acc: 0.9667 - val_loss: 0.0267 - val_acc: 0.8067\n",
      "Epoch 148/200\n",
      "1350/1350 [==============================] - 0s 33us/sample - loss: 0.0070 - acc: 0.9667 - val_loss: 0.0266 - val_acc: 0.8067\n",
      "Epoch 149/200\n",
      "1350/1350 [==============================] - 0s 30us/sample - loss: 0.0069 - acc: 0.9689 - val_loss: 0.0267 - val_acc: 0.8067\n",
      "Epoch 150/200\n",
      "1350/1350 [==============================] - 0s 31us/sample - loss: 0.0069 - acc: 0.9704 - val_loss: 0.0267 - val_acc: 0.8133\n",
      "Epoch 151/200\n",
      "1350/1350 [==============================] - 0s 31us/sample - loss: 0.0068 - acc: 0.9696 - val_loss: 0.0265 - val_acc: 0.8133\n",
      "Epoch 152/200\n",
      "1350/1350 [==============================] - 0s 31us/sample - loss: 0.0068 - acc: 0.9689 - val_loss: 0.0266 - val_acc: 0.8067\n",
      "Epoch 153/200\n",
      "1350/1350 [==============================] - 0s 31us/sample - loss: 0.0067 - acc: 0.9704 - val_loss: 0.0264 - val_acc: 0.8133\n",
      "Epoch 154/200\n",
      "1350/1350 [==============================] - 0s 31us/sample - loss: 0.0067 - acc: 0.9696 - val_loss: 0.0266 - val_acc: 0.8067\n",
      "Epoch 155/200\n",
      "1350/1350 [==============================] - 0s 32us/sample - loss: 0.0066 - acc: 0.9711 - val_loss: 0.0265 - val_acc: 0.8000\n",
      "Epoch 156/200\n",
      "1350/1350 [==============================] - 0s 30us/sample - loss: 0.0066 - acc: 0.9711 - val_loss: 0.0264 - val_acc: 0.8067\n",
      "Epoch 157/200\n",
      "1350/1350 [==============================] - 0s 30us/sample - loss: 0.0066 - acc: 0.9711 - val_loss: 0.0264 - val_acc: 0.8000\n",
      "Epoch 158/200\n",
      "1350/1350 [==============================] - 0s 34us/sample - loss: 0.0065 - acc: 0.9719 - val_loss: 0.0264 - val_acc: 0.8067\n",
      "Epoch 159/200\n",
      "1350/1350 [==============================] - 0s 33us/sample - loss: 0.0065 - acc: 0.9719 - val_loss: 0.0264 - val_acc: 0.8067\n",
      "Epoch 160/200\n",
      "1350/1350 [==============================] - 0s 32us/sample - loss: 0.0064 - acc: 0.9719 - val_loss: 0.0263 - val_acc: 0.8067\n",
      "Epoch 161/200\n",
      "1350/1350 [==============================] - 0s 32us/sample - loss: 0.0064 - acc: 0.9726 - val_loss: 0.0265 - val_acc: 0.8000\n",
      "Epoch 162/200\n",
      "1350/1350 [==============================] - 0s 31us/sample - loss: 0.0064 - acc: 0.9726 - val_loss: 0.0265 - val_acc: 0.8133\n",
      "Epoch 163/200\n",
      "1350/1350 [==============================] - 0s 34us/sample - loss: 0.0063 - acc: 0.9726 - val_loss: 0.0265 - val_acc: 0.8067\n",
      "Epoch 164/200\n",
      "1350/1350 [==============================] - 0s 32us/sample - loss: 0.0063 - acc: 0.9726 - val_loss: 0.0265 - val_acc: 0.8067\n",
      "Epoch 165/200\n",
      "1350/1350 [==============================] - 0s 34us/sample - loss: 0.0062 - acc: 0.9726 - val_loss: 0.0263 - val_acc: 0.8067\n",
      "Epoch 166/200\n",
      "1350/1350 [==============================] - 0s 31us/sample - loss: 0.0062 - acc: 0.9726 - val_loss: 0.0263 - val_acc: 0.8067\n",
      "Epoch 167/200\n",
      "1350/1350 [==============================] - 0s 29us/sample - loss: 0.0062 - acc: 0.9726 - val_loss: 0.0264 - val_acc: 0.8067\n",
      "Epoch 168/200\n",
      "1350/1350 [==============================] - 0s 32us/sample - loss: 0.0061 - acc: 0.9733 - val_loss: 0.0264 - val_acc: 0.8067\n",
      "Epoch 169/200\n",
      "1350/1350 [==============================] - 0s 31us/sample - loss: 0.0061 - acc: 0.9733 - val_loss: 0.0263 - val_acc: 0.8133\n",
      "Epoch 170/200\n",
      "1350/1350 [==============================] - 0s 31us/sample - loss: 0.0060 - acc: 0.9733 - val_loss: 0.0264 - val_acc: 0.8067\n",
      "Epoch 171/200\n",
      "1350/1350 [==============================] - 0s 31us/sample - loss: 0.0060 - acc: 0.9733 - val_loss: 0.0263 - val_acc: 0.8133\n",
      "Epoch 172/200\n",
      "1350/1350 [==============================] - 0s 31us/sample - loss: 0.0060 - acc: 0.9733 - val_loss: 0.0264 - val_acc: 0.8067\n",
      "Epoch 173/200\n",
      "1350/1350 [==============================] - 0s 32us/sample - loss: 0.0059 - acc: 0.9733 - val_loss: 0.0262 - val_acc: 0.8067\n",
      "Epoch 174/200\n",
      "1350/1350 [==============================] - 0s 30us/sample - loss: 0.0059 - acc: 0.9733 - val_loss: 0.0262 - val_acc: 0.8067\n",
      "Epoch 175/200\n",
      "1350/1350 [==============================] - 0s 32us/sample - loss: 0.0059 - acc: 0.9733 - val_loss: 0.0264 - val_acc: 0.8133\n",
      "Epoch 176/200\n",
      "1350/1350 [==============================] - 0s 31us/sample - loss: 0.0058 - acc: 0.9733 - val_loss: 0.0264 - val_acc: 0.8200\n",
      "Epoch 177/200\n",
      "1350/1350 [==============================] - 0s 32us/sample - loss: 0.0058 - acc: 0.9733 - val_loss: 0.0263 - val_acc: 0.8133\n",
      "Epoch 178/200\n",
      "1350/1350 [==============================] - 0s 30us/sample - loss: 0.0058 - acc: 0.9741 - val_loss: 0.0262 - val_acc: 0.8133\n",
      "Epoch 179/200\n",
      "1350/1350 [==============================] - 0s 31us/sample - loss: 0.0057 - acc: 0.9733 - val_loss: 0.0263 - val_acc: 0.8133\n",
      "Epoch 180/200\n",
      "1350/1350 [==============================] - 0s 31us/sample - loss: 0.0057 - acc: 0.9733 - val_loss: 0.0263 - val_acc: 0.8133\n",
      "Epoch 181/200\n",
      "1350/1350 [==============================] - 0s 30us/sample - loss: 0.0057 - acc: 0.9741 - val_loss: 0.0263 - val_acc: 0.8200\n",
      "Epoch 182/200\n",
      "1350/1350 [==============================] - 0s 31us/sample - loss: 0.0056 - acc: 0.9741 - val_loss: 0.0263 - val_acc: 0.8200\n",
      "Epoch 183/200\n",
      "1350/1350 [==============================] - 0s 32us/sample - loss: 0.0056 - acc: 0.9741 - val_loss: 0.0262 - val_acc: 0.8200\n",
      "Epoch 184/200\n",
      "1350/1350 [==============================] - 0s 31us/sample - loss: 0.0056 - acc: 0.9741 - val_loss: 0.0262 - val_acc: 0.8200\n",
      "Epoch 185/200\n",
      "1350/1350 [==============================] - 0s 30us/sample - loss: 0.0055 - acc: 0.9741 - val_loss: 0.0262 - val_acc: 0.8200\n",
      "Epoch 186/200\n",
      "1350/1350 [==============================] - 0s 31us/sample - loss: 0.0055 - acc: 0.9741 - val_loss: 0.0263 - val_acc: 0.8267\n",
      "Epoch 187/200\n",
      "1350/1350 [==============================] - 0s 30us/sample - loss: 0.0055 - acc: 0.9741 - val_loss: 0.0262 - val_acc: 0.8133\n",
      "Epoch 188/200\n",
      "1350/1350 [==============================] - 0s 30us/sample - loss: 0.0054 - acc: 0.9741 - val_loss: 0.0261 - val_acc: 0.8267\n",
      "Epoch 189/200\n",
      "1350/1350 [==============================] - 0s 30us/sample - loss: 0.0054 - acc: 0.9741 - val_loss: 0.0262 - val_acc: 0.8200\n",
      "Epoch 190/200\n",
      "1350/1350 [==============================] - 0s 33us/sample - loss: 0.0054 - acc: 0.9741 - val_loss: 0.0262 - val_acc: 0.8200\n",
      "Epoch 191/200\n",
      "1350/1350 [==============================] - 0s 31us/sample - loss: 0.0054 - acc: 0.9741 - val_loss: 0.0262 - val_acc: 0.8267\n",
      "Epoch 192/200\n",
      "1350/1350 [==============================] - 0s 32us/sample - loss: 0.0053 - acc: 0.9741 - val_loss: 0.0261 - val_acc: 0.8200\n",
      "Epoch 193/200\n",
      "1350/1350 [==============================] - 0s 32us/sample - loss: 0.0053 - acc: 0.9741 - val_loss: 0.0261 - val_acc: 0.8267\n",
      "Epoch 194/200\n",
      "1350/1350 [==============================] - 0s 32us/sample - loss: 0.0053 - acc: 0.9741 - val_loss: 0.0261 - val_acc: 0.8267\n",
      "Epoch 195/200\n",
      "1350/1350 [==============================] - 0s 31us/sample - loss: 0.0052 - acc: 0.9741 - val_loss: 0.0260 - val_acc: 0.8333\n",
      "Epoch 196/200\n",
      "1350/1350 [==============================] - 0s 31us/sample - loss: 0.0052 - acc: 0.9741 - val_loss: 0.0260 - val_acc: 0.8267\n",
      "Epoch 197/200\n",
      "1350/1350 [==============================] - 0s 31us/sample - loss: 0.0052 - acc: 0.9741 - val_loss: 0.0261 - val_acc: 0.8267\n",
      "Epoch 198/200\n",
      "1350/1350 [==============================] - 0s 29us/sample - loss: 0.0052 - acc: 0.9741 - val_loss: 0.0261 - val_acc: 0.8200\n",
      "Epoch 199/200\n",
      "1350/1350 [==============================] - 0s 31us/sample - loss: 0.0051 - acc: 0.9741 - val_loss: 0.0262 - val_acc: 0.8133\n",
      "Epoch 200/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1350/1350 [==============================] - 0s 31us/sample - loss: 0.0051 - acc: 0.9748 - val_loss: 0.0261 - val_acc: 0.8200\n",
      "Elasped Time: 0:00:09.258416\n"
     ]
    }
   ],
   "source": [
    "for i in range(len(model_list)):\n",
    "    compile_model(model_list[i])\n",
    "    fit_model_with_datasets(model_list[i], (i+1)*10, X_local_list[i], Y_local_list[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's see how these models are different from each other, compared to the base model(before training)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_list[0].get"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "l = [np.array(m.get_weights()) for m in model_list]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "l = np.array(l)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "av = np.average(l, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(200,)"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "av[1].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(200, 200)"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "l[1][2].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(784, 200)"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "l[2][0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "avg_weight = np.average(np.array([np.array(m.get_weights()) for m in model_list]), axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "model1.set_weights(avg_weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.72831976, 1.2300797, 1.6375992, 1.8042661, 2.101626, 2.2287998, 2.3368342, 2.5651116, 2.6788723, 2.932489, 2.9436498, 3.1036754, 3.1189368, 3.1189752, 3.233824, 3.4184306, 3.505584, 3.4839816, 3.4630003, 3.7790039]\n"
     ]
    }
   ],
   "source": [
    "dists = [semantic_drift.l2_distance(standard_model, m) for m in model_list]\n",
    "print(dists)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_list.sort(key=lambda m : semantic_drift.l2_distance(standard_model, m))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.72831976, 1.2300797, 1.6375992, 1.8042661, 2.101626, 2.2287998, 2.3368342, 2.5651116, 2.6788723, 2.932489, 2.9436498, 3.1036754, 3.1189368, 3.1189752, 3.233824, 3.4184306, 3.4630003, 3.4839816, 3.505584, 3.7790039]\n"
     ]
    }
   ],
   "source": [
    "dists = [semantic_drift.l2_distance(standard_model, m) for m in model_list]\n",
    "print(dists)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "theta_list = list(np.arange(0, 1.05, 0.05))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_combs(model_list):\n",
    "    combs = list()\n",
    "    l = len(model_list)\n",
    "    for i in range(l):\n",
    "        for j in range(l):\n",
    "            if i > j:\n",
    "                combs.append([model_list[i], model_list[j]])\n",
    "    return combs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "agg_weights_list_per_pi = list()\n",
    "dist_list = list()\n",
    "for model_comp in model_combs(model_list):\n",
    "    if model_comp[0] is model_comp[1]:    #disregard same models\n",
    "        continue\n",
    "    weights = [model_comp[0].get_weights(), model_comp[1].get_weights()]\n",
    "    agg_weights_list = list()\n",
    "    for theta in theta_list:\n",
    "        agg_weights = list()\n",
    "        for weights_list_tuple in zip(*weights):\n",
    "            agg_weights.append(np.array([np.average(np.array(w), axis=0, weights=[1. - theta, theta]) for w in zip(*weights_list_tuple)]))\n",
    "        agg_weights_list.append(agg_weights)\n",
    "    dist_list.append(semantic_drift.l2_distance(model_comp[0], model_comp[1]))\n",
    "    agg_weights_list_per_pi.append(agg_weights_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "190"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# this has to be nC_2\n",
    "len(dist_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.6790231,\n",
       " 1.0674238,\n",
       " 0.76220334,\n",
       " 1.2572709,\n",
       " 0.9291775,\n",
       " 0.87450594,\n",
       " 1.5597007,\n",
       " 1.2231365,\n",
       " 1.0425137,\n",
       " 1.0706325,\n",
       " 1.7077744,\n",
       " 1.3930168,\n",
       " 1.2464184,\n",
       " 1.2036492,\n",
       " 1.3275005,\n",
       " 1.8415955,\n",
       " 1.4904561,\n",
       " 1.3264697,\n",
       " 1.334568,\n",
       " 1.2624314,\n",
       " 1.4890555,\n",
       " 2.0484147,\n",
       " 1.6922038,\n",
       " 1.4902464,\n",
       " 1.448433,\n",
       " 1.429275,\n",
       " 1.5020276,\n",
       " 1.5401735,\n",
       " 2.1922908,\n",
       " 1.8737718,\n",
       " 1.6852325,\n",
       " 1.6184423,\n",
       " 1.668025,\n",
       " 1.6669601,\n",
       " 1.723613,\n",
       " 1.7175785,\n",
       " 2.4662292,\n",
       " 2.1276922,\n",
       " 1.9431642,\n",
       " 1.8858895,\n",
       " 1.8604085,\n",
       " 1.9284871,\n",
       " 1.8568151,\n",
       " 1.9258292,\n",
       " 2.0116045,\n",
       " 2.44775,\n",
       " 2.1112788,\n",
       " 1.9144111,\n",
       " 1.8178817,\n",
       " 1.7878528,\n",
       " 1.8373672,\n",
       " 1.7802958,\n",
       " 1.8183829,\n",
       " 1.8848307,\n",
       " 2.04441,\n",
       " 2.630128,\n",
       " 2.312654,\n",
       " 2.0886657,\n",
       " 2.0123613,\n",
       " 1.9700245,\n",
       " 1.918427,\n",
       " 1.9843751,\n",
       " 1.9962378,\n",
       " 2.015119,\n",
       " 2.115679,\n",
       " 2.0416331,\n",
       " 2.6332684,\n",
       " 2.274623,\n",
       " 2.0428298,\n",
       " 1.9744081,\n",
       " 1.8964151,\n",
       " 1.915635,\n",
       " 1.9040977,\n",
       " 1.8982803,\n",
       " 1.9788328,\n",
       " 2.0905643,\n",
       " 1.9930409,\n",
       " 2.0892036,\n",
       " 2.651401,\n",
       " 2.3060958,\n",
       " 2.0930645,\n",
       " 2.0427055,\n",
       " 2.0135384,\n",
       " 2.0219011,\n",
       " 2.0162084,\n",
       " 1.9988661,\n",
       " 2.0714881,\n",
       " 2.0743117,\n",
       " 2.1304088,\n",
       " 2.1402361,\n",
       " 2.1119604,\n",
       " 2.7551765,\n",
       " 2.411163,\n",
       " 2.2117047,\n",
       " 2.153813,\n",
       " 2.052897,\n",
       " 2.0917046,\n",
       " 2.0433514,\n",
       " 2.0471692,\n",
       " 2.1957836,\n",
       " 2.1890812,\n",
       " 2.131615,\n",
       " 2.2577913,\n",
       " 2.1941364,\n",
       " 2.3079967,\n",
       " 2.9757798,\n",
       " 2.6579528,\n",
       " 2.4409258,\n",
       " 2.3998845,\n",
       " 2.347141,\n",
       " 2.282969,\n",
       " 2.3310738,\n",
       " 2.2636905,\n",
       " 2.2920465,\n",
       " 2.3646827,\n",
       " 2.3653991,\n",
       " 2.3843303,\n",
       " 2.3249996,\n",
       " 2.4406223,\n",
       " 2.440993,\n",
       " 3.0214832,\n",
       " 2.6906247,\n",
       " 2.5218334,\n",
       " 2.4291334,\n",
       " 2.3701787,\n",
       " 2.4233599,\n",
       " 2.3585534,\n",
       " 2.3452675,\n",
       " 2.434575,\n",
       " 2.4163535,\n",
       " 2.4148562,\n",
       " 2.4564784,\n",
       " 2.4438345,\n",
       " 2.5573273,\n",
       " 2.4239593,\n",
       " 2.6789312,\n",
       " 3.0471635,\n",
       " 2.6964815,\n",
       " 2.4913237,\n",
       " 2.4594915,\n",
       " 2.3350291,\n",
       " 2.4451857,\n",
       " 2.2913795,\n",
       " 2.3470352,\n",
       " 2.469785,\n",
       " 2.4570205,\n",
       " 2.402244,\n",
       " 2.512681,\n",
       " 2.3584669,\n",
       " 2.5226,\n",
       " 2.4618943,\n",
       " 2.6599212,\n",
       " 2.5421505,\n",
       " 3.0627875,\n",
       " 2.7386737,\n",
       " 2.514449,\n",
       " 2.4424772,\n",
       " 2.3449116,\n",
       " 2.356043,\n",
       " 2.3136525,\n",
       " 2.2826586,\n",
       " 2.4113677,\n",
       " 2.4529827,\n",
       " 2.3101406,\n",
       " 2.4481273,\n",
       " 2.346089,\n",
       " 2.4815276,\n",
       " 2.50925,\n",
       " 2.574938,\n",
       " 2.667576,\n",
       " 2.6907701,\n",
       " 3.3339112,\n",
       " 3.0275912,\n",
       " 2.8076208,\n",
       " 2.7183104,\n",
       " 2.6590898,\n",
       " 2.6176636,\n",
       " 2.5801287,\n",
       " 2.5432317,\n",
       " 2.59878,\n",
       " 2.646041,\n",
       " 2.5301504,\n",
       " 2.5752583,\n",
       " 2.5473282,\n",
       " 2.6513882,\n",
       " 2.6742516,\n",
       " 2.761921,\n",
       " 2.866682,\n",
       " 2.783162,\n",
       " 2.7635677]"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dist_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "agg_weights_list_per_pi_sorted = [x for _,x in sorted(zip(dist_list,agg_weights_list_per_pi))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, Y = np.meshgrid(np.array(theta_list), np.array(sorted(dist_list)))\n",
    "Z = np.zeros(X.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000/10000 [==============================] - 0s 38us/sample - loss: 0.0263 - accuracy: 0.8606\n",
      "10000/10000 [==============================] - 0s 38us/sample - loss: 0.0266 - accuracy: 0.8598\n",
      "10000/10000 [==============================] - 0s 37us/sample - loss: 0.0270 - accuracy: 0.8599\n",
      "10000/10000 [==============================] - 0s 38us/sample - loss: 0.0274 - accuracy: 0.8590\n",
      "10000/10000 [==============================] - 0s 37us/sample - loss: 0.0277 - accuracy: 0.8564\n",
      "10000/10000 [==============================] - 1s 52us/sample - loss: 0.0281 - accuracy: 0.8547\n",
      "10000/10000 [==============================] - 0s 37us/sample - loss: 0.0286 - accuracy: 0.8528\n",
      "10000/10000 [==============================] - 0s 38us/sample - loss: 0.0290 - accuracy: 0.8521\n",
      "10000/10000 [==============================] - 0s 35us/sample - loss: 0.0294 - accuracy: 0.8498\n",
      "10000/10000 [==============================] - 0s 37us/sample - loss: 0.0299 - accuracy: 0.8474\n",
      "10000/10000 [==============================] - 0s 36us/sample - loss: 0.0303 - accuracy: 0.8438\n",
      "10000/10000 [==============================] - 0s 37us/sample - loss: 0.0308 - accuracy: 0.8396\n",
      "10000/10000 [==============================] - 0s 35us/sample - loss: 0.0313 - accuracy: 0.8365\n",
      "10000/10000 [==============================] - 0s 37us/sample - loss: 0.0318 - accuracy: 0.8333\n",
      "10000/10000 [==============================] - 0s 38us/sample - loss: 0.0323 - accuracy: 0.8297\n",
      "10000/10000 [==============================] - 0s 37us/sample - loss: 0.0328 - accuracy: 0.8263\n",
      "10000/10000 [==============================] - 0s 35us/sample - loss: 0.0333 - accuracy: 0.8208\n",
      "10000/10000 [==============================] - 0s 37us/sample - loss: 0.0339 - accuracy: 0.8168\n",
      "10000/10000 [==============================] - 0s 39us/sample - loss: 0.0344 - accuracy: 0.8121\n",
      "10000/10000 [==============================] - 0s 36us/sample - loss: 0.0350 - accuracy: 0.8064\n",
      "10000/10000 [==============================] - 0s 38us/sample - loss: 0.0355 - accuracy: 0.8016\n",
      "1th iteration\n",
      "10000/10000 [==============================] - 0s 38us/sample - loss: 0.0224 - accuracy: 0.8711\n",
      "10000/10000 [==============================] - 0s 35us/sample - loss: 0.0225 - accuracy: 0.8718\n",
      "10000/10000 [==============================] - 0s 37us/sample - loss: 0.0226 - accuracy: 0.8723\n",
      "10000/10000 [==============================] - 0s 36us/sample - loss: 0.0227 - accuracy: 0.8727\n",
      "10000/10000 [==============================] - 0s 37us/sample - loss: 0.0229 - accuracy: 0.8733\n",
      "10000/10000 [==============================] - 0s 37us/sample - loss: 0.0230 - accuracy: 0.8737\n",
      "10000/10000 [==============================] - 0s 36us/sample - loss: 0.0231 - accuracy: 0.8731\n",
      "10000/10000 [==============================] - 0s 36us/sample - loss: 0.0233 - accuracy: 0.8732\n",
      "10000/10000 [==============================] - 0s 36us/sample - loss: 0.0235 - accuracy: 0.8736\n",
      "10000/10000 [==============================] - 0s 37us/sample - loss: 0.0236 - accuracy: 0.8732\n",
      "10000/10000 [==============================] - 0s 37us/sample - loss: 0.0238 - accuracy: 0.8726\n",
      "10000/10000 [==============================] - 0s 37us/sample - loss: 0.0240 - accuracy: 0.8715\n",
      "10000/10000 [==============================] - 0s 36us/sample - loss: 0.0242 - accuracy: 0.8709\n",
      "10000/10000 [==============================] - 0s 37us/sample - loss: 0.0245 - accuracy: 0.8706\n",
      "10000/10000 [==============================] - 0s 36us/sample - loss: 0.0247 - accuracy: 0.8691\n",
      "10000/10000 [==============================] - 0s 36us/sample - loss: 0.0249 - accuracy: 0.8679\n",
      "10000/10000 [==============================] - 0s 38us/sample - loss: 0.0252 - accuracy: 0.8668\n",
      "10000/10000 [==============================] - 0s 37us/sample - loss: 0.0255 - accuracy: 0.8654\n",
      "10000/10000 [==============================] - 0s 36us/sample - loss: 0.0257 - accuracy: 0.8643\n",
      "10000/10000 [==============================] - 0s 38us/sample - loss: 0.0260 - accuracy: 0.8626\n",
      "10000/10000 [==============================] - 0s 38us/sample - loss: 0.0263 - accuracy: 0.8606\n",
      "2th iteration\n",
      "10000/10000 [==============================] - 0s 35us/sample - loss: 0.0206 - accuracy: 0.8813\n",
      "10000/10000 [==============================] - 0s 38us/sample - loss: 0.0206 - accuracy: 0.8822\n",
      "10000/10000 [==============================] - 0s 35us/sample - loss: 0.0206 - accuracy: 0.8812\n",
      "10000/10000 [==============================] - 0s 37us/sample - loss: 0.0207 - accuracy: 0.8811\n",
      "10000/10000 [==============================] - 0s 37us/sample - loss: 0.0207 - accuracy: 0.8812\n",
      "10000/10000 [==============================] - 0s 37us/sample - loss: 0.0207 - accuracy: 0.8816\n",
      "10000/10000 [==============================] - 0s 36us/sample - loss: 0.0208 - accuracy: 0.8815\n",
      "10000/10000 [==============================] - 0s 36us/sample - loss: 0.0209 - accuracy: 0.8824\n",
      "10000/10000 [==============================] - 0s 37us/sample - loss: 0.0209 - accuracy: 0.8821\n",
      "10000/10000 [==============================] - 0s 37us/sample - loss: 0.0210 - accuracy: 0.8817\n",
      "10000/10000 [==============================] - 0s 36us/sample - loss: 0.0211 - accuracy: 0.8824\n",
      "10000/10000 [==============================] - 0s 36us/sample - loss: 0.0212 - accuracy: 0.8825\n",
      "10000/10000 [==============================] - 0s 36us/sample - loss: 0.0213 - accuracy: 0.8817\n",
      "10000/10000 [==============================] - 0s 36us/sample - loss: 0.0214 - accuracy: 0.8807\n",
      "10000/10000 [==============================] - 0s 36us/sample - loss: 0.0215 - accuracy: 0.8799\n",
      "10000/10000 [==============================] - 0s 36us/sample - loss: 0.0217 - accuracy: 0.8786\n",
      "10000/10000 [==============================] - 0s 36us/sample - loss: 0.0218 - accuracy: 0.8772\n",
      "10000/10000 [==============================] - 0s 38us/sample - loss: 0.0219 - accuracy: 0.8755\n",
      "10000/10000 [==============================] - 0s 37us/sample - loss: 0.0221 - accuracy: 0.8740\n",
      "10000/10000 [==============================] - 0s 38us/sample - loss: 0.0222 - accuracy: 0.8730\n",
      "10000/10000 [==============================] - 0s 38us/sample - loss: 0.0224 - accuracy: 0.8711\n",
      "3th iteration\n",
      "10000/10000 [==============================] - 0s 38us/sample - loss: 0.0206 - accuracy: 0.8813\n",
      "10000/10000 [==============================] - 0s 38us/sample - loss: 0.0207 - accuracy: 0.8811\n",
      "10000/10000 [==============================] - 0s 35us/sample - loss: 0.0209 - accuracy: 0.8806\n",
      "10000/10000 [==============================] - 0s 37us/sample - loss: 0.0211 - accuracy: 0.8799\n",
      "10000/10000 [==============================] - 0s 38us/sample - loss: 0.0213 - accuracy: 0.8786\n",
      "10000/10000 [==============================] - 0s 37us/sample - loss: 0.0215 - accuracy: 0.8783\n",
      "10000/10000 [==============================] - 0s 38us/sample - loss: 0.0217 - accuracy: 0.8774\n",
      "10000/10000 [==============================] - 0s 36us/sample - loss: 0.0220 - accuracy: 0.8771\n",
      "10000/10000 [==============================] - 0s 37us/sample - loss: 0.0222 - accuracy: 0.8762\n",
      "10000/10000 [==============================] - 0s 38us/sample - loss: 0.0225 - accuracy: 0.8753\n",
      "10000/10000 [==============================] - 0s 36us/sample - loss: 0.0228 - accuracy: 0.8747\n",
      "10000/10000 [==============================] - 0s 35us/sample - loss: 0.0231 - accuracy: 0.8739\n",
      "10000/10000 [==============================] - 0s 37us/sample - loss: 0.0234 - accuracy: 0.8729\n",
      "10000/10000 [==============================] - 0s 37us/sample - loss: 0.0237 - accuracy: 0.8722\n",
      "10000/10000 [==============================] - 0s 36us/sample - loss: 0.0240 - accuracy: 0.8723\n",
      "10000/10000 [==============================] - 0s 36us/sample - loss: 0.0244 - accuracy: 0.8712\n",
      "10000/10000 [==============================] - 0s 37us/sample - loss: 0.0247 - accuracy: 0.8686\n",
      "10000/10000 [==============================] - 0s 37us/sample - loss: 0.0251 - accuracy: 0.8664\n",
      "10000/10000 [==============================] - 1s 59us/sample - loss: 0.0255 - accuracy: 0.8646\n",
      "10000/10000 [==============================] - 0s 37us/sample - loss: 0.0259 - accuracy: 0.8632\n",
      "10000/10000 [==============================] - 0s 37us/sample - loss: 0.0263 - accuracy: 0.8606\n",
      "4th iteration\n",
      "10000/10000 [==============================] - 0s 36us/sample - loss: 0.0190 - accuracy: 0.8848\n",
      "10000/10000 [==============================] - 0s 38us/sample - loss: 0.0190 - accuracy: 0.8854\n",
      "10000/10000 [==============================] - 0s 35us/sample - loss: 0.0191 - accuracy: 0.8864\n",
      "10000/10000 [==============================] - 0s 37us/sample - loss: 0.0191 - accuracy: 0.8864\n",
      "10000/10000 [==============================] - 0s 38us/sample - loss: 0.0192 - accuracy: 0.8865\n",
      "10000/10000 [==============================] - 0s 36us/sample - loss: 0.0193 - accuracy: 0.8855\n",
      "10000/10000 [==============================] - 0s 36us/sample - loss: 0.0194 - accuracy: 0.8852\n",
      "10000/10000 [==============================] - 0s 37us/sample - loss: 0.0195 - accuracy: 0.8855\n",
      "10000/10000 [==============================] - 0s 36us/sample - loss: 0.0197 - accuracy: 0.8850\n",
      "10000/10000 [==============================] - 0s 38us/sample - loss: 0.0198 - accuracy: 0.8844\n",
      "10000/10000 [==============================] - 0s 37us/sample - loss: 0.0200 - accuracy: 0.8837\n",
      "10000/10000 [==============================] - 0s 37us/sample - loss: 0.0202 - accuracy: 0.8830\n",
      "10000/10000 [==============================] - 0s 37us/sample - loss: 0.0204 - accuracy: 0.8813\n",
      "10000/10000 [==============================] - 0s 40us/sample - loss: 0.0206 - accuracy: 0.8811\n",
      "10000/10000 [==============================] - 0s 36us/sample - loss: 0.0208 - accuracy: 0.8792\n",
      "10000/10000 [==============================] - 0s 35us/sample - loss: 0.0210 - accuracy: 0.8779\n",
      "10000/10000 [==============================] - 0s 37us/sample - loss: 0.0213 - accuracy: 0.8771\n",
      "10000/10000 [==============================] - 0s 36us/sample - loss: 0.0215 - accuracy: 0.8763\n",
      "10000/10000 [==============================] - 0s 37us/sample - loss: 0.0218 - accuracy: 0.8749\n",
      "10000/10000 [==============================] - 0s 37us/sample - loss: 0.0221 - accuracy: 0.8724\n",
      "10000/10000 [==============================] - 0s 37us/sample - loss: 0.0224 - accuracy: 0.8711\n",
      "5th iteration\n",
      "10000/10000 [==============================] - 1s 59us/sample - loss: 0.0224 - accuracy: 0.8711\n",
      "10000/10000 [==============================] - 0s 37us/sample - loss: 0.0228 - accuracy: 0.8704\n",
      "10000/10000 [==============================] - 0s 35us/sample - loss: 0.0232 - accuracy: 0.8687\n",
      "10000/10000 [==============================] - 0s 37us/sample - loss: 0.0236 - accuracy: 0.8685\n",
      "10000/10000 [==============================] - 0s 36us/sample - loss: 0.0241 - accuracy: 0.8672\n",
      "10000/10000 [==============================] - 0s 37us/sample - loss: 0.0246 - accuracy: 0.8663\n",
      "10000/10000 [==============================] - 0s 36us/sample - loss: 0.0251 - accuracy: 0.8652\n",
      "10000/10000 [==============================] - 0s 36us/sample - loss: 0.0257 - accuracy: 0.8629\n",
      "10000/10000 [==============================] - 0s 37us/sample - loss: 0.0263 - accuracy: 0.8611\n",
      "10000/10000 [==============================] - 0s 36us/sample - loss: 0.0269 - accuracy: 0.8579\n",
      "10000/10000 [==============================] - 0s 35us/sample - loss: 0.0276 - accuracy: 0.8546\n",
      "10000/10000 [==============================] - 0s 36us/sample - loss: 0.0283 - accuracy: 0.8511\n",
      "10000/10000 [==============================] - 0s 36us/sample - loss: 0.0290 - accuracy: 0.8466\n",
      "10000/10000 [==============================] - 0s 36us/sample - loss: 0.0297 - accuracy: 0.8428\n",
      "10000/10000 [==============================] - 0s 36us/sample - loss: 0.0305 - accuracy: 0.8383\n",
      "10000/10000 [==============================] - 0s 38us/sample - loss: 0.0313 - accuracy: 0.8325\n",
      "10000/10000 [==============================] - 0s 36us/sample - loss: 0.0321 - accuracy: 0.8272\n",
      "10000/10000 [==============================] - 0s 36us/sample - loss: 0.0329 - accuracy: 0.8220\n",
      "10000/10000 [==============================] - 0s 36us/sample - loss: 0.0338 - accuracy: 0.8157\n",
      "10000/10000 [==============================] - 0s 37us/sample - loss: 0.0346 - accuracy: 0.8089\n",
      "10000/10000 [==============================] - 0s 37us/sample - loss: 0.0355 - accuracy: 0.8016\n",
      "6th iteration\n",
      "10000/10000 [==============================] - 0s 37us/sample - loss: 0.0190 - accuracy: 0.8848\n",
      "10000/10000 [==============================] - 0s 36us/sample - loss: 0.0189 - accuracy: 0.8855\n",
      "10000/10000 [==============================] - 0s 37us/sample - loss: 0.0189 - accuracy: 0.8858\n",
      "10000/10000 [==============================] - 0s 36us/sample - loss: 0.0189 - accuracy: 0.8868\n",
      "10000/10000 [==============================] - 0s 36us/sample - loss: 0.0189 - accuracy: 0.8878\n",
      "10000/10000 [==============================] - 0s 35us/sample - loss: 0.0190 - accuracy: 0.8877\n",
      "10000/10000 [==============================] - 0s 36us/sample - loss: 0.0190 - accuracy: 0.8882\n",
      "10000/10000 [==============================] - 0s 35us/sample - loss: 0.0190 - accuracy: 0.8881\n",
      "10000/10000 [==============================] - 0s 37us/sample - loss: 0.0191 - accuracy: 0.8885\n",
      "10000/10000 [==============================] - 0s 39us/sample - loss: 0.0191 - accuracy: 0.8887\n",
      "10000/10000 [==============================] - 0s 36us/sample - loss: 0.0192 - accuracy: 0.8880\n",
      "10000/10000 [==============================] - 0s 38us/sample - loss: 0.0193 - accuracy: 0.8885\n",
      "10000/10000 [==============================] - 0s 36us/sample - loss: 0.0194 - accuracy: 0.8879\n",
      "10000/10000 [==============================] - 0s 38us/sample - loss: 0.0195 - accuracy: 0.8869\n",
      "10000/10000 [==============================] - 0s 37us/sample - loss: 0.0196 - accuracy: 0.8870\n",
      "10000/10000 [==============================] - 0s 36us/sample - loss: 0.0198 - accuracy: 0.8858\n",
      "10000/10000 [==============================] - 0s 36us/sample - loss: 0.0199 - accuracy: 0.8859\n",
      "10000/10000 [==============================] - 0s 36us/sample - loss: 0.0201 - accuracy: 0.8850\n",
      "10000/10000 [==============================] - 0s 37us/sample - loss: 0.0202 - accuracy: 0.8838\n",
      "10000/10000 [==============================] - 0s 36us/sample - loss: 0.0204 - accuracy: 0.8822\n",
      "10000/10000 [==============================] - 0s 37us/sample - loss: 0.0206 - accuracy: 0.8813\n",
      "7th iteration\n",
      "10000/10000 [==============================] - 0s 36us/sample - loss: 0.0191 - accuracy: 0.8818\n",
      "10000/10000 [==============================] - 0s 36us/sample - loss: 0.0190 - accuracy: 0.8829\n",
      "10000/10000 [==============================] - 0s 37us/sample - loss: 0.0190 - accuracy: 0.8838\n",
      "10000/10000 [==============================] - 0s 37us/sample - loss: 0.0189 - accuracy: 0.8856\n",
      "10000/10000 [==============================] - 0s 37us/sample - loss: 0.0189 - accuracy: 0.8869\n",
      "10000/10000 [==============================] - 0s 36us/sample - loss: 0.0189 - accuracy: 0.8874\n",
      "10000/10000 [==============================] - 0s 38us/sample - loss: 0.0189 - accuracy: 0.8880\n",
      "10000/10000 [==============================] - 0s 36us/sample - loss: 0.0189 - accuracy: 0.8893\n",
      "10000/10000 [==============================] - 0s 37us/sample - loss: 0.0190 - accuracy: 0.8897\n",
      "10000/10000 [==============================] - 0s 38us/sample - loss: 0.0190 - accuracy: 0.8885\n",
      "10000/10000 [==============================] - 0s 38us/sample - loss: 0.0191 - accuracy: 0.8880\n",
      "10000/10000 [==============================] - 0s 37us/sample - loss: 0.0192 - accuracy: 0.8882\n",
      "10000/10000 [==============================] - 0s 37us/sample - loss: 0.0193 - accuracy: 0.8880\n",
      "10000/10000 [==============================] - 0s 37us/sample - loss: 0.0194 - accuracy: 0.8876\n",
      "10000/10000 [==============================] - 0s 36us/sample - loss: 0.0195 - accuracy: 0.8871\n",
      "10000/10000 [==============================] - 0s 36us/sample - loss: 0.0197 - accuracy: 0.8861\n",
      "10000/10000 [==============================] - 0s 37us/sample - loss: 0.0198 - accuracy: 0.8844\n",
      "10000/10000 [==============================] - 0s 36us/sample - loss: 0.0200 - accuracy: 0.8844\n",
      "10000/10000 [==============================] - 0s 37us/sample - loss: 0.0202 - accuracy: 0.8838\n",
      "10000/10000 [==============================] - 0s 37us/sample - loss: 0.0204 - accuracy: 0.8833\n",
      "10000/10000 [==============================] - 0s 38us/sample - loss: 0.0206 - accuracy: 0.8813\n",
      "8th iteration\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000/10000 [==============================] - 0s 36us/sample - loss: 0.0190 - accuracy: 0.8848\n",
      "10000/10000 [==============================] - 0s 37us/sample - loss: 0.0191 - accuracy: 0.8852\n",
      "10000/10000 [==============================] - 0s 36us/sample - loss: 0.0193 - accuracy: 0.8859\n",
      "10000/10000 [==============================] - 0s 37us/sample - loss: 0.0195 - accuracy: 0.8861\n",
      "10000/10000 [==============================] - 0s 39us/sample - loss: 0.0197 - accuracy: 0.8856\n",
      "10000/10000 [==============================] - 0s 36us/sample - loss: 0.0199 - accuracy: 0.8847\n",
      "10000/10000 [==============================] - 0s 38us/sample - loss: 0.0202 - accuracy: 0.8847\n",
      "10000/10000 [==============================] - 0s 37us/sample - loss: 0.0204 - accuracy: 0.8831\n",
      "10000/10000 [==============================] - 0s 38us/sample - loss: 0.0207 - accuracy: 0.8828\n",
      "10000/10000 [==============================] - 0s 35us/sample - loss: 0.0211 - accuracy: 0.8817\n",
      "10000/10000 [==============================] - 0s 37us/sample - loss: 0.0214 - accuracy: 0.8808\n",
      "10000/10000 [==============================] - 0s 38us/sample - loss: 0.0218 - accuracy: 0.8792\n",
      "10000/10000 [==============================] - 0s 36us/sample - loss: 0.0222 - accuracy: 0.8784\n",
      "10000/10000 [==============================] - 0s 36us/sample - loss: 0.0226 - accuracy: 0.8766\n",
      "10000/10000 [==============================] - 0s 36us/sample - loss: 0.0231 - accuracy: 0.8746\n",
      "10000/10000 [==============================] - 0s 36us/sample - loss: 0.0236 - accuracy: 0.8728\n",
      "10000/10000 [==============================] - 0s 37us/sample - loss: 0.0241 - accuracy: 0.8706\n",
      "10000/10000 [==============================] - 0s 37us/sample - loss: 0.0246 - accuracy: 0.8681\n",
      "10000/10000 [==============================] - 0s 35us/sample - loss: 0.0251 - accuracy: 0.8663\n",
      "10000/10000 [==============================] - 0s 36us/sample - loss: 0.0257 - accuracy: 0.8642\n",
      "10000/10000 [==============================] - 0s 37us/sample - loss: 0.0263 - accuracy: 0.8606\n",
      "9th iteration\n",
      "10000/10000 [==============================] - 0s 35us/sample - loss: 0.0191 - accuracy: 0.8818\n",
      "10000/10000 [==============================] - 0s 38us/sample - loss: 0.0191 - accuracy: 0.8821\n",
      "10000/10000 [==============================] - 0s 36us/sample - loss: 0.0191 - accuracy: 0.8828\n",
      "10000/10000 [==============================] - 0s 38us/sample - loss: 0.0192 - accuracy: 0.8839\n",
      "10000/10000 [==============================] - 0s 38us/sample - loss: 0.0192 - accuracy: 0.8845\n",
      "10000/10000 [==============================] - 0s 36us/sample - loss: 0.0193 - accuracy: 0.8843\n",
      "10000/10000 [==============================] - 0s 38us/sample - loss: 0.0194 - accuracy: 0.8848\n",
      "10000/10000 [==============================] - 0s 37us/sample - loss: 0.0195 - accuracy: 0.8845\n",
      "10000/10000 [==============================] - 0s 37us/sample - loss: 0.0196 - accuracy: 0.8842\n",
      "10000/10000 [==============================] - 0s 36us/sample - loss: 0.0198 - accuracy: 0.8843\n",
      "10000/10000 [==============================] - 0s 36us/sample - loss: 0.0199 - accuracy: 0.8836\n",
      "10000/10000 [==============================] - 0s 37us/sample - loss: 0.0201 - accuracy: 0.8826\n",
      "10000/10000 [==============================] - 0s 36us/sample - loss: 0.0203 - accuracy: 0.8827\n",
      "10000/10000 [==============================] - 0s 37us/sample - loss: 0.0205 - accuracy: 0.8821\n",
      "10000/10000 [==============================] - 0s 36us/sample - loss: 0.0207 - accuracy: 0.8813\n",
      "10000/10000 [==============================] - 0s 37us/sample - loss: 0.0210 - accuracy: 0.8793\n",
      "10000/10000 [==============================] - 0s 37us/sample - loss: 0.0212 - accuracy: 0.8781\n",
      "10000/10000 [==============================] - 0s 37us/sample - loss: 0.0215 - accuracy: 0.8767\n",
      "10000/10000 [==============================] - 0s 36us/sample - loss: 0.0218 - accuracy: 0.8752\n",
      "10000/10000 [==============================] - 0s 37us/sample - loss: 0.0221 - accuracy: 0.8731\n",
      "10000/10000 [==============================] - 0s 39us/sample - loss: 0.0224 - accuracy: 0.8711\n",
      "10th iteration\n",
      "10000/10000 [==============================] - 0s 37us/sample - loss: 0.0206 - accuracy: 0.8813\n",
      "10000/10000 [==============================] - 1s 58us/sample - loss: 0.0210 - accuracy: 0.8811\n",
      "10000/10000 [==============================] - 0s 35us/sample - loss: 0.0214 - accuracy: 0.8793\n",
      "10000/10000 [==============================] - 0s 36us/sample - loss: 0.0219 - accuracy: 0.8779\n",
      "10000/10000 [==============================] - 0s 35us/sample - loss: 0.0224 - accuracy: 0.8764\n",
      "10000/10000 [==============================] - 0s 35us/sample - loss: 0.0229 - accuracy: 0.8744\n",
      "10000/10000 [==============================] - 0s 36us/sample - loss: 0.0235 - accuracy: 0.8722\n",
      "10000/10000 [==============================] - 0s 35us/sample - loss: 0.0242 - accuracy: 0.8703\n",
      "10000/10000 [==============================] - 0s 37us/sample - loss: 0.0249 - accuracy: 0.8677\n",
      "10000/10000 [==============================] - 0s 35us/sample - loss: 0.0256 - accuracy: 0.8654\n",
      "10000/10000 [==============================] - 0s 36us/sample - loss: 0.0263 - accuracy: 0.8627\n",
      "10000/10000 [==============================] - 0s 36us/sample - loss: 0.0271 - accuracy: 0.8594\n",
      "10000/10000 [==============================] - 0s 38us/sample - loss: 0.0279 - accuracy: 0.8552\n",
      "10000/10000 [==============================] - 0s 37us/sample - loss: 0.0288 - accuracy: 0.8509\n",
      "10000/10000 [==============================] - 0s 37us/sample - loss: 0.0297 - accuracy: 0.8439\n",
      "10000/10000 [==============================] - 0s 36us/sample - loss: 0.0306 - accuracy: 0.8388\n",
      "10000/10000 [==============================] - 0s 35us/sample - loss: 0.0315 - accuracy: 0.8333\n",
      "10000/10000 [==============================] - 0s 37us/sample - loss: 0.0325 - accuracy: 0.8246\n",
      "10000/10000 [==============================] - 0s 36us/sample - loss: 0.0335 - accuracy: 0.8175\n",
      "10000/10000 [==============================] - 0s 37us/sample - loss: 0.0345 - accuracy: 0.8101\n",
      "10000/10000 [==============================] - 0s 37us/sample - loss: 0.0355 - accuracy: 0.8016\n",
      "11th iteration\n",
      "10000/10000 [==============================] - 0s 36us/sample - loss: 0.0181 - accuracy: 0.8868\n",
      "10000/10000 [==============================] - 0s 37us/sample - loss: 0.0180 - accuracy: 0.8880\n",
      "10000/10000 [==============================] - 0s 38us/sample - loss: 0.0179 - accuracy: 0.8887\n",
      "10000/10000 [==============================] - 0s 37us/sample - loss: 0.0179 - accuracy: 0.8891\n",
      "10000/10000 [==============================] - 1s 60us/sample - loss: 0.0178 - accuracy: 0.8898\n",
      "10000/10000 [==============================] - 0s 38us/sample - loss: 0.0178 - accuracy: 0.8904\n",
      "10000/10000 [==============================] - 0s 39us/sample - loss: 0.0178 - accuracy: 0.8908\n",
      "10000/10000 [==============================] - 0s 38us/sample - loss: 0.0178 - accuracy: 0.8909\n",
      "10000/10000 [==============================] - 0s 37us/sample - loss: 0.0178 - accuracy: 0.8921\n",
      "10000/10000 [==============================] - 0s 36us/sample - loss: 0.0178 - accuracy: 0.8918\n",
      "10000/10000 [==============================] - 0s 36us/sample - loss: 0.0179 - accuracy: 0.8914\n",
      "10000/10000 [==============================] - 0s 36us/sample - loss: 0.0179 - accuracy: 0.8913\n",
      "10000/10000 [==============================] - 0s 36us/sample - loss: 0.0180 - accuracy: 0.8912\n",
      "10000/10000 [==============================] - 0s 38us/sample - loss: 0.0181 - accuracy: 0.8912\n",
      "10000/10000 [==============================] - 0s 36us/sample - loss: 0.0181 - accuracy: 0.8903\n",
      "10000/10000 [==============================] - 0s 39us/sample - loss: 0.0182 - accuracy: 0.8900\n",
      "10000/10000 [==============================] - 0s 37us/sample - loss: 0.0184 - accuracy: 0.8890\n",
      "10000/10000 [==============================] - 0s 36us/sample - loss: 0.0185 - accuracy: 0.8877\n",
      "10000/10000 [==============================] - 0s 37us/sample - loss: 0.0186 - accuracy: 0.8871\n",
      "10000/10000 [==============================] - 0s 36us/sample - loss: 0.0188 - accuracy: 0.8857\n",
      "10000/10000 [==============================] - 0s 37us/sample - loss: 0.0190 - accuracy: 0.8848\n",
      "12th iteration\n",
      "10000/10000 [==============================] - 0s 35us/sample - loss: 0.0181 - accuracy: 0.8868\n",
      "10000/10000 [==============================] - 0s 37us/sample - loss: 0.0181 - accuracy: 0.8881\n",
      "10000/10000 [==============================] - 0s 37us/sample - loss: 0.0181 - accuracy: 0.8877\n",
      "10000/10000 [==============================] - 0s 35us/sample - loss: 0.0182 - accuracy: 0.8882\n",
      "10000/10000 [==============================] - 0s 37us/sample - loss: 0.0183 - accuracy: 0.8878\n",
      "10000/10000 [==============================] - 0s 36us/sample - loss: 0.0184 - accuracy: 0.8880\n",
      "10000/10000 [==============================] - 0s 38us/sample - loss: 0.0185 - accuracy: 0.8879\n",
      "10000/10000 [==============================] - 0s 38us/sample - loss: 0.0186 - accuracy: 0.8877\n",
      "10000/10000 [==============================] - 0s 37us/sample - loss: 0.0188 - accuracy: 0.8876\n",
      "10000/10000 [==============================] - 0s 36us/sample - loss: 0.0190 - accuracy: 0.8873\n",
      "10000/10000 [==============================] - 0s 36us/sample - loss: 0.0192 - accuracy: 0.8871\n",
      "10000/10000 [==============================] - 0s 38us/sample - loss: 0.0194 - accuracy: 0.8872\n",
      "10000/10000 [==============================] - 0s 37us/sample - loss: 0.0196 - accuracy: 0.8864\n",
      "10000/10000 [==============================] - 0s 36us/sample - loss: 0.0199 - accuracy: 0.8854\n",
      "10000/10000 [==============================] - 0s 36us/sample - loss: 0.0202 - accuracy: 0.8840\n",
      "10000/10000 [==============================] - 0s 37us/sample - loss: 0.0205 - accuracy: 0.8815\n",
      "10000/10000 [==============================] - 0s 38us/sample - loss: 0.0209 - accuracy: 0.8795\n",
      "10000/10000 [==============================] - 0s 38us/sample - loss: 0.0212 - accuracy: 0.8780\n",
      "10000/10000 [==============================] - 0s 36us/sample - loss: 0.0216 - accuracy: 0.8759\n",
      "10000/10000 [==============================] - 0s 36us/sample - loss: 0.0220 - accuracy: 0.8734\n",
      "10000/10000 [==============================] - 0s 37us/sample - loss: 0.0224 - accuracy: 0.8711\n",
      "13th iteration\n",
      "10000/10000 [==============================] - 0s 36us/sample - loss: 0.0191 - accuracy: 0.8818\n",
      "10000/10000 [==============================] - 0s 36us/sample - loss: 0.0189 - accuracy: 0.8834\n",
      "10000/10000 [==============================] - 0s 37us/sample - loss: 0.0187 - accuracy: 0.8851\n",
      "10000/10000 [==============================] - 0s 37us/sample - loss: 0.0186 - accuracy: 0.8873\n",
      "10000/10000 [==============================] - 0s 36us/sample - loss: 0.0185 - accuracy: 0.8885\n",
      "10000/10000 [==============================] - 0s 36us/sample - loss: 0.0184 - accuracy: 0.8891\n",
      "10000/10000 [==============================] - 0s 37us/sample - loss: 0.0183 - accuracy: 0.8898\n",
      "10000/10000 [==============================] - 0s 37us/sample - loss: 0.0182 - accuracy: 0.8898\n",
      "10000/10000 [==============================] - 0s 35us/sample - loss: 0.0182 - accuracy: 0.8912\n",
      "10000/10000 [==============================] - 0s 36us/sample - loss: 0.0181 - accuracy: 0.8907\n",
      "10000/10000 [==============================] - 0s 38us/sample - loss: 0.0181 - accuracy: 0.8902\n",
      "10000/10000 [==============================] - 0s 38us/sample - loss: 0.0181 - accuracy: 0.8906\n",
      "10000/10000 [==============================] - 0s 37us/sample - loss: 0.0182 - accuracy: 0.8902\n",
      "10000/10000 [==============================] - 0s 36us/sample - loss: 0.0182 - accuracy: 0.8899\n",
      "10000/10000 [==============================] - 0s 36us/sample - loss: 0.0183 - accuracy: 0.8896\n",
      "10000/10000 [==============================] - 0s 35us/sample - loss: 0.0183 - accuracy: 0.8900\n",
      "10000/10000 [==============================] - 0s 35us/sample - loss: 0.0184 - accuracy: 0.8890\n",
      "10000/10000 [==============================] - 0s 37us/sample - loss: 0.0185 - accuracy: 0.8884\n",
      "10000/10000 [==============================] - 0s 36us/sample - loss: 0.0187 - accuracy: 0.8875\n",
      "10000/10000 [==============================] - 0s 36us/sample - loss: 0.0188 - accuracy: 0.8863\n",
      "10000/10000 [==============================] - 0s 37us/sample - loss: 0.0190 - accuracy: 0.8848\n",
      "14th iteration\n",
      "10000/10000 [==============================] - 0s 37us/sample - loss: 0.0181 - accuracy: 0.8868\n",
      "10000/10000 [==============================] - 0s 37us/sample - loss: 0.0180 - accuracy: 0.8883\n",
      "10000/10000 [==============================] - 0s 37us/sample - loss: 0.0180 - accuracy: 0.8884\n",
      "10000/10000 [==============================] - 0s 38us/sample - loss: 0.0180 - accuracy: 0.8901\n",
      "10000/10000 [==============================] - 0s 36us/sample - loss: 0.0180 - accuracy: 0.8916\n",
      "10000/10000 [==============================] - 0s 36us/sample - loss: 0.0180 - accuracy: 0.8912\n",
      "10000/10000 [==============================] - 0s 37us/sample - loss: 0.0180 - accuracy: 0.8913\n",
      "10000/10000 [==============================] - 0s 36us/sample - loss: 0.0181 - accuracy: 0.8914\n",
      "10000/10000 [==============================] - 0s 38us/sample - loss: 0.0182 - accuracy: 0.8908\n",
      "10000/10000 [==============================] - 0s 36us/sample - loss: 0.0183 - accuracy: 0.8909\n",
      "10000/10000 [==============================] - 0s 36us/sample - loss: 0.0184 - accuracy: 0.8916\n",
      "10000/10000 [==============================] - 0s 37us/sample - loss: 0.0185 - accuracy: 0.8907\n",
      "10000/10000 [==============================] - 0s 37us/sample - loss: 0.0187 - accuracy: 0.8902\n",
      "10000/10000 [==============================] - 0s 36us/sample - loss: 0.0188 - accuracy: 0.8897\n",
      "10000/10000 [==============================] - 0s 37us/sample - loss: 0.0190 - accuracy: 0.8887\n",
      "10000/10000 [==============================] - 0s 36us/sample - loss: 0.0192 - accuracy: 0.8884\n",
      "10000/10000 [==============================] - 0s 36us/sample - loss: 0.0195 - accuracy: 0.8879\n",
      "10000/10000 [==============================] - 0s 38us/sample - loss: 0.0197 - accuracy: 0.8864\n",
      "10000/10000 [==============================] - 0s 37us/sample - loss: 0.0200 - accuracy: 0.8852\n",
      "10000/10000 [==============================] - 0s 36us/sample - loss: 0.0203 - accuracy: 0.8836\n",
      "10000/10000 [==============================] - 0s 36us/sample - loss: 0.0206 - accuracy: 0.8813\n",
      "15th iteration\n",
      "10000/10000 [==============================] - 0s 36us/sample - loss: 0.0191 - accuracy: 0.8818\n",
      "10000/10000 [==============================] - 0s 35us/sample - loss: 0.0192 - accuracy: 0.8821\n",
      "10000/10000 [==============================] - 0s 37us/sample - loss: 0.0193 - accuracy: 0.8832\n",
      "10000/10000 [==============================] - 0s 37us/sample - loss: 0.0195 - accuracy: 0.8832\n",
      "10000/10000 [==============================] - 0s 36us/sample - loss: 0.0197 - accuracy: 0.8832\n",
      "10000/10000 [==============================] - 0s 37us/sample - loss: 0.0199 - accuracy: 0.8836\n",
      "10000/10000 [==============================] - 0s 39us/sample - loss: 0.0201 - accuracy: 0.8839\n",
      "10000/10000 [==============================] - 0s 38us/sample - loss: 0.0204 - accuracy: 0.8832\n",
      "10000/10000 [==============================] - 0s 36us/sample - loss: 0.0207 - accuracy: 0.8825\n",
      "10000/10000 [==============================] - 0s 37us/sample - loss: 0.0210 - accuracy: 0.8821\n",
      "10000/10000 [==============================] - 0s 37us/sample - loss: 0.0214 - accuracy: 0.8810\n",
      "10000/10000 [==============================] - 0s 36us/sample - loss: 0.0218 - accuracy: 0.8808\n",
      "10000/10000 [==============================] - 0s 37us/sample - loss: 0.0222 - accuracy: 0.8803\n",
      "10000/10000 [==============================] - 0s 38us/sample - loss: 0.0226 - accuracy: 0.8793\n",
      "10000/10000 [==============================] - 0s 37us/sample - loss: 0.0230 - accuracy: 0.8778\n",
      "10000/10000 [==============================] - 0s 37us/sample - loss: 0.0235 - accuracy: 0.8762\n",
      "10000/10000 [==============================] - 0s 36us/sample - loss: 0.0240 - accuracy: 0.8734\n",
      "10000/10000 [==============================] - 0s 36us/sample - loss: 0.0246 - accuracy: 0.8702\n",
      "10000/10000 [==============================] - 0s 37us/sample - loss: 0.0251 - accuracy: 0.8673\n",
      "10000/10000 [==============================] - 0s 36us/sample - loss: 0.0257 - accuracy: 0.8647\n",
      "10000/10000 [==============================] - 0s 38us/sample - loss: 0.0263 - accuracy: 0.8606\n",
      "16th iteration\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000/10000 [==============================] - 0s 38us/sample - loss: 0.0179 - accuracy: 0.8823\n",
      "10000/10000 [==============================] - 0s 37us/sample - loss: 0.0178 - accuracy: 0.8835\n",
      "10000/10000 [==============================] - 0s 36us/sample - loss: 0.0177 - accuracy: 0.8862\n",
      "10000/10000 [==============================] - 0s 37us/sample - loss: 0.0176 - accuracy: 0.8867\n",
      "10000/10000 [==============================] - 0s 37us/sample - loss: 0.0175 - accuracy: 0.8882\n",
      "10000/10000 [==============================] - 1s 59us/sample - loss: 0.0175 - accuracy: 0.8881\n",
      "10000/10000 [==============================] - 0s 36us/sample - loss: 0.0175 - accuracy: 0.8879\n",
      "10000/10000 [==============================] - 0s 37us/sample - loss: 0.0175 - accuracy: 0.8882\n",
      "10000/10000 [==============================] - 0s 38us/sample - loss: 0.0175 - accuracy: 0.8893\n",
      "10000/10000 [==============================] - 0s 38us/sample - loss: 0.0175 - accuracy: 0.8896\n",
      "10000/10000 [==============================] - 0s 37us/sample - loss: 0.0175 - accuracy: 0.8891\n",
      "10000/10000 [==============================] - 0s 38us/sample - loss: 0.0176 - accuracy: 0.8889\n",
      "10000/10000 [==============================] - 0s 37us/sample - loss: 0.0177 - accuracy: 0.8895\n",
      "10000/10000 [==============================] - 0s 38us/sample - loss: 0.0178 - accuracy: 0.8890\n",
      "10000/10000 [==============================] - 0s 37us/sample - loss: 0.0179 - accuracy: 0.8891\n",
      "10000/10000 [==============================] - 0s 35us/sample - loss: 0.0180 - accuracy: 0.8891\n",
      "10000/10000 [==============================] - 0s 36us/sample - loss: 0.0182 - accuracy: 0.8885\n",
      "10000/10000 [==============================] - 0s 38us/sample - loss: 0.0183 - accuracy: 0.8873\n",
      "10000/10000 [==============================] - 0s 36us/sample - loss: 0.0185 - accuracy: 0.8874\n",
      "10000/10000 [==============================] - 0s 37us/sample - loss: 0.0187 - accuracy: 0.8863\n",
      "10000/10000 [==============================] - 0s 37us/sample - loss: 0.0190 - accuracy: 0.8848\n",
      "17th iteration\n",
      "10000/10000 [==============================] - 0s 36us/sample - loss: 0.0179 - accuracy: 0.8823\n",
      "10000/10000 [==============================] - 0s 38us/sample - loss: 0.0178 - accuracy: 0.8836\n",
      "10000/10000 [==============================] - 0s 36us/sample - loss: 0.0178 - accuracy: 0.8852\n",
      "10000/10000 [==============================] - 0s 37us/sample - loss: 0.0178 - accuracy: 0.8854\n",
      "10000/10000 [==============================] - 0s 37us/sample - loss: 0.0178 - accuracy: 0.8867\n",
      "10000/10000 [==============================] - 0s 36us/sample - loss: 0.0178 - accuracy: 0.8870\n",
      "10000/10000 [==============================] - 0s 37us/sample - loss: 0.0178 - accuracy: 0.8868\n",
      "10000/10000 [==============================] - 0s 37us/sample - loss: 0.0179 - accuracy: 0.8867\n",
      "10000/10000 [==============================] - 1s 59us/sample - loss: 0.0180 - accuracy: 0.8870\n",
      "10000/10000 [==============================] - 0s 38us/sample - loss: 0.0181 - accuracy: 0.8870\n",
      "10000/10000 [==============================] - 0s 37us/sample - loss: 0.0182 - accuracy: 0.8868\n",
      "10000/10000 [==============================] - 0s 36us/sample - loss: 0.0183 - accuracy: 0.8880\n",
      "10000/10000 [==============================] - 0s 38us/sample - loss: 0.0185 - accuracy: 0.8886\n",
      "10000/10000 [==============================] - 0s 37us/sample - loss: 0.0187 - accuracy: 0.8880\n",
      "10000/10000 [==============================] - 0s 37us/sample - loss: 0.0189 - accuracy: 0.8876\n",
      "10000/10000 [==============================] - 0s 37us/sample - loss: 0.0191 - accuracy: 0.8865\n",
      "10000/10000 [==============================] - 0s 37us/sample - loss: 0.0194 - accuracy: 0.8856\n",
      "10000/10000 [==============================] - 0s 38us/sample - loss: 0.0196 - accuracy: 0.8860\n",
      "10000/10000 [==============================] - 0s 36us/sample - loss: 0.0199 - accuracy: 0.8847\n",
      "10000/10000 [==============================] - 0s 37us/sample - loss: 0.0202 - accuracy: 0.8835\n",
      "10000/10000 [==============================] - 0s 37us/sample - loss: 0.0206 - accuracy: 0.8813\n",
      "18th iteration\n",
      "10000/10000 [==============================] - 0s 36us/sample - loss: 0.0181 - accuracy: 0.8868\n",
      "10000/10000 [==============================] - 0s 35us/sample - loss: 0.0179 - accuracy: 0.8890\n",
      "10000/10000 [==============================] - 0s 37us/sample - loss: 0.0178 - accuracy: 0.8898\n",
      "10000/10000 [==============================] - 0s 37us/sample - loss: 0.0177 - accuracy: 0.8909\n",
      "10000/10000 [==============================] - 0s 35us/sample - loss: 0.0176 - accuracy: 0.8921\n",
      "10000/10000 [==============================] - 0s 36us/sample - loss: 0.0175 - accuracy: 0.8926\n",
      "10000/10000 [==============================] - 0s 37us/sample - loss: 0.0175 - accuracy: 0.8928\n",
      "10000/10000 [==============================] - 0s 37us/sample - loss: 0.0175 - accuracy: 0.8927\n",
      "10000/10000 [==============================] - 0s 37us/sample - loss: 0.0174 - accuracy: 0.8936\n",
      "10000/10000 [==============================] - 0s 36us/sample - loss: 0.0175 - accuracy: 0.8939\n",
      "10000/10000 [==============================] - 0s 36us/sample - loss: 0.0175 - accuracy: 0.8938\n",
      "10000/10000 [==============================] - 0s 37us/sample - loss: 0.0175 - accuracy: 0.8948\n",
      "10000/10000 [==============================] - 0s 38us/sample - loss: 0.0176 - accuracy: 0.8938\n",
      "10000/10000 [==============================] - 0s 37us/sample - loss: 0.0177 - accuracy: 0.8927\n",
      "10000/10000 [==============================] - 0s 37us/sample - loss: 0.0178 - accuracy: 0.8920\n",
      "10000/10000 [==============================] - 0s 36us/sample - loss: 0.0180 - accuracy: 0.8913\n",
      "10000/10000 [==============================] - 0s 38us/sample - loss: 0.0181 - accuracy: 0.8906\n",
      "10000/10000 [==============================] - 0s 37us/sample - loss: 0.0183 - accuracy: 0.8884\n",
      "10000/10000 [==============================] - 0s 38us/sample - loss: 0.0186 - accuracy: 0.8860\n",
      "10000/10000 [==============================] - 0s 36us/sample - loss: 0.0188 - accuracy: 0.8838\n",
      "10000/10000 [==============================] - 0s 37us/sample - loss: 0.0191 - accuracy: 0.8818\n",
      "19th iteration\n",
      "10000/10000 [==============================] - 0s 35us/sample - loss: 0.0179 - accuracy: 0.8823\n",
      "10000/10000 [==============================] - 0s 38us/sample - loss: 0.0179 - accuracy: 0.8832\n",
      "10000/10000 [==============================] - 0s 37us/sample - loss: 0.0179 - accuracy: 0.8851\n",
      "10000/10000 [==============================] - 0s 36us/sample - loss: 0.0180 - accuracy: 0.8850\n",
      "10000/10000 [==============================] - 0s 37us/sample - loss: 0.0180 - accuracy: 0.8854\n",
      "10000/10000 [==============================] - 0s 36us/sample - loss: 0.0181 - accuracy: 0.8849\n",
      "10000/10000 [==============================] - 0s 38us/sample - loss: 0.0183 - accuracy: 0.8847\n",
      "10000/10000 [==============================] - 0s 36us/sample - loss: 0.0184 - accuracy: 0.8846\n",
      "10000/10000 [==============================] - 0s 35us/sample - loss: 0.0186 - accuracy: 0.8846\n",
      "10000/10000 [==============================] - 0s 37us/sample - loss: 0.0188 - accuracy: 0.8836\n",
      "10000/10000 [==============================] - 0s 37us/sample - loss: 0.0190 - accuracy: 0.8837\n",
      "10000/10000 [==============================] - 0s 36us/sample - loss: 0.0192 - accuracy: 0.8837\n",
      "10000/10000 [==============================] - 0s 37us/sample - loss: 0.0195 - accuracy: 0.8838\n",
      "10000/10000 [==============================] - 0s 36us/sample - loss: 0.0198 - accuracy: 0.8827\n",
      "10000/10000 [==============================] - 0s 35us/sample - loss: 0.0201 - accuracy: 0.8818\n",
      "10000/10000 [==============================] - 0s 37us/sample - loss: 0.0204 - accuracy: 0.8812\n",
      "10000/10000 [==============================] - 0s 38us/sample - loss: 0.0208 - accuracy: 0.8797\n",
      "10000/10000 [==============================] - 0s 37us/sample - loss: 0.0211 - accuracy: 0.8762\n",
      "10000/10000 [==============================] - 0s 36us/sample - loss: 0.0215 - accuracy: 0.8748\n",
      "10000/10000 [==============================] - 0s 36us/sample - loss: 0.0220 - accuracy: 0.8730\n",
      "10000/10000 [==============================] - 0s 37us/sample - loss: 0.0224 - accuracy: 0.8711\n",
      "20th iteration\n",
      "10000/10000 [==============================] - 0s 36us/sample - loss: 0.0181 - accuracy: 0.8868\n",
      "10000/10000 [==============================] - 0s 37us/sample - loss: 0.0182 - accuracy: 0.8875\n",
      "10000/10000 [==============================] - 0s 36us/sample - loss: 0.0184 - accuracy: 0.8877\n",
      "10000/10000 [==============================] - 0s 38us/sample - loss: 0.0185 - accuracy: 0.8864\n",
      "10000/10000 [==============================] - 0s 37us/sample - loss: 0.0187 - accuracy: 0.8862\n",
      "10000/10000 [==============================] - 0s 38us/sample - loss: 0.0190 - accuracy: 0.8857\n",
      "10000/10000 [==============================] - 0s 39us/sample - loss: 0.0192 - accuracy: 0.8850\n",
      "10000/10000 [==============================] - 0s 38us/sample - loss: 0.0195 - accuracy: 0.8853\n",
      "10000/10000 [==============================] - 0s 42us/sample - loss: 0.0198 - accuracy: 0.8848\n",
      "10000/10000 [==============================] - 0s 39us/sample - loss: 0.0202 - accuracy: 0.8842\n",
      "10000/10000 [==============================] - 0s 36us/sample - loss: 0.0206 - accuracy: 0.8829\n",
      "10000/10000 [==============================] - 0s 39us/sample - loss: 0.0210 - accuracy: 0.8821\n",
      "10000/10000 [==============================] - 0s 41us/sample - loss: 0.0215 - accuracy: 0.8816\n",
      "10000/10000 [==============================] - 0s 38us/sample - loss: 0.0219 - accuracy: 0.8806\n",
      "10000/10000 [==============================] - 0s 37us/sample - loss: 0.0225 - accuracy: 0.8798\n",
      "10000/10000 [==============================] - 0s 35us/sample - loss: 0.0230 - accuracy: 0.8770\n",
      "10000/10000 [==============================] - 0s 38us/sample - loss: 0.0236 - accuracy: 0.8746\n",
      "10000/10000 [==============================] - 0s 38us/sample - loss: 0.0242 - accuracy: 0.8714\n",
      "10000/10000 [==============================] - 0s 38us/sample - loss: 0.0249 - accuracy: 0.8676\n",
      "10000/10000 [==============================] - 0s 38us/sample - loss: 0.0256 - accuracy: 0.8644\n",
      "10000/10000 [==============================] - 0s 38us/sample - loss: 0.0263 - accuracy: 0.8606\n",
      "21th iteration\n",
      "10000/10000 [==============================] - 0s 41us/sample - loss: 0.0179 - accuracy: 0.8823\n",
      "10000/10000 [==============================] - 0s 36us/sample - loss: 0.0177 - accuracy: 0.8843\n",
      "10000/10000 [==============================] - 0s 38us/sample - loss: 0.0177 - accuracy: 0.8860\n",
      "10000/10000 [==============================] - 0s 38us/sample - loss: 0.0176 - accuracy: 0.8869\n",
      "10000/10000 [==============================] - 0s 39us/sample - loss: 0.0175 - accuracy: 0.8891\n",
      "10000/10000 [==============================] - 0s 38us/sample - loss: 0.0175 - accuracy: 0.8890\n",
      "10000/10000 [==============================] - 0s 37us/sample - loss: 0.0174 - accuracy: 0.8882\n",
      "10000/10000 [==============================] - 0s 35us/sample - loss: 0.0174 - accuracy: 0.8883\n",
      "10000/10000 [==============================] - 0s 38us/sample - loss: 0.0175 - accuracy: 0.8890\n",
      "10000/10000 [==============================] - 0s 38us/sample - loss: 0.0175 - accuracy: 0.8887\n",
      "10000/10000 [==============================] - 0s 35us/sample - loss: 0.0175 - accuracy: 0.8885\n",
      "10000/10000 [==============================] - 0s 36us/sample - loss: 0.0176 - accuracy: 0.8887\n",
      "10000/10000 [==============================] - 0s 37us/sample - loss: 0.0177 - accuracy: 0.8896\n",
      "10000/10000 [==============================] - 0s 36us/sample - loss: 0.0178 - accuracy: 0.8889\n",
      "10000/10000 [==============================] - 0s 36us/sample - loss: 0.0179 - accuracy: 0.8890\n",
      "10000/10000 [==============================] - 0s 36us/sample - loss: 0.0181 - accuracy: 0.8879\n",
      "10000/10000 [==============================] - 0s 37us/sample - loss: 0.0182 - accuracy: 0.8869\n",
      "10000/10000 [==============================] - 0s 36us/sample - loss: 0.0184 - accuracy: 0.8864\n",
      "10000/10000 [==============================] - 0s 36us/sample - loss: 0.0186 - accuracy: 0.8852\n",
      "10000/10000 [==============================] - 0s 35us/sample - loss: 0.0188 - accuracy: 0.8832\n",
      "10000/10000 [==============================] - 0s 36us/sample - loss: 0.0191 - accuracy: 0.8818\n",
      "22th iteration\n",
      "10000/10000 [==============================] - 0s 37us/sample - loss: 0.0179 - accuracy: 0.8823\n",
      "10000/10000 [==============================] - 0s 36us/sample - loss: 0.0177 - accuracy: 0.8843\n",
      "10000/10000 [==============================] - 0s 36us/sample - loss: 0.0175 - accuracy: 0.8862\n",
      "10000/10000 [==============================] - 0s 35us/sample - loss: 0.0174 - accuracy: 0.8888\n",
      "10000/10000 [==============================] - 0s 36us/sample - loss: 0.0172 - accuracy: 0.8893\n",
      "10000/10000 [==============================] - 0s 36us/sample - loss: 0.0171 - accuracy: 0.8900\n",
      "10000/10000 [==============================] - 0s 34us/sample - loss: 0.0171 - accuracy: 0.8908\n",
      "10000/10000 [==============================] - 0s 37us/sample - loss: 0.0170 - accuracy: 0.8917\n",
      "10000/10000 [==============================] - 0s 35us/sample - loss: 0.0170 - accuracy: 0.8934\n",
      "10000/10000 [==============================] - 1s 59us/sample - loss: 0.0169 - accuracy: 0.8945\n",
      "10000/10000 [==============================] - 0s 38us/sample - loss: 0.0169 - accuracy: 0.8947\n",
      "10000/10000 [==============================] - 0s 36us/sample - loss: 0.0170 - accuracy: 0.8947\n",
      "10000/10000 [==============================] - 0s 36us/sample - loss: 0.0170 - accuracy: 0.8935\n",
      "10000/10000 [==============================] - 0s 37us/sample - loss: 0.0171 - accuracy: 0.8934\n",
      "10000/10000 [==============================] - 0s 36us/sample - loss: 0.0171 - accuracy: 0.8938\n",
      "10000/10000 [==============================] - 0s 36us/sample - loss: 0.0172 - accuracy: 0.8929\n",
      "10000/10000 [==============================] - 0s 36us/sample - loss: 0.0174 - accuracy: 0.8913\n",
      "10000/10000 [==============================] - 0s 35us/sample - loss: 0.0175 - accuracy: 0.8911\n",
      "10000/10000 [==============================] - 0s 36us/sample - loss: 0.0177 - accuracy: 0.8901\n",
      "10000/10000 [==============================] - 0s 37us/sample - loss: 0.0179 - accuracy: 0.8888\n",
      "10000/10000 [==============================] - 0s 34us/sample - loss: 0.0181 - accuracy: 0.8868\n",
      "23th iteration\n",
      "10000/10000 [==============================] - 0s 36us/sample - loss: 0.0190 - accuracy: 0.8848\n",
      "10000/10000 [==============================] - 0s 34us/sample - loss: 0.0193 - accuracy: 0.8847\n",
      "10000/10000 [==============================] - 0s 34us/sample - loss: 0.0196 - accuracy: 0.8844\n",
      "10000/10000 [==============================] - 0s 35us/sample - loss: 0.0201 - accuracy: 0.8842\n",
      "10000/10000 [==============================] - 0s 35us/sample - loss: 0.0205 - accuracy: 0.8825\n",
      "10000/10000 [==============================] - 0s 35us/sample - loss: 0.0211 - accuracy: 0.8815\n",
      "10000/10000 [==============================] - 0s 34us/sample - loss: 0.0216 - accuracy: 0.8795\n",
      "10000/10000 [==============================] - 0s 38us/sample - loss: 0.0223 - accuracy: 0.8762\n",
      "10000/10000 [==============================] - 0s 37us/sample - loss: 0.0230 - accuracy: 0.8753\n",
      "10000/10000 [==============================] - 0s 37us/sample - loss: 0.0238 - accuracy: 0.8715\n",
      "10000/10000 [==============================] - 0s 38us/sample - loss: 0.0246 - accuracy: 0.8676\n",
      "10000/10000 [==============================] - 0s 37us/sample - loss: 0.0255 - accuracy: 0.8635\n",
      "10000/10000 [==============================] - 1s 58us/sample - loss: 0.0264 - accuracy: 0.8602\n",
      "10000/10000 [==============================] - 0s 37us/sample - loss: 0.0274 - accuracy: 0.8552\n",
      "10000/10000 [==============================] - 0s 36us/sample - loss: 0.0284 - accuracy: 0.8502\n",
      "10000/10000 [==============================] - 0s 36us/sample - loss: 0.0295 - accuracy: 0.8425\n",
      "10000/10000 [==============================] - 0s 36us/sample - loss: 0.0306 - accuracy: 0.8354\n",
      "10000/10000 [==============================] - 0s 36us/sample - loss: 0.0318 - accuracy: 0.8283\n",
      "10000/10000 [==============================] - 0s 36us/sample - loss: 0.0330 - accuracy: 0.8201\n",
      "10000/10000 [==============================] - 0s 37us/sample - loss: 0.0342 - accuracy: 0.8120\n",
      "10000/10000 [==============================] - 0s 37us/sample - loss: 0.0355 - accuracy: 0.8016\n",
      "24th iteration\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000/10000 [==============================] - 0s 37us/sample - loss: 0.0167 - accuracy: 0.8959\n",
      "10000/10000 [==============================] - 0s 37us/sample - loss: 0.0167 - accuracy: 0.8965\n",
      "10000/10000 [==============================] - 0s 40us/sample - loss: 0.0167 - accuracy: 0.8969\n",
      "10000/10000 [==============================] - 0s 37us/sample - loss: 0.0167 - accuracy: 0.8972\n",
      "10000/10000 [==============================] - 0s 37us/sample - loss: 0.0168 - accuracy: 0.8966\n",
      "10000/10000 [==============================] - 0s 37us/sample - loss: 0.0169 - accuracy: 0.8972\n",
      "10000/10000 [==============================] - 0s 37us/sample - loss: 0.0170 - accuracy: 0.8976\n",
      "10000/10000 [==============================] - 0s 37us/sample - loss: 0.0171 - accuracy: 0.8977\n",
      "10000/10000 [==============================] - 0s 35us/sample - loss: 0.0173 - accuracy: 0.8969\n",
      "10000/10000 [==============================] - 0s 36us/sample - loss: 0.0174 - accuracy: 0.8966\n",
      "10000/10000 [==============================] - 0s 37us/sample - loss: 0.0176 - accuracy: 0.8971\n",
      "10000/10000 [==============================] - 0s 37us/sample - loss: 0.0178 - accuracy: 0.8971\n",
      "10000/10000 [==============================] - 0s 37us/sample - loss: 0.0180 - accuracy: 0.8960\n",
      "10000/10000 [==============================] - 0s 38us/sample - loss: 0.0183 - accuracy: 0.8939\n",
      "10000/10000 [==============================] - 0s 38us/sample - loss: 0.0185 - accuracy: 0.8933\n",
      "10000/10000 [==============================] - 0s 35us/sample - loss: 0.0188 - accuracy: 0.8913\n",
      "10000/10000 [==============================] - 0s 37us/sample - loss: 0.0191 - accuracy: 0.8898\n",
      "10000/10000 [==============================] - 0s 36us/sample - loss: 0.0195 - accuracy: 0.8873\n",
      "10000/10000 [==============================] - 0s 38us/sample - loss: 0.0198 - accuracy: 0.8861\n",
      "10000/10000 [==============================] - 0s 37us/sample - loss: 0.0202 - accuracy: 0.8835\n",
      "10000/10000 [==============================] - 0s 37us/sample - loss: 0.0206 - accuracy: 0.8813\n",
      "25th iteration\n",
      "10000/10000 [==============================] - 0s 39us/sample - loss: 0.0167 - accuracy: 0.8959\n",
      "10000/10000 [==============================] - 0s 35us/sample - loss: 0.0166 - accuracy: 0.8968\n",
      "10000/10000 [==============================] - 0s 36us/sample - loss: 0.0165 - accuracy: 0.8980\n",
      "10000/10000 [==============================] - 0s 38us/sample - loss: 0.0165 - accuracy: 0.8976\n",
      "10000/10000 [==============================] - 0s 38us/sample - loss: 0.0165 - accuracy: 0.8978\n",
      "10000/10000 [==============================] - 0s 37us/sample - loss: 0.0165 - accuracy: 0.8985\n",
      "10000/10000 [==============================] - 0s 38us/sample - loss: 0.0165 - accuracy: 0.8974\n",
      "10000/10000 [==============================] - 0s 37us/sample - loss: 0.0165 - accuracy: 0.8974\n",
      "10000/10000 [==============================] - 0s 40us/sample - loss: 0.0166 - accuracy: 0.8972\n",
      "10000/10000 [==============================] - 0s 37us/sample - loss: 0.0167 - accuracy: 0.8982\n",
      "10000/10000 [==============================] - 0s 37us/sample - loss: 0.0168 - accuracy: 0.8975\n",
      "10000/10000 [==============================] - 0s 38us/sample - loss: 0.0169 - accuracy: 0.8977\n",
      "10000/10000 [==============================] - 0s 37us/sample - loss: 0.0171 - accuracy: 0.8974\n",
      "10000/10000 [==============================] - 0s 38us/sample - loss: 0.0173 - accuracy: 0.8969\n",
      "10000/10000 [==============================] - 0s 37us/sample - loss: 0.0174 - accuracy: 0.8957\n",
      "10000/10000 [==============================] - 0s 36us/sample - loss: 0.0177 - accuracy: 0.8940\n",
      "10000/10000 [==============================] - 0s 38us/sample - loss: 0.0179 - accuracy: 0.8928\n",
      "10000/10000 [==============================] - 0s 36us/sample - loss: 0.0182 - accuracy: 0.8908\n",
      "10000/10000 [==============================] - 0s 40us/sample - loss: 0.0184 - accuracy: 0.8870\n",
      "10000/10000 [==============================] - 0s 36us/sample - loss: 0.0187 - accuracy: 0.8848\n",
      "10000/10000 [==============================] - 0s 39us/sample - loss: 0.0191 - accuracy: 0.8818\n",
      "26th iteration\n",
      "10000/10000 [==============================] - 0s 36us/sample - loss: 0.0167 - accuracy: 0.8959\n",
      "10000/10000 [==============================] - 0s 39us/sample - loss: 0.0166 - accuracy: 0.8967\n",
      "10000/10000 [==============================] - 0s 35us/sample - loss: 0.0165 - accuracy: 0.8965\n",
      "10000/10000 [==============================] - 0s 38us/sample - loss: 0.0165 - accuracy: 0.8975\n",
      "10000/10000 [==============================] - 0s 38us/sample - loss: 0.0165 - accuracy: 0.8983\n",
      "10000/10000 [==============================] - 0s 37us/sample - loss: 0.0165 - accuracy: 0.8983\n",
      "10000/10000 [==============================] - 0s 36us/sample - loss: 0.0165 - accuracy: 0.8985\n",
      "10000/10000 [==============================] - 0s 36us/sample - loss: 0.0166 - accuracy: 0.8979\n",
      "10000/10000 [==============================] - 0s 38us/sample - loss: 0.0167 - accuracy: 0.8973\n",
      "10000/10000 [==============================] - 0s 39us/sample - loss: 0.0168 - accuracy: 0.8975\n",
      "10000/10000 [==============================] - 0s 39us/sample - loss: 0.0169 - accuracy: 0.8967\n",
      "10000/10000 [==============================] - 0s 38us/sample - loss: 0.0170 - accuracy: 0.8951\n",
      "10000/10000 [==============================] - 0s 37us/sample - loss: 0.0171 - accuracy: 0.8948\n",
      "10000/10000 [==============================] - 0s 38us/sample - loss: 0.0173 - accuracy: 0.8944\n",
      "10000/10000 [==============================] - 0s 36us/sample - loss: 0.0175 - accuracy: 0.8938\n",
      "10000/10000 [==============================] - 0s 37us/sample - loss: 0.0177 - accuracy: 0.8922\n",
      "10000/10000 [==============================] - 0s 37us/sample - loss: 0.0179 - accuracy: 0.8914\n",
      "10000/10000 [==============================] - 0s 37us/sample - loss: 0.0181 - accuracy: 0.8902\n",
      "10000/10000 [==============================] - 0s 37us/sample - loss: 0.0184 - accuracy: 0.8893\n",
      "10000/10000 [==============================] - 0s 37us/sample - loss: 0.0187 - accuracy: 0.8872\n",
      "10000/10000 [==============================] - 0s 37us/sample - loss: 0.0190 - accuracy: 0.8848\n",
      "27th iteration\n",
      "10000/10000 [==============================] - 0s 36us/sample - loss: 0.0167 - accuracy: 0.8959\n",
      "10000/10000 [==============================] - 0s 38us/sample - loss: 0.0167 - accuracy: 0.8958\n",
      "10000/10000 [==============================] - 0s 37us/sample - loss: 0.0168 - accuracy: 0.8955\n",
      "10000/10000 [==============================] - 0s 36us/sample - loss: 0.0170 - accuracy: 0.8951\n",
      "10000/10000 [==============================] - 0s 38us/sample - loss: 0.0171 - accuracy: 0.8950\n",
      "10000/10000 [==============================] - 0s 39us/sample - loss: 0.0173 - accuracy: 0.8946\n",
      "10000/10000 [==============================] - 0s 38us/sample - loss: 0.0175 - accuracy: 0.8931\n",
      "10000/10000 [==============================] - 0s 36us/sample - loss: 0.0177 - accuracy: 0.8922\n",
      "10000/10000 [==============================] - 0s 38us/sample - loss: 0.0179 - accuracy: 0.8922\n",
      "10000/10000 [==============================] - 0s 39us/sample - loss: 0.0181 - accuracy: 0.8919\n",
      "10000/10000 [==============================] - 0s 37us/sample - loss: 0.0184 - accuracy: 0.8917\n",
      "10000/10000 [==============================] - 0s 38us/sample - loss: 0.0187 - accuracy: 0.8896\n",
      "10000/10000 [==============================] - 0s 40us/sample - loss: 0.0190 - accuracy: 0.8878\n",
      "10000/10000 [==============================] - 0s 37us/sample - loss: 0.0194 - accuracy: 0.8876\n",
      "10000/10000 [==============================] - 0s 38us/sample - loss: 0.0197 - accuracy: 0.8858\n",
      "10000/10000 [==============================] - 0s 38us/sample - loss: 0.0201 - accuracy: 0.8833\n",
      "10000/10000 [==============================] - 0s 36us/sample - loss: 0.0205 - accuracy: 0.8812\n",
      "10000/10000 [==============================] - 0s 37us/sample - loss: 0.0210 - accuracy: 0.8786\n",
      "10000/10000 [==============================] - 0s 37us/sample - loss: 0.0214 - accuracy: 0.8757\n",
      "10000/10000 [==============================] - 0s 39us/sample - loss: 0.0219 - accuracy: 0.8734\n",
      "10000/10000 [==============================] - 0s 38us/sample - loss: 0.0224 - accuracy: 0.8711\n",
      "28th iteration\n",
      "10000/10000 [==============================] - 0s 37us/sample - loss: 0.0179 - accuracy: 0.8823\n",
      "10000/10000 [==============================] - 0s 35us/sample - loss: 0.0179 - accuracy: 0.8836\n",
      "10000/10000 [==============================] - 0s 38us/sample - loss: 0.0180 - accuracy: 0.8846\n",
      "10000/10000 [==============================] - 0s 37us/sample - loss: 0.0182 - accuracy: 0.8846\n",
      "10000/10000 [==============================] - 0s 37us/sample - loss: 0.0183 - accuracy: 0.8842\n",
      "10000/10000 [==============================] - 0s 37us/sample - loss: 0.0186 - accuracy: 0.8831\n",
      "10000/10000 [==============================] - 0s 38us/sample - loss: 0.0188 - accuracy: 0.8840\n",
      "10000/10000 [==============================] - 0s 38us/sample - loss: 0.0191 - accuracy: 0.8826\n",
      "10000/10000 [==============================] - 0s 37us/sample - loss: 0.0194 - accuracy: 0.8830\n",
      "10000/10000 [==============================] - 0s 37us/sample - loss: 0.0197 - accuracy: 0.8824\n",
      "10000/10000 [==============================] - 0s 36us/sample - loss: 0.0201 - accuracy: 0.8828\n",
      "10000/10000 [==============================] - 0s 36us/sample - loss: 0.0205 - accuracy: 0.8823\n",
      "10000/10000 [==============================] - 0s 36us/sample - loss: 0.0210 - accuracy: 0.8814\n",
      "10000/10000 [==============================] - 1s 57us/sample - loss: 0.0215 - accuracy: 0.8806\n",
      "10000/10000 [==============================] - 0s 38us/sample - loss: 0.0221 - accuracy: 0.8783\n",
      "10000/10000 [==============================] - 0s 37us/sample - loss: 0.0227 - accuracy: 0.8769\n",
      "10000/10000 [==============================] - 0s 35us/sample - loss: 0.0233 - accuracy: 0.8742\n",
      "10000/10000 [==============================] - 0s 38us/sample - loss: 0.0240 - accuracy: 0.8716\n",
      "10000/10000 [==============================] - 0s 35us/sample - loss: 0.0247 - accuracy: 0.8680\n",
      "10000/10000 [==============================] - 0s 37us/sample - loss: 0.0255 - accuracy: 0.8656\n",
      "10000/10000 [==============================] - 0s 37us/sample - loss: 0.0263 - accuracy: 0.8606\n",
      "29th iteration\n",
      "10000/10000 [==============================] - 0s 37us/sample - loss: 0.0191 - accuracy: 0.8818\n",
      "10000/10000 [==============================] - 0s 36us/sample - loss: 0.0194 - accuracy: 0.8816\n",
      "10000/10000 [==============================] - 0s 35us/sample - loss: 0.0197 - accuracy: 0.8815\n",
      "10000/10000 [==============================] - 0s 36us/sample - loss: 0.0202 - accuracy: 0.8806\n",
      "10000/10000 [==============================] - 0s 37us/sample - loss: 0.0206 - accuracy: 0.8795\n",
      "10000/10000 [==============================] - 0s 36us/sample - loss: 0.0212 - accuracy: 0.8785\n",
      "10000/10000 [==============================] - 0s 36us/sample - loss: 0.0217 - accuracy: 0.8768\n",
      "10000/10000 [==============================] - 0s 36us/sample - loss: 0.0224 - accuracy: 0.8749\n",
      "10000/10000 [==============================] - 0s 38us/sample - loss: 0.0231 - accuracy: 0.8709\n",
      "10000/10000 [==============================] - 0s 35us/sample - loss: 0.0238 - accuracy: 0.8688\n",
      "10000/10000 [==============================] - 0s 37us/sample - loss: 0.0247 - accuracy: 0.8652\n",
      "10000/10000 [==============================] - 0s 36us/sample - loss: 0.0255 - accuracy: 0.8614\n",
      "10000/10000 [==============================] - 0s 36us/sample - loss: 0.0265 - accuracy: 0.8570\n",
      "10000/10000 [==============================] - 0s 36us/sample - loss: 0.0274 - accuracy: 0.8506\n",
      "10000/10000 [==============================] - 0s 37us/sample - loss: 0.0284 - accuracy: 0.8464\n",
      "10000/10000 [==============================] - 0s 36us/sample - loss: 0.0295 - accuracy: 0.8409\n",
      "10000/10000 [==============================] - 1s 59us/sample - loss: 0.0306 - accuracy: 0.8347\n",
      "10000/10000 [==============================] - 0s 36us/sample - loss: 0.0318 - accuracy: 0.8264\n",
      "10000/10000 [==============================] - 0s 37us/sample - loss: 0.0330 - accuracy: 0.8180\n",
      "10000/10000 [==============================] - 0s 37us/sample - loss: 0.0342 - accuracy: 0.8102\n",
      "10000/10000 [==============================] - 0s 36us/sample - loss: 0.0355 - accuracy: 0.8016\n",
      "30th iteration\n",
      "10000/10000 [==============================] - 0s 37us/sample - loss: 0.0167 - accuracy: 0.8959\n",
      "10000/10000 [==============================] - 0s 36us/sample - loss: 0.0165 - accuracy: 0.8968\n",
      "10000/10000 [==============================] - 0s 37us/sample - loss: 0.0164 - accuracy: 0.8982\n",
      "10000/10000 [==============================] - 0s 37us/sample - loss: 0.0163 - accuracy: 0.8992\n",
      "10000/10000 [==============================] - 0s 35us/sample - loss: 0.0162 - accuracy: 0.9002\n",
      "10000/10000 [==============================] - 0s 38us/sample - loss: 0.0161 - accuracy: 0.9003\n",
      "10000/10000 [==============================] - 0s 35us/sample - loss: 0.0161 - accuracy: 0.9003\n",
      "10000/10000 [==============================] - 0s 37us/sample - loss: 0.0161 - accuracy: 0.9001\n",
      "10000/10000 [==============================] - 0s 37us/sample - loss: 0.0161 - accuracy: 0.9013\n",
      "10000/10000 [==============================] - 0s 35us/sample - loss: 0.0161 - accuracy: 0.9004\n",
      "10000/10000 [==============================] - 0s 36us/sample - loss: 0.0162 - accuracy: 0.8995\n",
      "10000/10000 [==============================] - 0s 37us/sample - loss: 0.0162 - accuracy: 0.8979\n",
      "10000/10000 [==============================] - 0s 37us/sample - loss: 0.0163 - accuracy: 0.8971\n",
      "10000/10000 [==============================] - 0s 36us/sample - loss: 0.0165 - accuracy: 0.8960\n",
      "10000/10000 [==============================] - 0s 35us/sample - loss: 0.0166 - accuracy: 0.8948\n",
      "10000/10000 [==============================] - 0s 37us/sample - loss: 0.0168 - accuracy: 0.8940\n",
      "10000/10000 [==============================] - 0s 36us/sample - loss: 0.0169 - accuracy: 0.8916\n",
      "10000/10000 [==============================] - 0s 36us/sample - loss: 0.0171 - accuracy: 0.8898\n",
      "10000/10000 [==============================] - 0s 37us/sample - loss: 0.0174 - accuracy: 0.8878\n",
      "10000/10000 [==============================] - 0s 36us/sample - loss: 0.0176 - accuracy: 0.8852\n",
      "10000/10000 [==============================] - 0s 36us/sample - loss: 0.0179 - accuracy: 0.8823\n",
      "31th iteration\n",
      "10000/10000 [==============================] - 0s 36us/sample - loss: 0.0167 - accuracy: 0.8959\n",
      "10000/10000 [==============================] - 0s 38us/sample - loss: 0.0165 - accuracy: 0.8972\n",
      "10000/10000 [==============================] - 0s 37us/sample - loss: 0.0164 - accuracy: 0.8981\n",
      "10000/10000 [==============================] - 0s 38us/sample - loss: 0.0163 - accuracy: 0.8986\n",
      "10000/10000 [==============================] - 0s 37us/sample - loss: 0.0162 - accuracy: 0.8994\n",
      "10000/10000 [==============================] - 0s 37us/sample - loss: 0.0161 - accuracy: 0.8991\n",
      "10000/10000 [==============================] - 0s 37us/sample - loss: 0.0161 - accuracy: 0.8993\n",
      "10000/10000 [==============================] - 0s 37us/sample - loss: 0.0161 - accuracy: 0.9004\n",
      "10000/10000 [==============================] - 0s 37us/sample - loss: 0.0161 - accuracy: 0.9009\n",
      "10000/10000 [==============================] - 0s 37us/sample - loss: 0.0162 - accuracy: 0.9007\n",
      "10000/10000 [==============================] - 0s 37us/sample - loss: 0.0162 - accuracy: 0.9011\n",
      "10000/10000 [==============================] - 0s 36us/sample - loss: 0.0163 - accuracy: 0.9009\n",
      "10000/10000 [==============================] - 0s 36us/sample - loss: 0.0164 - accuracy: 0.8998\n",
      "10000/10000 [==============================] - 0s 36us/sample - loss: 0.0166 - accuracy: 0.8980\n",
      "10000/10000 [==============================] - 0s 37us/sample - loss: 0.0167 - accuracy: 0.8975\n",
      "10000/10000 [==============================] - 0s 36us/sample - loss: 0.0169 - accuracy: 0.8963\n",
      "10000/10000 [==============================] - 0s 35us/sample - loss: 0.0171 - accuracy: 0.8929\n",
      "10000/10000 [==============================] - 0s 37us/sample - loss: 0.0173 - accuracy: 0.8917\n",
      "10000/10000 [==============================] - 0s 36us/sample - loss: 0.0176 - accuracy: 0.8903\n",
      "10000/10000 [==============================] - 0s 38us/sample - loss: 0.0178 - accuracy: 0.8890\n",
      "10000/10000 [==============================] - 0s 37us/sample - loss: 0.0181 - accuracy: 0.8868\n",
      "32th iteration\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000/10000 [==============================] - 0s 37us/sample - loss: 0.0163 - accuracy: 0.8942\n",
      "10000/10000 [==============================] - 0s 37us/sample - loss: 0.0161 - accuracy: 0.8950\n",
      "10000/10000 [==============================] - 0s 36us/sample - loss: 0.0160 - accuracy: 0.8966\n",
      "10000/10000 [==============================] - 0s 37us/sample - loss: 0.0160 - accuracy: 0.8973\n",
      "10000/10000 [==============================] - 0s 38us/sample - loss: 0.0159 - accuracy: 0.8985\n",
      "10000/10000 [==============================] - 0s 37us/sample - loss: 0.0159 - accuracy: 0.8994\n",
      "10000/10000 [==============================] - 0s 37us/sample - loss: 0.0159 - accuracy: 0.9011\n",
      "10000/10000 [==============================] - 0s 36us/sample - loss: 0.0159 - accuracy: 0.8997\n",
      "10000/10000 [==============================] - 0s 37us/sample - loss: 0.0159 - accuracy: 0.9000\n",
      "10000/10000 [==============================] - 0s 35us/sample - loss: 0.0160 - accuracy: 0.8996\n",
      "10000/10000 [==============================] - 0s 36us/sample - loss: 0.0161 - accuracy: 0.8997\n",
      "10000/10000 [==============================] - 0s 35us/sample - loss: 0.0162 - accuracy: 0.8993\n",
      "10000/10000 [==============================] - 0s 38us/sample - loss: 0.0163 - accuracy: 0.8992\n",
      "10000/10000 [==============================] - 0s 37us/sample - loss: 0.0165 - accuracy: 0.8978\n",
      "10000/10000 [==============================] - 0s 37us/sample - loss: 0.0166 - accuracy: 0.8963\n",
      "10000/10000 [==============================] - 0s 37us/sample - loss: 0.0168 - accuracy: 0.8947\n",
      "10000/10000 [==============================] - 0s 37us/sample - loss: 0.0170 - accuracy: 0.8939\n",
      "10000/10000 [==============================] - 0s 36us/sample - loss: 0.0173 - accuracy: 0.8934\n",
      "10000/10000 [==============================] - 0s 37us/sample - loss: 0.0175 - accuracy: 0.8918\n",
      "10000/10000 [==============================] - 0s 40us/sample - loss: 0.0178 - accuracy: 0.8896\n",
      "10000/10000 [==============================] - 0s 36us/sample - loss: 0.0181 - accuracy: 0.8868\n",
      "33th iteration\n",
      "10000/10000 [==============================] - 0s 38us/sample - loss: 0.0163 - accuracy: 0.8942\n",
      "10000/10000 [==============================] - 0s 37us/sample - loss: 0.0162 - accuracy: 0.8949\n",
      "10000/10000 [==============================] - 0s 37us/sample - loss: 0.0161 - accuracy: 0.8947\n",
      "10000/10000 [==============================] - 0s 38us/sample - loss: 0.0161 - accuracy: 0.8957\n",
      "10000/10000 [==============================] - 0s 35us/sample - loss: 0.0161 - accuracy: 0.8958\n",
      "10000/10000 [==============================] - 0s 37us/sample - loss: 0.0161 - accuracy: 0.8967\n",
      "10000/10000 [==============================] - 0s 36us/sample - loss: 0.0161 - accuracy: 0.8974\n",
      "10000/10000 [==============================] - 0s 37us/sample - loss: 0.0162 - accuracy: 0.8974\n",
      "10000/10000 [==============================] - 0s 37us/sample - loss: 0.0163 - accuracy: 0.8986\n",
      "10000/10000 [==============================] - 0s 37us/sample - loss: 0.0164 - accuracy: 0.8986\n",
      "10000/10000 [==============================] - 0s 37us/sample - loss: 0.0165 - accuracy: 0.8979\n",
      "10000/10000 [==============================] - 0s 36us/sample - loss: 0.0166 - accuracy: 0.8973\n",
      "10000/10000 [==============================] - 0s 38us/sample - loss: 0.0168 - accuracy: 0.8966\n",
      "10000/10000 [==============================] - 0s 36us/sample - loss: 0.0170 - accuracy: 0.8963\n",
      "10000/10000 [==============================] - 0s 37us/sample - loss: 0.0172 - accuracy: 0.8945\n",
      "10000/10000 [==============================] - 0s 36us/sample - loss: 0.0175 - accuracy: 0.8929\n",
      "10000/10000 [==============================] - 0s 36us/sample - loss: 0.0177 - accuracy: 0.8921\n",
      "10000/10000 [==============================] - 0s 36us/sample - loss: 0.0180 - accuracy: 0.8903\n",
      "10000/10000 [==============================] - 0s 36us/sample - loss: 0.0183 - accuracy: 0.8884\n",
      "10000/10000 [==============================] - 0s 37us/sample - loss: 0.0186 - accuracy: 0.8874\n",
      "10000/10000 [==============================] - 0s 35us/sample - loss: 0.0190 - accuracy: 0.8848\n",
      "34th iteration\n",
      "10000/10000 [==============================] - 0s 39us/sample - loss: 0.0163 - accuracy: 0.8942\n",
      "10000/10000 [==============================] - 0s 37us/sample - loss: 0.0162 - accuracy: 0.8941\n",
      "10000/10000 [==============================] - 0s 37us/sample - loss: 0.0162 - accuracy: 0.8941\n",
      "10000/10000 [==============================] - 0s 36us/sample - loss: 0.0162 - accuracy: 0.8952\n",
      "10000/10000 [==============================] - 0s 36us/sample - loss: 0.0163 - accuracy: 0.8956\n",
      "10000/10000 [==============================] - 0s 39us/sample - loss: 0.0163 - accuracy: 0.8969\n",
      "10000/10000 [==============================] - 0s 36us/sample - loss: 0.0164 - accuracy: 0.8970\n",
      "10000/10000 [==============================] - 0s 36us/sample - loss: 0.0165 - accuracy: 0.8971\n",
      "10000/10000 [==============================] - 0s 36us/sample - loss: 0.0166 - accuracy: 0.8971\n",
      "10000/10000 [==============================] - 0s 38us/sample - loss: 0.0168 - accuracy: 0.8970\n",
      "10000/10000 [==============================] - 0s 37us/sample - loss: 0.0170 - accuracy: 0.8975\n",
      "10000/10000 [==============================] - 0s 35us/sample - loss: 0.0172 - accuracy: 0.8973\n",
      "10000/10000 [==============================] - 0s 37us/sample - loss: 0.0175 - accuracy: 0.8974\n",
      "10000/10000 [==============================] - 0s 37us/sample - loss: 0.0178 - accuracy: 0.8972\n",
      "10000/10000 [==============================] - 0s 40us/sample - loss: 0.0181 - accuracy: 0.8949\n",
      "10000/10000 [==============================] - 0s 37us/sample - loss: 0.0184 - accuracy: 0.8926\n",
      "10000/10000 [==============================] - 0s 36us/sample - loss: 0.0188 - accuracy: 0.8911\n",
      "10000/10000 [==============================] - 1s 60us/sample - loss: 0.0192 - accuracy: 0.8887\n",
      "10000/10000 [==============================] - 0s 36us/sample - loss: 0.0196 - accuracy: 0.8863\n",
      "10000/10000 [==============================] - 0s 36us/sample - loss: 0.0201 - accuracy: 0.8839\n",
      "10000/10000 [==============================] - 0s 36us/sample - loss: 0.0206 - accuracy: 0.8813\n",
      "35th iteration\n",
      "10000/10000 [==============================] - 0s 37us/sample - loss: 0.0163 - accuracy: 0.8942\n",
      "10000/10000 [==============================] - 0s 37us/sample - loss: 0.0161 - accuracy: 0.8949\n",
      "10000/10000 [==============================] - 0s 37us/sample - loss: 0.0160 - accuracy: 0.8979\n",
      "10000/10000 [==============================] - 0s 37us/sample - loss: 0.0159 - accuracy: 0.8978\n",
      "10000/10000 [==============================] - 0s 36us/sample - loss: 0.0158 - accuracy: 0.8995\n",
      "10000/10000 [==============================] - 0s 36us/sample - loss: 0.0157 - accuracy: 0.8996\n",
      "10000/10000 [==============================] - 0s 37us/sample - loss: 0.0157 - accuracy: 0.8993\n",
      "10000/10000 [==============================] - 0s 37us/sample - loss: 0.0157 - accuracy: 0.8997\n",
      "10000/10000 [==============================] - 0s 36us/sample - loss: 0.0157 - accuracy: 0.8999\n",
      "10000/10000 [==============================] - 0s 38us/sample - loss: 0.0157 - accuracy: 0.9006\n",
      "10000/10000 [==============================] - 0s 36us/sample - loss: 0.0158 - accuracy: 0.8997\n",
      "10000/10000 [==============================] - 0s 37us/sample - loss: 0.0159 - accuracy: 0.8982\n",
      "10000/10000 [==============================] - 0s 38us/sample - loss: 0.0160 - accuracy: 0.8983\n",
      "10000/10000 [==============================] - 0s 36us/sample - loss: 0.0162 - accuracy: 0.8968\n",
      "10000/10000 [==============================] - 0s 38us/sample - loss: 0.0164 - accuracy: 0.8957\n",
      "10000/10000 [==============================] - 0s 37us/sample - loss: 0.0165 - accuracy: 0.8936\n",
      "10000/10000 [==============================] - 0s 38us/sample - loss: 0.0168 - accuracy: 0.8921\n",
      "10000/10000 [==============================] - 0s 37us/sample - loss: 0.0170 - accuracy: 0.8906\n",
      "10000/10000 [==============================] - 0s 36us/sample - loss: 0.0173 - accuracy: 0.8893\n",
      "10000/10000 [==============================] - 0s 37us/sample - loss: 0.0176 - accuracy: 0.8857\n",
      "10000/10000 [==============================] - 1s 57us/sample - loss: 0.0179 - accuracy: 0.8823\n",
      "36th iteration\n",
      "10000/10000 [==============================] - 0s 37us/sample - loss: 0.0163 - accuracy: 0.8942\n",
      "10000/10000 [==============================] - 0s 37us/sample - loss: 0.0162 - accuracy: 0.8944\n",
      "10000/10000 [==============================] - 0s 38us/sample - loss: 0.0161 - accuracy: 0.8960\n",
      "10000/10000 [==============================] - 0s 37us/sample - loss: 0.0160 - accuracy: 0.8968\n",
      "10000/10000 [==============================] - 0s 36us/sample - loss: 0.0160 - accuracy: 0.8977\n",
      "10000/10000 [==============================] - 0s 37us/sample - loss: 0.0159 - accuracy: 0.8978\n",
      "10000/10000 [==============================] - 0s 37us/sample - loss: 0.0159 - accuracy: 0.8985\n",
      "10000/10000 [==============================] - 0s 37us/sample - loss: 0.0160 - accuracy: 0.8982\n",
      "10000/10000 [==============================] - 0s 36us/sample - loss: 0.0160 - accuracy: 0.8980\n",
      "10000/10000 [==============================] - 0s 40us/sample - loss: 0.0161 - accuracy: 0.8992\n",
      "10000/10000 [==============================] - 0s 36us/sample - loss: 0.0163 - accuracy: 0.8998\n",
      "10000/10000 [==============================] - 0s 36us/sample - loss: 0.0164 - accuracy: 0.8982\n",
      "10000/10000 [==============================] - 0s 37us/sample - loss: 0.0166 - accuracy: 0.8971\n",
      "10000/10000 [==============================] - 0s 36us/sample - loss: 0.0168 - accuracy: 0.8964\n",
      "10000/10000 [==============================] - 0s 37us/sample - loss: 0.0170 - accuracy: 0.8956\n",
      "10000/10000 [==============================] - 0s 36us/sample - loss: 0.0173 - accuracy: 0.8939\n",
      "10000/10000 [==============================] - 0s 37us/sample - loss: 0.0176 - accuracy: 0.8924\n",
      "10000/10000 [==============================] - 0s 37us/sample - loss: 0.0179 - accuracy: 0.8897\n",
      "10000/10000 [==============================] - 0s 37us/sample - loss: 0.0183 - accuracy: 0.8875\n",
      "10000/10000 [==============================] - 0s 37us/sample - loss: 0.0186 - accuracy: 0.8844\n",
      "10000/10000 [==============================] - 0s 35us/sample - loss: 0.0191 - accuracy: 0.8818\n",
      "37th iteration\n",
      "10000/10000 [==============================] - 0s 38us/sample - loss: 0.0181 - accuracy: 0.8868\n",
      "10000/10000 [==============================] - 0s 37us/sample - loss: 0.0183 - accuracy: 0.8871\n",
      "10000/10000 [==============================] - 0s 38us/sample - loss: 0.0187 - accuracy: 0.8869\n",
      "10000/10000 [==============================] - 0s 37us/sample - loss: 0.0190 - accuracy: 0.8858\n",
      "10000/10000 [==============================] - 0s 37us/sample - loss: 0.0195 - accuracy: 0.8844\n",
      "10000/10000 [==============================] - 0s 37us/sample - loss: 0.0200 - accuracy: 0.8831\n",
      "10000/10000 [==============================] - 0s 37us/sample - loss: 0.0205 - accuracy: 0.8806\n",
      "10000/10000 [==============================] - 0s 36us/sample - loss: 0.0212 - accuracy: 0.8793\n",
      "10000/10000 [==============================] - 0s 36us/sample - loss: 0.0219 - accuracy: 0.8772\n",
      "10000/10000 [==============================] - 0s 38us/sample - loss: 0.0226 - accuracy: 0.8736\n",
      "10000/10000 [==============================] - 0s 37us/sample - loss: 0.0235 - accuracy: 0.8713\n",
      "10000/10000 [==============================] - 0s 37us/sample - loss: 0.0244 - accuracy: 0.8679\n",
      "10000/10000 [==============================] - 0s 38us/sample - loss: 0.0254 - accuracy: 0.8634\n",
      "10000/10000 [==============================] - 0s 38us/sample - loss: 0.0265 - accuracy: 0.8574\n",
      "10000/10000 [==============================] - 0s 37us/sample - loss: 0.0276 - accuracy: 0.8529\n",
      "10000/10000 [==============================] - 0s 37us/sample - loss: 0.0288 - accuracy: 0.8477\n",
      "10000/10000 [==============================] - 0s 37us/sample - loss: 0.0300 - accuracy: 0.8393\n",
      "10000/10000 [==============================] - 0s 37us/sample - loss: 0.0313 - accuracy: 0.8331\n",
      "10000/10000 [==============================] - 0s 38us/sample - loss: 0.0327 - accuracy: 0.8241\n",
      "10000/10000 [==============================] - 0s 35us/sample - loss: 0.0341 - accuracy: 0.8121\n",
      "10000/10000 [==============================] - 0s 37us/sample - loss: 0.0355 - accuracy: 0.8016\n",
      "38th iteration\n",
      "10000/10000 [==============================] - 0s 36us/sample - loss: 0.0161 - accuracy: 0.8961\n",
      "10000/10000 [==============================] - 0s 37us/sample - loss: 0.0160 - accuracy: 0.8973\n",
      "10000/10000 [==============================] - 0s 38us/sample - loss: 0.0159 - accuracy: 0.8974\n",
      "10000/10000 [==============================] - 0s 37us/sample - loss: 0.0158 - accuracy: 0.8975\n",
      "10000/10000 [==============================] - 0s 38us/sample - loss: 0.0158 - accuracy: 0.8982\n",
      "10000/10000 [==============================] - 0s 37us/sample - loss: 0.0157 - accuracy: 0.8986\n",
      "10000/10000 [==============================] - 0s 37us/sample - loss: 0.0157 - accuracy: 0.8990\n",
      "10000/10000 [==============================] - 0s 37us/sample - loss: 0.0157 - accuracy: 0.8994\n",
      "10000/10000 [==============================] - 0s 36us/sample - loss: 0.0158 - accuracy: 0.9003\n",
      "10000/10000 [==============================] - 0s 37us/sample - loss: 0.0158 - accuracy: 0.9002\n",
      "10000/10000 [==============================] - 0s 35us/sample - loss: 0.0159 - accuracy: 0.9004\n",
      "10000/10000 [==============================] - 0s 38us/sample - loss: 0.0160 - accuracy: 0.9002\n",
      "10000/10000 [==============================] - 0s 36us/sample - loss: 0.0162 - accuracy: 0.8988\n",
      "10000/10000 [==============================] - 0s 38us/sample - loss: 0.0163 - accuracy: 0.8982\n",
      "10000/10000 [==============================] - 0s 37us/sample - loss: 0.0165 - accuracy: 0.8963\n",
      "10000/10000 [==============================] - 0s 38us/sample - loss: 0.0167 - accuracy: 0.8956\n",
      "10000/10000 [==============================] - 0s 38us/sample - loss: 0.0170 - accuracy: 0.8946\n",
      "10000/10000 [==============================] - 0s 39us/sample - loss: 0.0172 - accuracy: 0.8933\n",
      "10000/10000 [==============================] - 0s 37us/sample - loss: 0.0175 - accuracy: 0.8913\n",
      "10000/10000 [==============================] - 0s 37us/sample - loss: 0.0178 - accuracy: 0.8886\n",
      "10000/10000 [==============================] - 0s 38us/sample - loss: 0.0181 - accuracy: 0.8868\n",
      "39th iteration\n",
      "10000/10000 [==============================] - 0s 36us/sample - loss: 0.0161 - accuracy: 0.8961\n",
      "10000/10000 [==============================] - 0s 37us/sample - loss: 0.0161 - accuracy: 0.8962\n",
      "10000/10000 [==============================] - 0s 37us/sample - loss: 0.0160 - accuracy: 0.8974\n",
      "10000/10000 [==============================] - 0s 38us/sample - loss: 0.0160 - accuracy: 0.8978\n",
      "10000/10000 [==============================] - 0s 37us/sample - loss: 0.0160 - accuracy: 0.8980\n",
      "10000/10000 [==============================] - 0s 35us/sample - loss: 0.0160 - accuracy: 0.8979\n",
      "10000/10000 [==============================] - 0s 37us/sample - loss: 0.0160 - accuracy: 0.8977\n",
      "10000/10000 [==============================] - 0s 37us/sample - loss: 0.0161 - accuracy: 0.8982\n",
      "10000/10000 [==============================] - 0s 37us/sample - loss: 0.0162 - accuracy: 0.8978\n",
      "10000/10000 [==============================] - 0s 37us/sample - loss: 0.0163 - accuracy: 0.8978\n",
      "10000/10000 [==============================] - 0s 37us/sample - loss: 0.0164 - accuracy: 0.8970\n",
      "10000/10000 [==============================] - 0s 37us/sample - loss: 0.0166 - accuracy: 0.8978\n",
      "10000/10000 [==============================] - 0s 37us/sample - loss: 0.0167 - accuracy: 0.8972\n",
      "10000/10000 [==============================] - 0s 38us/sample - loss: 0.0169 - accuracy: 0.8967\n",
      "10000/10000 [==============================] - 0s 35us/sample - loss: 0.0172 - accuracy: 0.8949\n",
      "10000/10000 [==============================] - 0s 41us/sample - loss: 0.0174 - accuracy: 0.8931\n",
      "10000/10000 [==============================] - 0s 39us/sample - loss: 0.0177 - accuracy: 0.8920\n",
      "10000/10000 [==============================] - 0s 37us/sample - loss: 0.0180 - accuracy: 0.8909\n",
      "10000/10000 [==============================] - 0s 37us/sample - loss: 0.0183 - accuracy: 0.8883\n",
      "10000/10000 [==============================] - 0s 38us/sample - loss: 0.0186 - accuracy: 0.8866\n",
      "10000/10000 [==============================] - 0s 37us/sample - loss: 0.0190 - accuracy: 0.8848\n",
      "40th iteration\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000/10000 [==============================] - 0s 36us/sample - loss: 0.0167 - accuracy: 0.8959\n",
      "10000/10000 [==============================] - 0s 36us/sample - loss: 0.0168 - accuracy: 0.8958\n",
      "10000/10000 [==============================] - 0s 37us/sample - loss: 0.0169 - accuracy: 0.8965\n",
      "10000/10000 [==============================] - 0s 37us/sample - loss: 0.0171 - accuracy: 0.8952\n",
      "10000/10000 [==============================] - 0s 38us/sample - loss: 0.0174 - accuracy: 0.8947\n",
      "10000/10000 [==============================] - 0s 37us/sample - loss: 0.0176 - accuracy: 0.8943\n",
      "10000/10000 [==============================] - 0s 35us/sample - loss: 0.0179 - accuracy: 0.8941\n",
      "10000/10000 [==============================] - 0s 36us/sample - loss: 0.0182 - accuracy: 0.8937\n",
      "10000/10000 [==============================] - 0s 37us/sample - loss: 0.0186 - accuracy: 0.8936\n",
      "10000/10000 [==============================] - 0s 37us/sample - loss: 0.0190 - accuracy: 0.8922\n",
      "10000/10000 [==============================] - 0s 36us/sample - loss: 0.0195 - accuracy: 0.8917\n",
      "10000/10000 [==============================] - 0s 37us/sample - loss: 0.0200 - accuracy: 0.8902\n",
      "10000/10000 [==============================] - 0s 37us/sample - loss: 0.0205 - accuracy: 0.8871\n",
      "10000/10000 [==============================] - 0s 36us/sample - loss: 0.0211 - accuracy: 0.8847\n",
      "10000/10000 [==============================] - 0s 38us/sample - loss: 0.0217 - accuracy: 0.8825\n",
      "10000/10000 [==============================] - 0s 36us/sample - loss: 0.0224 - accuracy: 0.8800\n",
      "10000/10000 [==============================] - 0s 37us/sample - loss: 0.0231 - accuracy: 0.8775\n",
      "10000/10000 [==============================] - 0s 35us/sample - loss: 0.0238 - accuracy: 0.8737\n",
      "10000/10000 [==============================] - 0s 38us/sample - loss: 0.0246 - accuracy: 0.8691\n",
      "10000/10000 [==============================] - 0s 36us/sample - loss: 0.0254 - accuracy: 0.8654\n",
      "10000/10000 [==============================] - 0s 38us/sample - loss: 0.0263 - accuracy: 0.8606\n",
      "41th iteration\n",
      "10000/10000 [==============================] - 1s 59us/sample - loss: 0.0163 - accuracy: 0.8942\n",
      "10000/10000 [==============================] - 0s 37us/sample - loss: 0.0161 - accuracy: 0.8948\n",
      "10000/10000 [==============================] - 0s 36us/sample - loss: 0.0159 - accuracy: 0.8975\n",
      "10000/10000 [==============================] - 0s 38us/sample - loss: 0.0158 - accuracy: 0.8990\n",
      "10000/10000 [==============================] - 0s 37us/sample - loss: 0.0157 - accuracy: 0.8998\n",
      "10000/10000 [==============================] - 0s 36us/sample - loss: 0.0156 - accuracy: 0.9011\n",
      "10000/10000 [==============================] - 0s 38us/sample - loss: 0.0155 - accuracy: 0.9020\n",
      "10000/10000 [==============================] - 0s 35us/sample - loss: 0.0155 - accuracy: 0.9029\n",
      "10000/10000 [==============================] - 0s 37us/sample - loss: 0.0155 - accuracy: 0.9035\n",
      "10000/10000 [==============================] - 0s 36us/sample - loss: 0.0154 - accuracy: 0.9041\n",
      "10000/10000 [==============================] - 0s 36us/sample - loss: 0.0154 - accuracy: 0.9040\n",
      "10000/10000 [==============================] - 0s 36us/sample - loss: 0.0155 - accuracy: 0.9038\n",
      "10000/10000 [==============================] - 0s 38us/sample - loss: 0.0155 - accuracy: 0.9030\n",
      "10000/10000 [==============================] - 0s 37us/sample - loss: 0.0156 - accuracy: 0.9029\n",
      "10000/10000 [==============================] - 0s 37us/sample - loss: 0.0157 - accuracy: 0.9021\n",
      "10000/10000 [==============================] - 0s 40us/sample - loss: 0.0158 - accuracy: 0.9020\n",
      "10000/10000 [==============================] - 0s 37us/sample - loss: 0.0159 - accuracy: 0.9017\n",
      "10000/10000 [==============================] - 0s 36us/sample - loss: 0.0161 - accuracy: 0.8998\n",
      "10000/10000 [==============================] - 0s 37us/sample - loss: 0.0162 - accuracy: 0.8980\n",
      "10000/10000 [==============================] - 0s 36us/sample - loss: 0.0164 - accuracy: 0.8974\n",
      "10000/10000 [==============================] - 0s 38us/sample - loss: 0.0167 - accuracy: 0.8959\n",
      "42th iteration\n",
      "10000/10000 [==============================] - 0s 37us/sample - loss: 0.0161 - accuracy: 0.8961\n",
      "10000/10000 [==============================] - 0s 37us/sample - loss: 0.0161 - accuracy: 0.8957\n",
      "10000/10000 [==============================] - 0s 36us/sample - loss: 0.0161 - accuracy: 0.8965\n",
      "10000/10000 [==============================] - 1s 57us/sample - loss: 0.0161 - accuracy: 0.8966\n",
      "10000/10000 [==============================] - 0s 38us/sample - loss: 0.0162 - accuracy: 0.8971\n",
      "10000/10000 [==============================] - 0s 38us/sample - loss: 0.0163 - accuracy: 0.8969\n",
      "10000/10000 [==============================] - 0s 36us/sample - loss: 0.0164 - accuracy: 0.8980\n",
      "10000/10000 [==============================] - 0s 37us/sample - loss: 0.0165 - accuracy: 0.8979\n",
      "10000/10000 [==============================] - 0s 38us/sample - loss: 0.0166 - accuracy: 0.8977\n",
      "10000/10000 [==============================] - 0s 35us/sample - loss: 0.0168 - accuracy: 0.8980\n",
      "10000/10000 [==============================] - 0s 38us/sample - loss: 0.0170 - accuracy: 0.8978\n",
      "10000/10000 [==============================] - 0s 36us/sample - loss: 0.0172 - accuracy: 0.8969\n",
      "10000/10000 [==============================] - 0s 37us/sample - loss: 0.0175 - accuracy: 0.8959\n",
      "10000/10000 [==============================] - 0s 35us/sample - loss: 0.0178 - accuracy: 0.8950\n",
      "10000/10000 [==============================] - 0s 38us/sample - loss: 0.0181 - accuracy: 0.8939\n",
      "10000/10000 [==============================] - 0s 37us/sample - loss: 0.0184 - accuracy: 0.8924\n",
      "10000/10000 [==============================] - 0s 34us/sample - loss: 0.0188 - accuracy: 0.8899\n",
      "10000/10000 [==============================] - 0s 38us/sample - loss: 0.0192 - accuracy: 0.8879\n",
      "10000/10000 [==============================] - 0s 36us/sample - loss: 0.0196 - accuracy: 0.8850\n",
      "10000/10000 [==============================] - 0s 36us/sample - loss: 0.0201 - accuracy: 0.8835\n",
      "10000/10000 [==============================] - 0s 37us/sample - loss: 0.0206 - accuracy: 0.8813\n",
      "43th iteration\n",
      "10000/10000 [==============================] - 0s 36us/sample - loss: 0.0159 - accuracy: 0.8943\n",
      "10000/10000 [==============================] - 0s 37us/sample - loss: 0.0158 - accuracy: 0.8957\n",
      "10000/10000 [==============================] - 0s 36us/sample - loss: 0.0158 - accuracy: 0.8959\n",
      "10000/10000 [==============================] - 0s 38us/sample - loss: 0.0158 - accuracy: 0.8968\n",
      "10000/10000 [==============================] - 0s 37us/sample - loss: 0.0158 - accuracy: 0.8967\n",
      "10000/10000 [==============================] - 0s 36us/sample - loss: 0.0158 - accuracy: 0.8966\n",
      "10000/10000 [==============================] - 0s 36us/sample - loss: 0.0158 - accuracy: 0.8962\n",
      "10000/10000 [==============================] - 0s 37us/sample - loss: 0.0159 - accuracy: 0.8964\n",
      "10000/10000 [==============================] - 0s 37us/sample - loss: 0.0160 - accuracy: 0.8969\n",
      "10000/10000 [==============================] - 0s 36us/sample - loss: 0.0161 - accuracy: 0.8972\n",
      "10000/10000 [==============================] - 0s 38us/sample - loss: 0.0162 - accuracy: 0.8965\n",
      "10000/10000 [==============================] - 0s 36us/sample - loss: 0.0164 - accuracy: 0.8966\n",
      "10000/10000 [==============================] - 0s 36us/sample - loss: 0.0166 - accuracy: 0.8955\n",
      "10000/10000 [==============================] - 0s 35us/sample - loss: 0.0168 - accuracy: 0.8945\n",
      "10000/10000 [==============================] - 0s 36us/sample - loss: 0.0170 - accuracy: 0.8940\n",
      "10000/10000 [==============================] - 0s 38us/sample - loss: 0.0173 - accuracy: 0.8928\n",
      "10000/10000 [==============================] - 0s 36us/sample - loss: 0.0175 - accuracy: 0.8913\n",
      "10000/10000 [==============================] - 0s 37us/sample - loss: 0.0178 - accuracy: 0.8901\n",
      "10000/10000 [==============================] - 0s 38us/sample - loss: 0.0182 - accuracy: 0.8892\n",
      "10000/10000 [==============================] - 0s 38us/sample - loss: 0.0186 - accuracy: 0.8876\n",
      "10000/10000 [==============================] - 0s 37us/sample - loss: 0.0190 - accuracy: 0.8848\n",
      "44th iteration\n",
      "10000/10000 [==============================] - 0s 36us/sample - loss: 0.0159 - accuracy: 0.8943\n",
      "10000/10000 [==============================] - 0s 37us/sample - loss: 0.0157 - accuracy: 0.8966\n",
      "10000/10000 [==============================] - 0s 37us/sample - loss: 0.0156 - accuracy: 0.8972\n",
      "10000/10000 [==============================] - 0s 38us/sample - loss: 0.0155 - accuracy: 0.8988\n",
      "10000/10000 [==============================] - 0s 37us/sample - loss: 0.0155 - accuracy: 0.9000\n",
      "10000/10000 [==============================] - 0s 36us/sample - loss: 0.0154 - accuracy: 0.9005\n",
      "10000/10000 [==============================] - 0s 35us/sample - loss: 0.0154 - accuracy: 0.8995\n",
      "10000/10000 [==============================] - 0s 37us/sample - loss: 0.0154 - accuracy: 0.8998\n",
      "10000/10000 [==============================] - 0s 37us/sample - loss: 0.0154 - accuracy: 0.9000\n",
      "10000/10000 [==============================] - 0s 37us/sample - loss: 0.0155 - accuracy: 0.8993\n",
      "10000/10000 [==============================] - 0s 37us/sample - loss: 0.0156 - accuracy: 0.8989\n",
      "10000/10000 [==============================] - 0s 38us/sample - loss: 0.0157 - accuracy: 0.8980\n",
      "10000/10000 [==============================] - 0s 35us/sample - loss: 0.0158 - accuracy: 0.8982\n",
      "10000/10000 [==============================] - 0s 36us/sample - loss: 0.0160 - accuracy: 0.8974\n",
      "10000/10000 [==============================] - 0s 37us/sample - loss: 0.0161 - accuracy: 0.8952\n",
      "10000/10000 [==============================] - 0s 36us/sample - loss: 0.0164 - accuracy: 0.8937\n",
      "10000/10000 [==============================] - 0s 36us/sample - loss: 0.0166 - accuracy: 0.8925\n",
      "10000/10000 [==============================] - 0s 38us/sample - loss: 0.0169 - accuracy: 0.8898\n",
      "10000/10000 [==============================] - 0s 35us/sample - loss: 0.0172 - accuracy: 0.8880\n",
      "10000/10000 [==============================] - 0s 37us/sample - loss: 0.0175 - accuracy: 0.8860\n",
      "10000/10000 [==============================] - 0s 36us/sample - loss: 0.0179 - accuracy: 0.8823\n",
      "45th iteration\n",
      "10000/10000 [==============================] - 0s 37us/sample - loss: 0.0159 - accuracy: 0.8943\n",
      "10000/10000 [==============================] - 0s 35us/sample - loss: 0.0158 - accuracy: 0.8957\n",
      "10000/10000 [==============================] - 0s 36us/sample - loss: 0.0157 - accuracy: 0.8969\n",
      "10000/10000 [==============================] - 0s 37us/sample - loss: 0.0157 - accuracy: 0.8967\n",
      "10000/10000 [==============================] - 0s 38us/sample - loss: 0.0156 - accuracy: 0.8975\n",
      "10000/10000 [==============================] - 0s 37us/sample - loss: 0.0156 - accuracy: 0.8980\n",
      "10000/10000 [==============================] - 0s 38us/sample - loss: 0.0157 - accuracy: 0.8984\n",
      "10000/10000 [==============================] - 0s 38us/sample - loss: 0.0157 - accuracy: 0.8988\n",
      "10000/10000 [==============================] - 0s 35us/sample - loss: 0.0157 - accuracy: 0.9003\n",
      "10000/10000 [==============================] - 0s 38us/sample - loss: 0.0158 - accuracy: 0.8999\n",
      "10000/10000 [==============================] - 0s 38us/sample - loss: 0.0159 - accuracy: 0.8986\n",
      "10000/10000 [==============================] - 0s 36us/sample - loss: 0.0160 - accuracy: 0.8983\n",
      "10000/10000 [==============================] - 0s 37us/sample - loss: 0.0162 - accuracy: 0.8985\n",
      "10000/10000 [==============================] - 0s 37us/sample - loss: 0.0163 - accuracy: 0.8974\n",
      "10000/10000 [==============================] - 0s 39us/sample - loss: 0.0165 - accuracy: 0.8962\n",
      "10000/10000 [==============================] - 0s 35us/sample - loss: 0.0167 - accuracy: 0.8946\n",
      "10000/10000 [==============================] - 0s 36us/sample - loss: 0.0170 - accuracy: 0.8933\n",
      "10000/10000 [==============================] - 0s 37us/sample - loss: 0.0172 - accuracy: 0.8922\n",
      "10000/10000 [==============================] - 0s 36us/sample - loss: 0.0175 - accuracy: 0.8907\n",
      "10000/10000 [==============================] - 0s 37us/sample - loss: 0.0178 - accuracy: 0.8891\n",
      "10000/10000 [==============================] - 0s 36us/sample - loss: 0.0181 - accuracy: 0.8868\n",
      "46th iteration\n",
      "10000/10000 [==============================] - 0s 36us/sample - loss: 0.0163 - accuracy: 0.8942\n",
      "10000/10000 [==============================] - 0s 38us/sample - loss: 0.0163 - accuracy: 0.8937\n",
      "10000/10000 [==============================] - 0s 37us/sample - loss: 0.0163 - accuracy: 0.8937\n",
      "10000/10000 [==============================] - 0s 37us/sample - loss: 0.0163 - accuracy: 0.8948\n",
      "10000/10000 [==============================] - 0s 37us/sample - loss: 0.0164 - accuracy: 0.8959\n",
      "10000/10000 [==============================] - 0s 37us/sample - loss: 0.0165 - accuracy: 0.8966\n",
      "10000/10000 [==============================] - 0s 37us/sample - loss: 0.0167 - accuracy: 0.8964\n",
      "10000/10000 [==============================] - 0s 38us/sample - loss: 0.0169 - accuracy: 0.8954\n",
      "10000/10000 [==============================] - 0s 37us/sample - loss: 0.0171 - accuracy: 0.8947\n",
      "10000/10000 [==============================] - 0s 38us/sample - loss: 0.0173 - accuracy: 0.8941\n",
      "10000/10000 [==============================] - 0s 36us/sample - loss: 0.0176 - accuracy: 0.8935\n",
      "10000/10000 [==============================] - 0s 37us/sample - loss: 0.0179 - accuracy: 0.8927\n",
      "10000/10000 [==============================] - 0s 38us/sample - loss: 0.0183 - accuracy: 0.8906\n",
      "10000/10000 [==============================] - 0s 37us/sample - loss: 0.0187 - accuracy: 0.8891\n",
      "10000/10000 [==============================] - 0s 38us/sample - loss: 0.0191 - accuracy: 0.8881\n",
      "10000/10000 [==============================] - 0s 37us/sample - loss: 0.0195 - accuracy: 0.8868\n",
      "10000/10000 [==============================] - 0s 36us/sample - loss: 0.0200 - accuracy: 0.8847\n",
      "10000/10000 [==============================] - 0s 38us/sample - loss: 0.0206 - accuracy: 0.8808\n",
      "10000/10000 [==============================] - 0s 37us/sample - loss: 0.0212 - accuracy: 0.8775\n",
      "10000/10000 [==============================] - 0s 36us/sample - loss: 0.0218 - accuracy: 0.8744\n",
      "10000/10000 [==============================] - 0s 37us/sample - loss: 0.0224 - accuracy: 0.8711\n",
      "47th iteration\n",
      "10000/10000 [==============================] - 0s 38us/sample - loss: 0.0159 - accuracy: 0.8943\n",
      "10000/10000 [==============================] - 0s 37us/sample - loss: 0.0158 - accuracy: 0.8952\n",
      "10000/10000 [==============================] - 0s 37us/sample - loss: 0.0157 - accuracy: 0.8966\n",
      "10000/10000 [==============================] - 0s 38us/sample - loss: 0.0156 - accuracy: 0.8971\n",
      "10000/10000 [==============================] - 1s 61us/sample - loss: 0.0156 - accuracy: 0.8982\n",
      "10000/10000 [==============================] - 0s 37us/sample - loss: 0.0155 - accuracy: 0.8985\n",
      "10000/10000 [==============================] - 0s 38us/sample - loss: 0.0156 - accuracy: 0.8985\n",
      "10000/10000 [==============================] - 0s 37us/sample - loss: 0.0156 - accuracy: 0.8988\n",
      "10000/10000 [==============================] - 0s 37us/sample - loss: 0.0157 - accuracy: 0.8990\n",
      "10000/10000 [==============================] - 0s 37us/sample - loss: 0.0158 - accuracy: 0.8993\n",
      "10000/10000 [==============================] - 0s 37us/sample - loss: 0.0159 - accuracy: 0.8989\n",
      "10000/10000 [==============================] - 0s 38us/sample - loss: 0.0161 - accuracy: 0.8983\n",
      "10000/10000 [==============================] - 0s 37us/sample - loss: 0.0162 - accuracy: 0.8971\n",
      "10000/10000 [==============================] - 0s 37us/sample - loss: 0.0165 - accuracy: 0.8963\n",
      "10000/10000 [==============================] - 0s 37us/sample - loss: 0.0167 - accuracy: 0.8951\n",
      "10000/10000 [==============================] - 0s 37us/sample - loss: 0.0170 - accuracy: 0.8930\n",
      "10000/10000 [==============================] - 0s 38us/sample - loss: 0.0174 - accuracy: 0.8908\n",
      "10000/10000 [==============================] - 0s 36us/sample - loss: 0.0177 - accuracy: 0.8889\n",
      "10000/10000 [==============================] - 0s 36us/sample - loss: 0.0181 - accuracy: 0.8872\n",
      "10000/10000 [==============================] - 0s 36us/sample - loss: 0.0186 - accuracy: 0.8846\n",
      "10000/10000 [==============================] - 0s 38us/sample - loss: 0.0191 - accuracy: 0.8818\n",
      "48th iteration\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000/10000 [==============================] - 0s 38us/sample - loss: 0.0151 - accuracy: 0.9033\n",
      "10000/10000 [==============================] - 0s 36us/sample - loss: 0.0151 - accuracy: 0.9031\n",
      "10000/10000 [==============================] - 0s 37us/sample - loss: 0.0151 - accuracy: 0.9038\n",
      "10000/10000 [==============================] - 0s 36us/sample - loss: 0.0151 - accuracy: 0.9050\n",
      "10000/10000 [==============================] - 0s 35us/sample - loss: 0.0151 - accuracy: 0.9046\n",
      "10000/10000 [==============================] - 0s 38us/sample - loss: 0.0151 - accuracy: 0.9049\n",
      "10000/10000 [==============================] - 0s 37us/sample - loss: 0.0152 - accuracy: 0.9048\n",
      "10000/10000 [==============================] - 1s 59us/sample - loss: 0.0153 - accuracy: 0.9049\n",
      "10000/10000 [==============================] - 0s 36us/sample - loss: 0.0154 - accuracy: 0.9047\n",
      "10000/10000 [==============================] - 0s 37us/sample - loss: 0.0156 - accuracy: 0.9037\n",
      "10000/10000 [==============================] - 0s 37us/sample - loss: 0.0158 - accuracy: 0.9026\n",
      "10000/10000 [==============================] - 0s 37us/sample - loss: 0.0160 - accuracy: 0.9027\n",
      "10000/10000 [==============================] - 0s 35us/sample - loss: 0.0162 - accuracy: 0.9019\n",
      "10000/10000 [==============================] - 0s 35us/sample - loss: 0.0165 - accuracy: 0.9008\n",
      "10000/10000 [==============================] - 0s 37us/sample - loss: 0.0168 - accuracy: 0.8995\n",
      "10000/10000 [==============================] - 0s 35us/sample - loss: 0.0171 - accuracy: 0.8973\n",
      "10000/10000 [==============================] - 0s 40us/sample - loss: 0.0174 - accuracy: 0.8951\n",
      "10000/10000 [==============================] - 0s 37us/sample - loss: 0.0178 - accuracy: 0.8921\n",
      "10000/10000 [==============================] - 0s 37us/sample - loss: 0.0182 - accuracy: 0.8887\n",
      "10000/10000 [==============================] - 0s 35us/sample - loss: 0.0186 - accuracy: 0.8851\n",
      "10000/10000 [==============================] - 0s 35us/sample - loss: 0.0191 - accuracy: 0.8818\n",
      "49th iteration\n",
      "10000/10000 [==============================] - 0s 38us/sample - loss: 0.0161 - accuracy: 0.8961\n",
      "10000/10000 [==============================] - 0s 37us/sample - loss: 0.0160 - accuracy: 0.8973\n",
      "10000/10000 [==============================] - 0s 37us/sample - loss: 0.0158 - accuracy: 0.8986\n",
      "10000/10000 [==============================] - 0s 37us/sample - loss: 0.0157 - accuracy: 0.8990\n",
      "10000/10000 [==============================] - 0s 37us/sample - loss: 0.0156 - accuracy: 0.8998\n",
      "10000/10000 [==============================] - 0s 36us/sample - loss: 0.0156 - accuracy: 0.9011\n",
      "10000/10000 [==============================] - 0s 35us/sample - loss: 0.0155 - accuracy: 0.9023\n",
      "10000/10000 [==============================] - 0s 36us/sample - loss: 0.0155 - accuracy: 0.9032\n",
      "10000/10000 [==============================] - 0s 36us/sample - loss: 0.0156 - accuracy: 0.9031\n",
      "10000/10000 [==============================] - 0s 38us/sample - loss: 0.0156 - accuracy: 0.9026\n",
      "10000/10000 [==============================] - 0s 36us/sample - loss: 0.0157 - accuracy: 0.9009\n",
      "10000/10000 [==============================] - 0s 36us/sample - loss: 0.0158 - accuracy: 0.8997\n",
      "10000/10000 [==============================] - 0s 36us/sample - loss: 0.0159 - accuracy: 0.8996\n",
      "10000/10000 [==============================] - 0s 36us/sample - loss: 0.0161 - accuracy: 0.8984\n",
      "10000/10000 [==============================] - 0s 38us/sample - loss: 0.0163 - accuracy: 0.8966\n",
      "10000/10000 [==============================] - 0s 35us/sample - loss: 0.0165 - accuracy: 0.8951\n",
      "10000/10000 [==============================] - 0s 36us/sample - loss: 0.0167 - accuracy: 0.8935\n",
      "10000/10000 [==============================] - 0s 37us/sample - loss: 0.0169 - accuracy: 0.8912\n",
      "10000/10000 [==============================] - 0s 36us/sample - loss: 0.0172 - accuracy: 0.8889\n",
      "10000/10000 [==============================] - 0s 38us/sample - loss: 0.0175 - accuracy: 0.8851\n",
      "10000/10000 [==============================] - 0s 38us/sample - loss: 0.0179 - accuracy: 0.8823\n",
      "50th iteration\n",
      "10000/10000 [==============================] - 0s 38us/sample - loss: 0.0161 - accuracy: 0.8961\n",
      "10000/10000 [==============================] - 0s 36us/sample - loss: 0.0160 - accuracy: 0.8970\n",
      "10000/10000 [==============================] - 0s 36us/sample - loss: 0.0159 - accuracy: 0.8982\n",
      "10000/10000 [==============================] - 0s 37us/sample - loss: 0.0159 - accuracy: 0.8998\n",
      "10000/10000 [==============================] - 0s 37us/sample - loss: 0.0158 - accuracy: 0.8998\n",
      "10000/10000 [==============================] - 0s 37us/sample - loss: 0.0158 - accuracy: 0.9012\n",
      "10000/10000 [==============================] - 0s 37us/sample - loss: 0.0158 - accuracy: 0.9013\n",
      "10000/10000 [==============================] - 0s 37us/sample - loss: 0.0158 - accuracy: 0.9015\n",
      "10000/10000 [==============================] - 0s 36us/sample - loss: 0.0159 - accuracy: 0.9022\n",
      "10000/10000 [==============================] - 0s 38us/sample - loss: 0.0160 - accuracy: 0.9031\n",
      "10000/10000 [==============================] - 0s 38us/sample - loss: 0.0161 - accuracy: 0.9029\n",
      "10000/10000 [==============================] - 0s 39us/sample - loss: 0.0163 - accuracy: 0.9024\n",
      "10000/10000 [==============================] - 0s 38us/sample - loss: 0.0165 - accuracy: 0.9000\n",
      "10000/10000 [==============================] - 0s 35us/sample - loss: 0.0167 - accuracy: 0.8998\n",
      "10000/10000 [==============================] - 0s 36us/sample - loss: 0.0169 - accuracy: 0.8977\n",
      "10000/10000 [==============================] - 0s 37us/sample - loss: 0.0172 - accuracy: 0.8952\n",
      "10000/10000 [==============================] - 0s 37us/sample - loss: 0.0175 - accuracy: 0.8927\n",
      "10000/10000 [==============================] - 0s 37us/sample - loss: 0.0179 - accuracy: 0.8907\n",
      "10000/10000 [==============================] - 0s 36us/sample - loss: 0.0182 - accuracy: 0.8880\n",
      "10000/10000 [==============================] - 0s 36us/sample - loss: 0.0186 - accuracy: 0.8848\n",
      "10000/10000 [==============================] - 0s 36us/sample - loss: 0.0191 - accuracy: 0.8818\n",
      "51th iteration\n",
      "10000/10000 [==============================] - 0s 37us/sample - loss: 0.0161 - accuracy: 0.8961\n",
      "10000/10000 [==============================] - 0s 37us/sample - loss: 0.0162 - accuracy: 0.8955\n",
      "10000/10000 [==============================] - 0s 38us/sample - loss: 0.0162 - accuracy: 0.8955\n",
      "10000/10000 [==============================] - 0s 37us/sample - loss: 0.0163 - accuracy: 0.8953\n",
      "10000/10000 [==============================] - 0s 37us/sample - loss: 0.0164 - accuracy: 0.8959\n",
      "10000/10000 [==============================] - 0s 37us/sample - loss: 0.0165 - accuracy: 0.8962\n",
      "10000/10000 [==============================] - 0s 39us/sample - loss: 0.0167 - accuracy: 0.8963\n",
      "10000/10000 [==============================] - 0s 37us/sample - loss: 0.0169 - accuracy: 0.8967\n",
      "10000/10000 [==============================] - 0s 37us/sample - loss: 0.0171 - accuracy: 0.8964\n",
      "10000/10000 [==============================] - 0s 38us/sample - loss: 0.0174 - accuracy: 0.8961\n",
      "10000/10000 [==============================] - 0s 37us/sample - loss: 0.0177 - accuracy: 0.8959\n",
      "10000/10000 [==============================] - 0s 39us/sample - loss: 0.0180 - accuracy: 0.8945\n",
      "10000/10000 [==============================] - 0s 37us/sample - loss: 0.0183 - accuracy: 0.8923\n",
      "10000/10000 [==============================] - 0s 37us/sample - loss: 0.0187 - accuracy: 0.8907\n",
      "10000/10000 [==============================] - 0s 36us/sample - loss: 0.0192 - accuracy: 0.8886\n",
      "10000/10000 [==============================] - 0s 38us/sample - loss: 0.0196 - accuracy: 0.8849\n",
      "10000/10000 [==============================] - 0s 39us/sample - loss: 0.0201 - accuracy: 0.8825\n",
      "10000/10000 [==============================] - 0s 37us/sample - loss: 0.0206 - accuracy: 0.8813\n",
      "10000/10000 [==============================] - 0s 37us/sample - loss: 0.0212 - accuracy: 0.8784\n",
      "10000/10000 [==============================] - 0s 38us/sample - loss: 0.0218 - accuracy: 0.8746\n",
      "10000/10000 [==============================] - 0s 37us/sample - loss: 0.0224 - accuracy: 0.8711\n",
      "52th iteration\n",
      "10000/10000 [==============================] - 0s 37us/sample - loss: 0.0151 - accuracy: 0.9033\n",
      "10000/10000 [==============================] - 0s 38us/sample - loss: 0.0151 - accuracy: 0.9029\n",
      "10000/10000 [==============================] - 0s 36us/sample - loss: 0.0151 - accuracy: 0.9031\n",
      "10000/10000 [==============================] - 0s 37us/sample - loss: 0.0151 - accuracy: 0.9037\n",
      "10000/10000 [==============================] - 0s 36us/sample - loss: 0.0152 - accuracy: 0.9036\n",
      "10000/10000 [==============================] - 0s 40us/sample - loss: 0.0153 - accuracy: 0.9038\n",
      "10000/10000 [==============================] - 0s 42us/sample - loss: 0.0154 - accuracy: 0.9037\n",
      "10000/10000 [==============================] - 0s 37us/sample - loss: 0.0155 - accuracy: 0.9029\n",
      "10000/10000 [==============================] - 0s 37us/sample - loss: 0.0156 - accuracy: 0.9025\n",
      "10000/10000 [==============================] - 0s 37us/sample - loss: 0.0157 - accuracy: 0.9013\n",
      "10000/10000 [==============================] - 0s 37us/sample - loss: 0.0159 - accuracy: 0.9017\n",
      "10000/10000 [==============================] - 0s 38us/sample - loss: 0.0161 - accuracy: 0.9007\n",
      "10000/10000 [==============================] - 0s 39us/sample - loss: 0.0163 - accuracy: 0.8990\n",
      "10000/10000 [==============================] - 0s 37us/sample - loss: 0.0166 - accuracy: 0.8974\n",
      "10000/10000 [==============================] - 0s 37us/sample - loss: 0.0168 - accuracy: 0.8962\n",
      "10000/10000 [==============================] - 0s 38us/sample - loss: 0.0171 - accuracy: 0.8944\n",
      "10000/10000 [==============================] - 0s 37us/sample - loss: 0.0175 - accuracy: 0.8923\n",
      "10000/10000 [==============================] - 0s 38us/sample - loss: 0.0178 - accuracy: 0.8914\n",
      "10000/10000 [==============================] - 0s 35us/sample - loss: 0.0182 - accuracy: 0.8905\n",
      "10000/10000 [==============================] - 0s 37us/sample - loss: 0.0185 - accuracy: 0.8879\n",
      "10000/10000 [==============================] - 0s 38us/sample - loss: 0.0190 - accuracy: 0.8848\n",
      "53th iteration\n",
      "10000/10000 [==============================] - 0s 37us/sample - loss: 0.0159 - accuracy: 0.8943\n",
      "10000/10000 [==============================] - 0s 36us/sample - loss: 0.0158 - accuracy: 0.8952\n",
      "10000/10000 [==============================] - 0s 38us/sample - loss: 0.0158 - accuracy: 0.8959\n",
      "10000/10000 [==============================] - 0s 41us/sample - loss: 0.0158 - accuracy: 0.8961\n",
      "10000/10000 [==============================] - 0s 36us/sample - loss: 0.0158 - accuracy: 0.8965\n",
      "10000/10000 [==============================] - 0s 38us/sample - loss: 0.0159 - accuracy: 0.8970\n",
      "10000/10000 [==============================] - 0s 37us/sample - loss: 0.0160 - accuracy: 0.8979\n",
      "10000/10000 [==============================] - 0s 38us/sample - loss: 0.0161 - accuracy: 0.8982\n",
      "10000/10000 [==============================] - 1s 58us/sample - loss: 0.0162 - accuracy: 0.8984\n",
      "10000/10000 [==============================] - 0s 37us/sample - loss: 0.0164 - accuracy: 0.8981\n",
      "10000/10000 [==============================] - 0s 37us/sample - loss: 0.0166 - accuracy: 0.8982\n",
      "10000/10000 [==============================] - 0s 36us/sample - loss: 0.0168 - accuracy: 0.8980\n",
      "10000/10000 [==============================] - 0s 39us/sample - loss: 0.0171 - accuracy: 0.8973\n",
      "10000/10000 [==============================] - 0s 37us/sample - loss: 0.0174 - accuracy: 0.8962\n",
      "10000/10000 [==============================] - 0s 40us/sample - loss: 0.0177 - accuracy: 0.8947\n",
      "10000/10000 [==============================] - 0s 37us/sample - loss: 0.0181 - accuracy: 0.8934\n",
      "10000/10000 [==============================] - 0s 37us/sample - loss: 0.0185 - accuracy: 0.8911\n",
      "10000/10000 [==============================] - 0s 36us/sample - loss: 0.0190 - accuracy: 0.8888\n",
      "10000/10000 [==============================] - 0s 37us/sample - loss: 0.0195 - accuracy: 0.8872\n",
      "10000/10000 [==============================] - 0s 37us/sample - loss: 0.0200 - accuracy: 0.8841\n",
      "10000/10000 [==============================] - 0s 36us/sample - loss: 0.0206 - accuracy: 0.8813\n",
      "54th iteration\n",
      "10000/10000 [==============================] - 0s 38us/sample - loss: 0.0159 - accuracy: 0.8943\n",
      "10000/10000 [==============================] - 0s 37us/sample - loss: 0.0157 - accuracy: 0.8961\n",
      "10000/10000 [==============================] - 0s 39us/sample - loss: 0.0155 - accuracy: 0.8977\n",
      "10000/10000 [==============================] - 0s 37us/sample - loss: 0.0154 - accuracy: 0.8984\n",
      "10000/10000 [==============================] - 0s 38us/sample - loss: 0.0153 - accuracy: 0.8997\n",
      "10000/10000 [==============================] - 0s 36us/sample - loss: 0.0152 - accuracy: 0.9019\n",
      "10000/10000 [==============================] - 0s 35us/sample - loss: 0.0151 - accuracy: 0.9034\n",
      "10000/10000 [==============================] - 0s 37us/sample - loss: 0.0150 - accuracy: 0.9039\n",
      "10000/10000 [==============================] - 0s 38us/sample - loss: 0.0150 - accuracy: 0.9049\n",
      "10000/10000 [==============================] - 0s 37us/sample - loss: 0.0150 - accuracy: 0.9043\n",
      "10000/10000 [==============================] - 0s 40us/sample - loss: 0.0150 - accuracy: 0.9052\n",
      "10000/10000 [==============================] - 1s 59us/sample - loss: 0.0151 - accuracy: 0.9055\n",
      "10000/10000 [==============================] - 0s 37us/sample - loss: 0.0151 - accuracy: 0.9054\n",
      "10000/10000 [==============================] - 0s 37us/sample - loss: 0.0152 - accuracy: 0.9057\n",
      "10000/10000 [==============================] - 0s 41us/sample - loss: 0.0153 - accuracy: 0.9041\n",
      "10000/10000 [==============================] - 0s 36us/sample - loss: 0.0155 - accuracy: 0.9031\n",
      "10000/10000 [==============================] - 0s 38us/sample - loss: 0.0157 - accuracy: 0.9020\n",
      "10000/10000 [==============================] - 0s 38us/sample - loss: 0.0159 - accuracy: 0.9003\n",
      "10000/10000 [==============================] - 0s 39us/sample - loss: 0.0161 - accuracy: 0.8987\n",
      "10000/10000 [==============================] - 0s 36us/sample - loss: 0.0164 - accuracy: 0.8974\n",
      "10000/10000 [==============================] - 0s 38us/sample - loss: 0.0167 - accuracy: 0.8959\n",
      "55th iteration\n",
      "10000/10000 [==============================] - 0s 39us/sample - loss: 0.0151 - accuracy: 0.9033\n",
      "10000/10000 [==============================] - 0s 38us/sample - loss: 0.0150 - accuracy: 0.9034\n",
      "10000/10000 [==============================] - 0s 38us/sample - loss: 0.0150 - accuracy: 0.9047\n",
      "10000/10000 [==============================] - 0s 36us/sample - loss: 0.0150 - accuracy: 0.9055\n",
      "10000/10000 [==============================] - 0s 38us/sample - loss: 0.0150 - accuracy: 0.9059\n",
      "10000/10000 [==============================] - 0s 37us/sample - loss: 0.0150 - accuracy: 0.9054\n",
      "10000/10000 [==============================] - 0s 37us/sample - loss: 0.0150 - accuracy: 0.9050\n",
      "10000/10000 [==============================] - 0s 37us/sample - loss: 0.0151 - accuracy: 0.9052\n",
      "10000/10000 [==============================] - 0s 37us/sample - loss: 0.0151 - accuracy: 0.9056\n",
      "10000/10000 [==============================] - 0s 36us/sample - loss: 0.0152 - accuracy: 0.9055\n",
      "10000/10000 [==============================] - 0s 37us/sample - loss: 0.0154 - accuracy: 0.9049\n",
      "10000/10000 [==============================] - 0s 38us/sample - loss: 0.0155 - accuracy: 0.9039\n",
      "10000/10000 [==============================] - 0s 35us/sample - loss: 0.0157 - accuracy: 0.9029\n",
      "10000/10000 [==============================] - 0s 37us/sample - loss: 0.0159 - accuracy: 0.9013\n",
      "10000/10000 [==============================] - 0s 38us/sample - loss: 0.0162 - accuracy: 0.9008\n",
      "10000/10000 [==============================] - 0s 37us/sample - loss: 0.0164 - accuracy: 0.8985\n",
      "10000/10000 [==============================] - 0s 39us/sample - loss: 0.0167 - accuracy: 0.8962\n",
      "10000/10000 [==============================] - 0s 37us/sample - loss: 0.0170 - accuracy: 0.8934\n",
      "10000/10000 [==============================] - 0s 37us/sample - loss: 0.0173 - accuracy: 0.8913\n",
      "10000/10000 [==============================] - 0s 35us/sample - loss: 0.0177 - accuracy: 0.8896\n",
      "10000/10000 [==============================] - 0s 37us/sample - loss: 0.0181 - accuracy: 0.8868\n",
      "56th iteration\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000/10000 [==============================] - 0s 37us/sample - loss: 0.0159 - accuracy: 0.8943\n",
      "10000/10000 [==============================] - 0s 37us/sample - loss: 0.0157 - accuracy: 0.8963\n",
      "10000/10000 [==============================] - 0s 38us/sample - loss: 0.0155 - accuracy: 0.8984\n",
      "10000/10000 [==============================] - 0s 37us/sample - loss: 0.0153 - accuracy: 0.8992\n",
      "10000/10000 [==============================] - 0s 38us/sample - loss: 0.0152 - accuracy: 0.9007\n",
      "10000/10000 [==============================] - 0s 37us/sample - loss: 0.0151 - accuracy: 0.9022\n",
      "10000/10000 [==============================] - 0s 36us/sample - loss: 0.0150 - accuracy: 0.9030\n",
      "10000/10000 [==============================] - 0s 36us/sample - loss: 0.0149 - accuracy: 0.9043\n",
      "10000/10000 [==============================] - 0s 37us/sample - loss: 0.0148 - accuracy: 0.9052\n",
      "10000/10000 [==============================] - 0s 36us/sample - loss: 0.0148 - accuracy: 0.9061\n",
      "10000/10000 [==============================] - 0s 38us/sample - loss: 0.0148 - accuracy: 0.9063\n",
      "10000/10000 [==============================] - 0s 38us/sample - loss: 0.0148 - accuracy: 0.9063\n",
      "10000/10000 [==============================] - 0s 36us/sample - loss: 0.0149 - accuracy: 0.9061\n",
      "10000/10000 [==============================] - 0s 36us/sample - loss: 0.0150 - accuracy: 0.9050\n",
      "10000/10000 [==============================] - 0s 37us/sample - loss: 0.0151 - accuracy: 0.9039\n",
      "10000/10000 [==============================] - 0s 38us/sample - loss: 0.0152 - accuracy: 0.9032\n",
      "10000/10000 [==============================] - 0s 37us/sample - loss: 0.0153 - accuracy: 0.9013\n",
      "10000/10000 [==============================] - 0s 39us/sample - loss: 0.0155 - accuracy: 0.8993\n",
      "10000/10000 [==============================] - 0s 37us/sample - loss: 0.0157 - accuracy: 0.8976\n",
      "10000/10000 [==============================] - 0s 39us/sample - loss: 0.0160 - accuracy: 0.8960\n",
      "10000/10000 [==============================] - 0s 37us/sample - loss: 0.0163 - accuracy: 0.8942\n",
      "57th iteration\n",
      "10000/10000 [==============================] - 0s 37us/sample - loss: 0.0151 - accuracy: 0.9033\n",
      "10000/10000 [==============================] - 0s 37us/sample - loss: 0.0150 - accuracy: 0.9036\n",
      "10000/10000 [==============================] - 0s 38us/sample - loss: 0.0150 - accuracy: 0.9043\n",
      "10000/10000 [==============================] - 0s 37us/sample - loss: 0.0149 - accuracy: 0.9049\n",
      "10000/10000 [==============================] - 0s 38us/sample - loss: 0.0149 - accuracy: 0.9052\n",
      "10000/10000 [==============================] - 0s 35us/sample - loss: 0.0149 - accuracy: 0.9057\n",
      "10000/10000 [==============================] - 0s 36us/sample - loss: 0.0150 - accuracy: 0.9058\n",
      "10000/10000 [==============================] - 0s 36us/sample - loss: 0.0150 - accuracy: 0.9052\n",
      "10000/10000 [==============================] - 0s 38us/sample - loss: 0.0151 - accuracy: 0.9042\n",
      "10000/10000 [==============================] - 0s 36us/sample - loss: 0.0152 - accuracy: 0.9035\n",
      "10000/10000 [==============================] - 0s 36us/sample - loss: 0.0153 - accuracy: 0.9024\n",
      "10000/10000 [==============================] - 0s 39us/sample - loss: 0.0154 - accuracy: 0.9014\n",
      "10000/10000 [==============================] - 0s 37us/sample - loss: 0.0156 - accuracy: 0.9008\n",
      "10000/10000 [==============================] - 0s 37us/sample - loss: 0.0158 - accuracy: 0.8989\n",
      "10000/10000 [==============================] - 0s 36us/sample - loss: 0.0160 - accuracy: 0.8979\n",
      "10000/10000 [==============================] - 0s 37us/sample - loss: 0.0163 - accuracy: 0.8955\n",
      "10000/10000 [==============================] - 0s 36us/sample - loss: 0.0166 - accuracy: 0.8942\n",
      "10000/10000 [==============================] - 0s 37us/sample - loss: 0.0169 - accuracy: 0.8914\n",
      "10000/10000 [==============================] - 0s 38us/sample - loss: 0.0172 - accuracy: 0.8890\n",
      "10000/10000 [==============================] - 0s 36us/sample - loss: 0.0175 - accuracy: 0.8854\n",
      "10000/10000 [==============================] - 0s 36us/sample - loss: 0.0179 - accuracy: 0.8823\n",
      "58th iteration\n",
      "10000/10000 [==============================] - 0s 37us/sample - loss: 0.0168 - accuracy: 0.8870\n",
      "10000/10000 [==============================] - 0s 37us/sample - loss: 0.0166 - accuracy: 0.8889\n",
      "10000/10000 [==============================] - 0s 35us/sample - loss: 0.0165 - accuracy: 0.8899\n",
      "10000/10000 [==============================] - 0s 37us/sample - loss: 0.0163 - accuracy: 0.8906\n",
      "10000/10000 [==============================] - 0s 35us/sample - loss: 0.0162 - accuracy: 0.8916\n",
      "10000/10000 [==============================] - 0s 35us/sample - loss: 0.0162 - accuracy: 0.8924\n",
      "10000/10000 [==============================] - 0s 37us/sample - loss: 0.0161 - accuracy: 0.8932\n",
      "10000/10000 [==============================] - 0s 36us/sample - loss: 0.0161 - accuracy: 0.8939\n",
      "10000/10000 [==============================] - 0s 37us/sample - loss: 0.0161 - accuracy: 0.8945\n",
      "10000/10000 [==============================] - 0s 37us/sample - loss: 0.0161 - accuracy: 0.8940\n",
      "10000/10000 [==============================] - 0s 37us/sample - loss: 0.0161 - accuracy: 0.8940\n",
      "10000/10000 [==============================] - 0s 36us/sample - loss: 0.0162 - accuracy: 0.8950\n",
      "10000/10000 [==============================] - 0s 36us/sample - loss: 0.0163 - accuracy: 0.8941\n",
      "10000/10000 [==============================] - 0s 37us/sample - loss: 0.0164 - accuracy: 0.8931\n",
      "10000/10000 [==============================] - 0s 38us/sample - loss: 0.0165 - accuracy: 0.8924\n",
      "10000/10000 [==============================] - 0s 38us/sample - loss: 0.0167 - accuracy: 0.8919\n",
      "10000/10000 [==============================] - 0s 35us/sample - loss: 0.0169 - accuracy: 0.8915\n",
      "10000/10000 [==============================] - 0s 37us/sample - loss: 0.0171 - accuracy: 0.8897\n",
      "10000/10000 [==============================] - 0s 38us/sample - loss: 0.0173 - accuracy: 0.8871\n",
      "10000/10000 [==============================] - 0s 39us/sample - loss: 0.0176 - accuracy: 0.8849\n",
      "10000/10000 [==============================] - 0s 37us/sample - loss: 0.0179 - accuracy: 0.8823\n",
      "59th iteration\n",
      "10000/10000 [==============================] - 0s 38us/sample - loss: 0.0161 - accuracy: 0.8961\n",
      "10000/10000 [==============================] - 0s 39us/sample - loss: 0.0159 - accuracy: 0.8971\n",
      "10000/10000 [==============================] - 0s 36us/sample - loss: 0.0158 - accuracy: 0.8987\n",
      "10000/10000 [==============================] - 0s 38us/sample - loss: 0.0156 - accuracy: 0.8998\n",
      "10000/10000 [==============================] - 0s 36us/sample - loss: 0.0155 - accuracy: 0.9012\n",
      "10000/10000 [==============================] - 0s 37us/sample - loss: 0.0154 - accuracy: 0.9031\n",
      "10000/10000 [==============================] - 0s 39us/sample - loss: 0.0153 - accuracy: 0.9045\n",
      "10000/10000 [==============================] - 0s 37us/sample - loss: 0.0152 - accuracy: 0.9051\n",
      "10000/10000 [==============================] - 0s 36us/sample - loss: 0.0152 - accuracy: 0.9057\n",
      "10000/10000 [==============================] - 0s 37us/sample - loss: 0.0152 - accuracy: 0.9054\n",
      "10000/10000 [==============================] - 0s 36us/sample - loss: 0.0152 - accuracy: 0.9055\n",
      "10000/10000 [==============================] - 0s 37us/sample - loss: 0.0152 - accuracy: 0.9051\n",
      "10000/10000 [==============================] - 1s 59us/sample - loss: 0.0153 - accuracy: 0.9039\n",
      "10000/10000 [==============================] - 0s 38us/sample - loss: 0.0154 - accuracy: 0.9038\n",
      "10000/10000 [==============================] - 0s 37us/sample - loss: 0.0155 - accuracy: 0.9036\n",
      "10000/10000 [==============================] - 0s 37us/sample - loss: 0.0156 - accuracy: 0.9023\n",
      "10000/10000 [==============================] - 0s 37us/sample - loss: 0.0158 - accuracy: 0.9017\n",
      "10000/10000 [==============================] - 0s 37us/sample - loss: 0.0160 - accuracy: 0.9002\n",
      "10000/10000 [==============================] - 0s 36us/sample - loss: 0.0162 - accuracy: 0.8981\n",
      "10000/10000 [==============================] - 0s 38us/sample - loss: 0.0164 - accuracy: 0.8972\n",
      "10000/10000 [==============================] - 0s 36us/sample - loss: 0.0167 - accuracy: 0.8959\n",
      "60th iteration\n",
      "10000/10000 [==============================] - 0s 37us/sample - loss: 0.0151 - accuracy: 0.9033\n",
      "10000/10000 [==============================] - 0s 38us/sample - loss: 0.0152 - accuracy: 0.9029\n",
      "10000/10000 [==============================] - 0s 39us/sample - loss: 0.0152 - accuracy: 0.9027\n",
      "10000/10000 [==============================] - 0s 37us/sample - loss: 0.0153 - accuracy: 0.9029\n",
      "10000/10000 [==============================] - 0s 35us/sample - loss: 0.0154 - accuracy: 0.9029\n",
      "10000/10000 [==============================] - 0s 38us/sample - loss: 0.0155 - accuracy: 0.9035\n",
      "10000/10000 [==============================] - 0s 36us/sample - loss: 0.0156 - accuracy: 0.9026\n",
      "10000/10000 [==============================] - 0s 35us/sample - loss: 0.0158 - accuracy: 0.9023\n",
      "10000/10000 [==============================] - 0s 37us/sample - loss: 0.0160 - accuracy: 0.9021\n",
      "10000/10000 [==============================] - 0s 36us/sample - loss: 0.0162 - accuracy: 0.9024\n",
      "10000/10000 [==============================] - 0s 38us/sample - loss: 0.0165 - accuracy: 0.9025\n",
      "10000/10000 [==============================] - 0s 38us/sample - loss: 0.0167 - accuracy: 0.9008\n",
      "10000/10000 [==============================] - 0s 38us/sample - loss: 0.0170 - accuracy: 0.9002\n",
      "10000/10000 [==============================] - 0s 37us/sample - loss: 0.0174 - accuracy: 0.8981\n",
      "10000/10000 [==============================] - 0s 37us/sample - loss: 0.0177 - accuracy: 0.8963\n",
      "10000/10000 [==============================] - 1s 59us/sample - loss: 0.0181 - accuracy: 0.8947\n",
      "10000/10000 [==============================] - 0s 36us/sample - loss: 0.0186 - accuracy: 0.8924\n",
      "10000/10000 [==============================] - 0s 37us/sample - loss: 0.0190 - accuracy: 0.8896\n",
      "10000/10000 [==============================] - 0s 38us/sample - loss: 0.0195 - accuracy: 0.8869\n",
      "10000/10000 [==============================] - 0s 38us/sample - loss: 0.0200 - accuracy: 0.8840\n",
      "10000/10000 [==============================] - 0s 35us/sample - loss: 0.0206 - accuracy: 0.8813\n",
      "61th iteration\n",
      "10000/10000 [==============================] - 0s 39us/sample - loss: 0.0168 - accuracy: 0.8870\n",
      "10000/10000 [==============================] - 0s 37us/sample - loss: 0.0166 - accuracy: 0.8876\n",
      "10000/10000 [==============================] - 0s 38us/sample - loss: 0.0165 - accuracy: 0.8900\n",
      "10000/10000 [==============================] - 0s 38us/sample - loss: 0.0164 - accuracy: 0.8916\n",
      "10000/10000 [==============================] - 0s 37us/sample - loss: 0.0164 - accuracy: 0.8924\n",
      "10000/10000 [==============================] - 0s 37us/sample - loss: 0.0163 - accuracy: 0.8940\n",
      "10000/10000 [==============================] - 0s 36us/sample - loss: 0.0163 - accuracy: 0.8950\n",
      "10000/10000 [==============================] - 0s 37us/sample - loss: 0.0163 - accuracy: 0.8964\n",
      "10000/10000 [==============================] - 0s 36us/sample - loss: 0.0163 - accuracy: 0.8964\n",
      "10000/10000 [==============================] - 0s 37us/sample - loss: 0.0164 - accuracy: 0.8970\n",
      "10000/10000 [==============================] - 0s 36us/sample - loss: 0.0165 - accuracy: 0.8972\n",
      "10000/10000 [==============================] - 0s 36us/sample - loss: 0.0166 - accuracy: 0.8965\n",
      "10000/10000 [==============================] - 0s 38us/sample - loss: 0.0167 - accuracy: 0.8958\n",
      "10000/10000 [==============================] - 0s 37us/sample - loss: 0.0169 - accuracy: 0.8951\n",
      "10000/10000 [==============================] - 0s 38us/sample - loss: 0.0171 - accuracy: 0.8941\n",
      "10000/10000 [==============================] - 0s 37us/sample - loss: 0.0173 - accuracy: 0.8929\n",
      "10000/10000 [==============================] - 0s 37us/sample - loss: 0.0176 - accuracy: 0.8912\n",
      "10000/10000 [==============================] - 0s 38us/sample - loss: 0.0179 - accuracy: 0.8893\n",
      "10000/10000 [==============================] - 0s 37us/sample - loss: 0.0182 - accuracy: 0.8878\n",
      "10000/10000 [==============================] - 0s 39us/sample - loss: 0.0186 - accuracy: 0.8863\n",
      "10000/10000 [==============================] - 0s 36us/sample - loss: 0.0190 - accuracy: 0.8848\n",
      "62th iteration\n",
      "10000/10000 [==============================] - 0s 38us/sample - loss: 0.0151 - accuracy: 0.9033\n",
      "10000/10000 [==============================] - 0s 37us/sample - loss: 0.0150 - accuracy: 0.9040\n",
      "10000/10000 [==============================] - 0s 37us/sample - loss: 0.0149 - accuracy: 0.9048\n",
      "10000/10000 [==============================] - 0s 38us/sample - loss: 0.0148 - accuracy: 0.9061\n",
      "10000/10000 [==============================] - 0s 37us/sample - loss: 0.0147 - accuracy: 0.9067\n",
      "10000/10000 [==============================] - 0s 36us/sample - loss: 0.0147 - accuracy: 0.9073\n",
      "10000/10000 [==============================] - 0s 37us/sample - loss: 0.0146 - accuracy: 0.9074\n",
      "10000/10000 [==============================] - 0s 37us/sample - loss: 0.0146 - accuracy: 0.9085\n",
      "10000/10000 [==============================] - 0s 36us/sample - loss: 0.0146 - accuracy: 0.9093\n",
      "10000/10000 [==============================] - 0s 38us/sample - loss: 0.0147 - accuracy: 0.9085\n",
      "10000/10000 [==============================] - 0s 36us/sample - loss: 0.0147 - accuracy: 0.9085\n",
      "10000/10000 [==============================] - 0s 38us/sample - loss: 0.0148 - accuracy: 0.9079\n",
      "10000/10000 [==============================] - 0s 38us/sample - loss: 0.0149 - accuracy: 0.9075\n",
      "10000/10000 [==============================] - 0s 36us/sample - loss: 0.0151 - accuracy: 0.9065\n",
      "10000/10000 [==============================] - 0s 37us/sample - loss: 0.0152 - accuracy: 0.9057\n",
      "10000/10000 [==============================] - 0s 37us/sample - loss: 0.0154 - accuracy: 0.9055\n",
      "10000/10000 [==============================] - 0s 36us/sample - loss: 0.0156 - accuracy: 0.9033\n",
      "10000/10000 [==============================] - 0s 37us/sample - loss: 0.0158 - accuracy: 0.9020\n",
      "10000/10000 [==============================] - 0s 37us/sample - loss: 0.0161 - accuracy: 0.8999\n",
      "10000/10000 [==============================] - 0s 38us/sample - loss: 0.0164 - accuracy: 0.8979\n",
      "10000/10000 [==============================] - 0s 36us/sample - loss: 0.0167 - accuracy: 0.8959\n",
      "63th iteration\n",
      "10000/10000 [==============================] - 0s 37us/sample - loss: 0.0168 - accuracy: 0.8870\n",
      "10000/10000 [==============================] - 0s 38us/sample - loss: 0.0166 - accuracy: 0.8886\n",
      "10000/10000 [==============================] - 0s 37us/sample - loss: 0.0164 - accuracy: 0.8919\n",
      "10000/10000 [==============================] - 0s 36us/sample - loss: 0.0162 - accuracy: 0.8937\n",
      "10000/10000 [==============================] - 0s 38us/sample - loss: 0.0161 - accuracy: 0.8956\n",
      "10000/10000 [==============================] - 0s 37us/sample - loss: 0.0160 - accuracy: 0.8965\n",
      "10000/10000 [==============================] - 0s 36us/sample - loss: 0.0159 - accuracy: 0.8971\n",
      "10000/10000 [==============================] - 0s 37us/sample - loss: 0.0159 - accuracy: 0.8985\n",
      "10000/10000 [==============================] - 0s 35us/sample - loss: 0.0159 - accuracy: 0.8995\n",
      "10000/10000 [==============================] - 0s 36us/sample - loss: 0.0159 - accuracy: 0.8985\n",
      "10000/10000 [==============================] - 0s 36us/sample - loss: 0.0160 - accuracy: 0.8983\n",
      "10000/10000 [==============================] - 0s 36us/sample - loss: 0.0160 - accuracy: 0.8972\n",
      "10000/10000 [==============================] - 0s 37us/sample - loss: 0.0161 - accuracy: 0.8981\n",
      "10000/10000 [==============================] - 0s 37us/sample - loss: 0.0163 - accuracy: 0.8972\n",
      "10000/10000 [==============================] - 0s 38us/sample - loss: 0.0165 - accuracy: 0.8951\n",
      "10000/10000 [==============================] - 0s 37us/sample - loss: 0.0167 - accuracy: 0.8942\n",
      "10000/10000 [==============================] - 0s 37us/sample - loss: 0.0169 - accuracy: 0.8925\n",
      "10000/10000 [==============================] - 0s 35us/sample - loss: 0.0172 - accuracy: 0.8910\n",
      "10000/10000 [==============================] - 0s 36us/sample - loss: 0.0174 - accuracy: 0.8901\n",
      "10000/10000 [==============================] - 0s 36us/sample - loss: 0.0177 - accuracy: 0.8886\n",
      "10000/10000 [==============================] - 0s 37us/sample - loss: 0.0181 - accuracy: 0.8868\n",
      "64th iteration\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000/10000 [==============================] - 0s 38us/sample - loss: 0.0168 - accuracy: 0.8870\n",
      "10000/10000 [==============================] - 0s 36us/sample - loss: 0.0166 - accuracy: 0.8883\n",
      "10000/10000 [==============================] - 0s 37us/sample - loss: 0.0165 - accuracy: 0.8900\n",
      "10000/10000 [==============================] - 0s 37us/sample - loss: 0.0164 - accuracy: 0.8912\n",
      "10000/10000 [==============================] - 0s 38us/sample - loss: 0.0163 - accuracy: 0.8929\n",
      "10000/10000 [==============================] - 0s 38us/sample - loss: 0.0162 - accuracy: 0.8939\n",
      "10000/10000 [==============================] - 0s 35us/sample - loss: 0.0162 - accuracy: 0.8948\n",
      "10000/10000 [==============================] - 0s 38us/sample - loss: 0.0162 - accuracy: 0.8956\n",
      "10000/10000 [==============================] - 0s 36us/sample - loss: 0.0162 - accuracy: 0.8961\n",
      "10000/10000 [==============================] - 0s 36us/sample - loss: 0.0163 - accuracy: 0.8967\n",
      "10000/10000 [==============================] - 0s 37us/sample - loss: 0.0163 - accuracy: 0.8960\n",
      "10000/10000 [==============================] - 0s 38us/sample - loss: 0.0165 - accuracy: 0.8964\n",
      "10000/10000 [==============================] - 0s 39us/sample - loss: 0.0166 - accuracy: 0.8962\n",
      "10000/10000 [==============================] - 0s 36us/sample - loss: 0.0168 - accuracy: 0.8955\n",
      "10000/10000 [==============================] - 0s 35us/sample - loss: 0.0170 - accuracy: 0.8934\n",
      "10000/10000 [==============================] - 0s 38us/sample - loss: 0.0173 - accuracy: 0.8924\n",
      "10000/10000 [==============================] - 0s 37us/sample - loss: 0.0176 - accuracy: 0.8909\n",
      "10000/10000 [==============================] - 0s 37us/sample - loss: 0.0179 - accuracy: 0.8897\n",
      "10000/10000 [==============================] - 0s 37us/sample - loss: 0.0182 - accuracy: 0.8865\n",
      "10000/10000 [==============================] - 0s 37us/sample - loss: 0.0186 - accuracy: 0.8843\n",
      "10000/10000 [==============================] - 0s 37us/sample - loss: 0.0191 - accuracy: 0.8818\n",
      "65th iteration\n",
      "10000/10000 [==============================] - 0s 37us/sample - loss: 0.0151 - accuracy: 0.9033\n",
      "10000/10000 [==============================] - 0s 37us/sample - loss: 0.0150 - accuracy: 0.9035\n",
      "10000/10000 [==============================] - 0s 36us/sample - loss: 0.0148 - accuracy: 0.9051\n",
      "10000/10000 [==============================] - 0s 38us/sample - loss: 0.0147 - accuracy: 0.9055\n",
      "10000/10000 [==============================] - 0s 38us/sample - loss: 0.0146 - accuracy: 0.9065\n",
      "10000/10000 [==============================] - 0s 36us/sample - loss: 0.0146 - accuracy: 0.9073\n",
      "10000/10000 [==============================] - 0s 36us/sample - loss: 0.0145 - accuracy: 0.9074\n",
      "10000/10000 [==============================] - 0s 36us/sample - loss: 0.0145 - accuracy: 0.9070\n",
      "10000/10000 [==============================] - 0s 37us/sample - loss: 0.0145 - accuracy: 0.9073\n",
      "10000/10000 [==============================] - 0s 37us/sample - loss: 0.0145 - accuracy: 0.9072\n",
      "10000/10000 [==============================] - 0s 38us/sample - loss: 0.0146 - accuracy: 0.9074\n",
      "10000/10000 [==============================] - 0s 36us/sample - loss: 0.0146 - accuracy: 0.9066\n",
      "10000/10000 [==============================] - 0s 37us/sample - loss: 0.0147 - accuracy: 0.9067\n",
      "10000/10000 [==============================] - 0s 35us/sample - loss: 0.0148 - accuracy: 0.9060\n",
      "10000/10000 [==============================] - 0s 37us/sample - loss: 0.0150 - accuracy: 0.9051\n",
      "10000/10000 [==============================] - 0s 37us/sample - loss: 0.0151 - accuracy: 0.9040\n",
      "10000/10000 [==============================] - 1s 59us/sample - loss: 0.0153 - accuracy: 0.9026\n",
      "10000/10000 [==============================] - 0s 36us/sample - loss: 0.0155 - accuracy: 0.9004\n",
      "10000/10000 [==============================] - 0s 36us/sample - loss: 0.0157 - accuracy: 0.8980\n",
      "10000/10000 [==============================] - 0s 37us/sample - loss: 0.0160 - accuracy: 0.8955\n",
      "10000/10000 [==============================] - 0s 37us/sample - loss: 0.0163 - accuracy: 0.8942\n",
      "66th iteration\n",
      "10000/10000 [==============================] - 0s 36us/sample - loss: 0.0168 - accuracy: 0.8870\n",
      "10000/10000 [==============================] - 0s 37us/sample - loss: 0.0167 - accuracy: 0.8883\n",
      "10000/10000 [==============================] - 0s 36us/sample - loss: 0.0166 - accuracy: 0.8897\n",
      "10000/10000 [==============================] - 0s 37us/sample - loss: 0.0166 - accuracy: 0.8904\n",
      "10000/10000 [==============================] - 0s 36us/sample - loss: 0.0166 - accuracy: 0.8911\n",
      "10000/10000 [==============================] - 0s 37us/sample - loss: 0.0166 - accuracy: 0.8916\n",
      "10000/10000 [==============================] - 0s 36us/sample - loss: 0.0166 - accuracy: 0.8932\n",
      "10000/10000 [==============================] - 0s 39us/sample - loss: 0.0167 - accuracy: 0.8942\n",
      "10000/10000 [==============================] - 0s 35us/sample - loss: 0.0168 - accuracy: 0.8944\n",
      "10000/10000 [==============================] - 0s 36us/sample - loss: 0.0169 - accuracy: 0.8941\n",
      "10000/10000 [==============================] - 0s 37us/sample - loss: 0.0171 - accuracy: 0.8943\n",
      "10000/10000 [==============================] - 0s 38us/sample - loss: 0.0173 - accuracy: 0.8947\n",
      "10000/10000 [==============================] - 0s 35us/sample - loss: 0.0175 - accuracy: 0.8939\n",
      "10000/10000 [==============================] - 0s 37us/sample - loss: 0.0178 - accuracy: 0.8921\n",
      "10000/10000 [==============================] - 0s 36us/sample - loss: 0.0181 - accuracy: 0.8913\n",
      "10000/10000 [==============================] - 0s 36us/sample - loss: 0.0184 - accuracy: 0.8912\n",
      "10000/10000 [==============================] - 0s 37us/sample - loss: 0.0188 - accuracy: 0.8893\n",
      "10000/10000 [==============================] - 0s 36us/sample - loss: 0.0192 - accuracy: 0.8872\n",
      "10000/10000 [==============================] - 0s 37us/sample - loss: 0.0196 - accuracy: 0.8847\n",
      "10000/10000 [==============================] - 1s 57us/sample - loss: 0.0201 - accuracy: 0.8829\n",
      "10000/10000 [==============================] - 0s 37us/sample - loss: 0.0206 - accuracy: 0.8813\n",
      "67th iteration\n",
      "10000/10000 [==============================] - 0s 36us/sample - loss: 0.0159 - accuracy: 0.8943\n",
      "10000/10000 [==============================] - 0s 37us/sample - loss: 0.0159 - accuracy: 0.8947\n",
      "10000/10000 [==============================] - 0s 36us/sample - loss: 0.0159 - accuracy: 0.8953\n",
      "10000/10000 [==============================] - 0s 37us/sample - loss: 0.0159 - accuracy: 0.8954\n",
      "10000/10000 [==============================] - 0s 37us/sample - loss: 0.0160 - accuracy: 0.8955\n",
      "10000/10000 [==============================] - 0s 37us/sample - loss: 0.0161 - accuracy: 0.8956\n",
      "10000/10000 [==============================] - 0s 37us/sample - loss: 0.0163 - accuracy: 0.8952\n",
      "10000/10000 [==============================] - 0s 37us/sample - loss: 0.0164 - accuracy: 0.8949\n",
      "10000/10000 [==============================] - 0s 36us/sample - loss: 0.0166 - accuracy: 0.8951\n",
      "10000/10000 [==============================] - 0s 35us/sample - loss: 0.0169 - accuracy: 0.8950\n",
      "10000/10000 [==============================] - 0s 36us/sample - loss: 0.0172 - accuracy: 0.8941\n",
      "10000/10000 [==============================] - 0s 37us/sample - loss: 0.0175 - accuracy: 0.8936\n",
      "10000/10000 [==============================] - 0s 37us/sample - loss: 0.0179 - accuracy: 0.8931\n",
      "10000/10000 [==============================] - 0s 37us/sample - loss: 0.0183 - accuracy: 0.8918\n",
      "10000/10000 [==============================] - 0s 37us/sample - loss: 0.0187 - accuracy: 0.8903\n",
      "10000/10000 [==============================] - 0s 36us/sample - loss: 0.0192 - accuracy: 0.8885\n",
      "10000/10000 [==============================] - 0s 37us/sample - loss: 0.0198 - accuracy: 0.8855\n",
      "10000/10000 [==============================] - 0s 37us/sample - loss: 0.0204 - accuracy: 0.8826\n",
      "10000/10000 [==============================] - 0s 38us/sample - loss: 0.0210 - accuracy: 0.8791\n",
      "10000/10000 [==============================] - 0s 36us/sample - loss: 0.0217 - accuracy: 0.8752\n",
      "10000/10000 [==============================] - 0s 37us/sample - loss: 0.0224 - accuracy: 0.8711\n",
      "68th iteration\n",
      "10000/10000 [==============================] - 0s 38us/sample - loss: 0.0161 - accuracy: 0.8926\n",
      "10000/10000 [==============================] - 0s 36us/sample - loss: 0.0160 - accuracy: 0.8936\n",
      "10000/10000 [==============================] - 0s 38us/sample - loss: 0.0159 - accuracy: 0.8949\n",
      "10000/10000 [==============================] - 0s 37us/sample - loss: 0.0159 - accuracy: 0.8960\n",
      "10000/10000 [==============================] - 0s 37us/sample - loss: 0.0158 - accuracy: 0.8957\n",
      "10000/10000 [==============================] - 0s 37us/sample - loss: 0.0158 - accuracy: 0.8964\n",
      "10000/10000 [==============================] - 0s 36us/sample - loss: 0.0158 - accuracy: 0.8969\n",
      "10000/10000 [==============================] - 0s 36us/sample - loss: 0.0158 - accuracy: 0.8960\n",
      "10000/10000 [==============================] - 0s 37us/sample - loss: 0.0158 - accuracy: 0.8958\n",
      "10000/10000 [==============================] - 0s 37us/sample - loss: 0.0159 - accuracy: 0.8955\n",
      "10000/10000 [==============================] - 0s 36us/sample - loss: 0.0159 - accuracy: 0.8942\n",
      "10000/10000 [==============================] - 0s 38us/sample - loss: 0.0160 - accuracy: 0.8946\n",
      "10000/10000 [==============================] - 0s 37us/sample - loss: 0.0162 - accuracy: 0.8951\n",
      "10000/10000 [==============================] - 0s 37us/sample - loss: 0.0163 - accuracy: 0.8939\n",
      "10000/10000 [==============================] - 0s 36us/sample - loss: 0.0165 - accuracy: 0.8936\n",
      "10000/10000 [==============================] - 0s 36us/sample - loss: 0.0167 - accuracy: 0.8931\n",
      "10000/10000 [==============================] - 0s 38us/sample - loss: 0.0169 - accuracy: 0.8930\n",
      "10000/10000 [==============================] - 0s 36us/sample - loss: 0.0172 - accuracy: 0.8914\n",
      "10000/10000 [==============================] - 0s 36us/sample - loss: 0.0175 - accuracy: 0.8904\n",
      "10000/10000 [==============================] - 0s 38us/sample - loss: 0.0178 - accuracy: 0.8890\n",
      "10000/10000 [==============================] - 0s 37us/sample - loss: 0.0181 - accuracy: 0.8868\n",
      "69th iteration\n",
      "10000/10000 [==============================] - 0s 37us/sample - loss: 0.0163 - accuracy: 0.8942\n",
      "10000/10000 [==============================] - 0s 37us/sample - loss: 0.0160 - accuracy: 0.8954\n",
      "10000/10000 [==============================] - 0s 37us/sample - loss: 0.0158 - accuracy: 0.8977\n",
      "10000/10000 [==============================] - 0s 37us/sample - loss: 0.0156 - accuracy: 0.8998\n",
      "10000/10000 [==============================] - 0s 37us/sample - loss: 0.0155 - accuracy: 0.9004\n",
      "10000/10000 [==============================] - 0s 37us/sample - loss: 0.0153 - accuracy: 0.9007\n",
      "10000/10000 [==============================] - 0s 37us/sample - loss: 0.0152 - accuracy: 0.9010\n",
      "10000/10000 [==============================] - 0s 37us/sample - loss: 0.0151 - accuracy: 0.9020\n",
      "10000/10000 [==============================] - 0s 36us/sample - loss: 0.0151 - accuracy: 0.9016\n",
      "10000/10000 [==============================] - 0s 38us/sample - loss: 0.0150 - accuracy: 0.9025\n",
      "10000/10000 [==============================] - 0s 37us/sample - loss: 0.0150 - accuracy: 0.9028\n",
      "10000/10000 [==============================] - 0s 38us/sample - loss: 0.0150 - accuracy: 0.9044\n",
      "10000/10000 [==============================] - 0s 37us/sample - loss: 0.0150 - accuracy: 0.9031\n",
      "10000/10000 [==============================] - 0s 35us/sample - loss: 0.0151 - accuracy: 0.9024\n",
      "10000/10000 [==============================] - 0s 37us/sample - loss: 0.0152 - accuracy: 0.9016\n",
      "10000/10000 [==============================] - 0s 37us/sample - loss: 0.0153 - accuracy: 0.9006\n",
      "10000/10000 [==============================] - 0s 36us/sample - loss: 0.0154 - accuracy: 0.8997\n",
      "10000/10000 [==============================] - 0s 38us/sample - loss: 0.0155 - accuracy: 0.8999\n",
      "10000/10000 [==============================] - 0s 40us/sample - loss: 0.0157 - accuracy: 0.8987\n",
      "10000/10000 [==============================] - 0s 38us/sample - loss: 0.0159 - accuracy: 0.8982\n",
      "10000/10000 [==============================] - 0s 38us/sample - loss: 0.0161 - accuracy: 0.8961\n",
      "70th iteration\n",
      "10000/10000 [==============================] - 0s 40us/sample - loss: 0.0161 - accuracy: 0.8926\n",
      "10000/10000 [==============================] - 0s 40us/sample - loss: 0.0160 - accuracy: 0.8934\n",
      "10000/10000 [==============================] - 0s 40us/sample - loss: 0.0158 - accuracy: 0.8950\n",
      "10000/10000 [==============================] - 0s 41us/sample - loss: 0.0157 - accuracy: 0.8964\n",
      "10000/10000 [==============================] - 0s 37us/sample - loss: 0.0156 - accuracy: 0.8971\n",
      "10000/10000 [==============================] - 0s 36us/sample - loss: 0.0155 - accuracy: 0.8979\n",
      "10000/10000 [==============================] - 0s 37us/sample - loss: 0.0155 - accuracy: 0.8988\n",
      "10000/10000 [==============================] - 0s 37us/sample - loss: 0.0155 - accuracy: 0.8988\n",
      "10000/10000 [==============================] - 0s 38us/sample - loss: 0.0155 - accuracy: 0.8991\n",
      "10000/10000 [==============================] - 0s 37us/sample - loss: 0.0156 - accuracy: 0.8987\n",
      "10000/10000 [==============================] - 0s 36us/sample - loss: 0.0156 - accuracy: 0.8985\n",
      "10000/10000 [==============================] - 0s 36us/sample - loss: 0.0157 - accuracy: 0.8972\n",
      "10000/10000 [==============================] - 0s 35us/sample - loss: 0.0158 - accuracy: 0.8954\n",
      "10000/10000 [==============================] - 0s 37us/sample - loss: 0.0160 - accuracy: 0.8945\n",
      "10000/10000 [==============================] - 0s 35us/sample - loss: 0.0162 - accuracy: 0.8932\n",
      "10000/10000 [==============================] - 0s 37us/sample - loss: 0.0164 - accuracy: 0.8917\n",
      "10000/10000 [==============================] - 0s 39us/sample - loss: 0.0166 - accuracy: 0.8906\n",
      "10000/10000 [==============================] - 0s 41us/sample - loss: 0.0169 - accuracy: 0.8891\n",
      "10000/10000 [==============================] - 0s 38us/sample - loss: 0.0172 - accuracy: 0.8875\n",
      "10000/10000 [==============================] - 0s 37us/sample - loss: 0.0175 - accuracy: 0.8855\n",
      "10000/10000 [==============================] - 0s 37us/sample - loss: 0.0179 - accuracy: 0.8823\n",
      "71th iteration\n",
      "10000/10000 [==============================] - 0s 38us/sample - loss: 0.0179 - accuracy: 0.8823\n",
      "10000/10000 [==============================] - 0s 37us/sample - loss: 0.0181 - accuracy: 0.8832\n",
      "10000/10000 [==============================] - 0s 35us/sample - loss: 0.0183 - accuracy: 0.8827\n",
      "10000/10000 [==============================] - 0s 38us/sample - loss: 0.0186 - accuracy: 0.8822\n",
      "10000/10000 [==============================] - 0s 39us/sample - loss: 0.0190 - accuracy: 0.8808\n",
      "10000/10000 [==============================] - 0s 37us/sample - loss: 0.0195 - accuracy: 0.8788\n",
      "10000/10000 [==============================] - 0s 40us/sample - loss: 0.0200 - accuracy: 0.8774\n",
      "10000/10000 [==============================] - 0s 38us/sample - loss: 0.0206 - accuracy: 0.8758\n",
      "10000/10000 [==============================] - 0s 38us/sample - loss: 0.0213 - accuracy: 0.8733\n",
      "10000/10000 [==============================] - 0s 38us/sample - loss: 0.0221 - accuracy: 0.8713\n",
      "10000/10000 [==============================] - 0s 42us/sample - loss: 0.0230 - accuracy: 0.8690\n",
      "10000/10000 [==============================] - 0s 40us/sample - loss: 0.0239 - accuracy: 0.8662\n",
      "10000/10000 [==============================] - 0s 43us/sample - loss: 0.0249 - accuracy: 0.8632\n",
      "10000/10000 [==============================] - 0s 41us/sample - loss: 0.0260 - accuracy: 0.8584\n",
      "10000/10000 [==============================] - 0s 40us/sample - loss: 0.0272 - accuracy: 0.8528\n",
      "10000/10000 [==============================] - 0s 40us/sample - loss: 0.0284 - accuracy: 0.8457\n",
      "10000/10000 [==============================] - 0s 37us/sample - loss: 0.0297 - accuracy: 0.8389\n",
      "10000/10000 [==============================] - 0s 39us/sample - loss: 0.0311 - accuracy: 0.8302\n",
      "10000/10000 [==============================] - 0s 42us/sample - loss: 0.0325 - accuracy: 0.8208\n",
      "10000/10000 [==============================] - 0s 39us/sample - loss: 0.0340 - accuracy: 0.8122\n",
      "10000/10000 [==============================] - 1s 62us/sample - loss: 0.0355 - accuracy: 0.8016\n",
      "72th iteration\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000/10000 [==============================] - 0s 39us/sample - loss: 0.0161 - accuracy: 0.8926\n",
      "10000/10000 [==============================] - 0s 38us/sample - loss: 0.0160 - accuracy: 0.8928\n",
      "10000/10000 [==============================] - 0s 38us/sample - loss: 0.0160 - accuracy: 0.8936\n",
      "10000/10000 [==============================] - 0s 38us/sample - loss: 0.0160 - accuracy: 0.8940\n",
      "10000/10000 [==============================] - 0s 38us/sample - loss: 0.0159 - accuracy: 0.8948\n",
      "10000/10000 [==============================] - 0s 39us/sample - loss: 0.0159 - accuracy: 0.8947\n",
      "10000/10000 [==============================] - 0s 38us/sample - loss: 0.0160 - accuracy: 0.8948\n",
      "10000/10000 [==============================] - 0s 39us/sample - loss: 0.0160 - accuracy: 0.8949\n",
      "10000/10000 [==============================] - 0s 37us/sample - loss: 0.0161 - accuracy: 0.8945\n",
      "10000/10000 [==============================] - 0s 37us/sample - loss: 0.0162 - accuracy: 0.8943\n",
      "10000/10000 [==============================] - 0s 38us/sample - loss: 0.0163 - accuracy: 0.8949\n",
      "10000/10000 [==============================] - 0s 39us/sample - loss: 0.0164 - accuracy: 0.8950\n",
      "10000/10000 [==============================] - 0s 40us/sample - loss: 0.0166 - accuracy: 0.8945\n",
      "10000/10000 [==============================] - 0s 38us/sample - loss: 0.0168 - accuracy: 0.8941\n",
      "10000/10000 [==============================] - 0s 39us/sample - loss: 0.0170 - accuracy: 0.8933\n",
      "10000/10000 [==============================] - 0s 39us/sample - loss: 0.0173 - accuracy: 0.8923\n",
      "10000/10000 [==============================] - 0s 38us/sample - loss: 0.0176 - accuracy: 0.8909\n",
      "10000/10000 [==============================] - 0s 38us/sample - loss: 0.0179 - accuracy: 0.8896\n",
      "10000/10000 [==============================] - 0s 37us/sample - loss: 0.0182 - accuracy: 0.8879\n",
      "10000/10000 [==============================] - 0s 38us/sample - loss: 0.0186 - accuracy: 0.8863\n",
      "10000/10000 [==============================] - 0s 38us/sample - loss: 0.0190 - accuracy: 0.8848\n",
      "73th iteration\n",
      "10000/10000 [==============================] - 0s 35us/sample - loss: 0.0168 - accuracy: 0.8870\n",
      "10000/10000 [==============================] - 0s 36us/sample - loss: 0.0165 - accuracy: 0.8896\n",
      "10000/10000 [==============================] - 1s 56us/sample - loss: 0.0163 - accuracy: 0.8921\n",
      "10000/10000 [==============================] - 0s 35us/sample - loss: 0.0162 - accuracy: 0.8941\n",
      "10000/10000 [==============================] - 0s 37us/sample - loss: 0.0160 - accuracy: 0.8951\n",
      "10000/10000 [==============================] - 0s 39us/sample - loss: 0.0159 - accuracy: 0.8965\n",
      "10000/10000 [==============================] - 0s 37us/sample - loss: 0.0157 - accuracy: 0.8968\n",
      "10000/10000 [==============================] - 0s 37us/sample - loss: 0.0157 - accuracy: 0.8987\n",
      "10000/10000 [==============================] - 0s 35us/sample - loss: 0.0156 - accuracy: 0.8994\n",
      "10000/10000 [==============================] - 0s 36us/sample - loss: 0.0155 - accuracy: 0.8998\n",
      "10000/10000 [==============================] - 0s 37us/sample - loss: 0.0155 - accuracy: 0.8996\n",
      "10000/10000 [==============================] - 0s 37us/sample - loss: 0.0155 - accuracy: 0.8996\n",
      "10000/10000 [==============================] - 0s 36us/sample - loss: 0.0156 - accuracy: 0.8995\n",
      "10000/10000 [==============================] - 0s 37us/sample - loss: 0.0156 - accuracy: 0.8992\n",
      "10000/10000 [==============================] - 0s 37us/sample - loss: 0.0157 - accuracy: 0.8996\n",
      "10000/10000 [==============================] - 0s 36us/sample - loss: 0.0158 - accuracy: 0.8994\n",
      "10000/10000 [==============================] - 0s 36us/sample - loss: 0.0159 - accuracy: 0.8997\n",
      "10000/10000 [==============================] - 0s 36us/sample - loss: 0.0161 - accuracy: 0.8995\n",
      "10000/10000 [==============================] - 0s 36us/sample - loss: 0.0162 - accuracy: 0.8986\n",
      "10000/10000 [==============================] - 0s 36us/sample - loss: 0.0164 - accuracy: 0.8969\n",
      "10000/10000 [==============================] - 0s 37us/sample - loss: 0.0167 - accuracy: 0.8959\n",
      "74th iteration\n",
      "10000/10000 [==============================] - 0s 37us/sample - loss: 0.0168 - accuracy: 0.8870\n",
      "10000/10000 [==============================] - 0s 37us/sample - loss: 0.0165 - accuracy: 0.8898\n",
      "10000/10000 [==============================] - 0s 37us/sample - loss: 0.0163 - accuracy: 0.8929\n",
      "10000/10000 [==============================] - 0s 39us/sample - loss: 0.0161 - accuracy: 0.8937\n",
      "10000/10000 [==============================] - 0s 37us/sample - loss: 0.0159 - accuracy: 0.8952\n",
      "10000/10000 [==============================] - 0s 37us/sample - loss: 0.0157 - accuracy: 0.8961\n",
      "10000/10000 [==============================] - 0s 38us/sample - loss: 0.0156 - accuracy: 0.8982\n",
      "10000/10000 [==============================] - 0s 37us/sample - loss: 0.0155 - accuracy: 0.8994\n",
      "10000/10000 [==============================] - 0s 38us/sample - loss: 0.0154 - accuracy: 0.9001\n",
      "10000/10000 [==============================] - 0s 39us/sample - loss: 0.0153 - accuracy: 0.9017\n",
      "10000/10000 [==============================] - 0s 36us/sample - loss: 0.0152 - accuracy: 0.9018\n",
      "10000/10000 [==============================] - 0s 37us/sample - loss: 0.0152 - accuracy: 0.9010\n",
      "10000/10000 [==============================] - 0s 40us/sample - loss: 0.0152 - accuracy: 0.9017\n",
      "10000/10000 [==============================] - 0s 35us/sample - loss: 0.0153 - accuracy: 0.9018\n",
      "10000/10000 [==============================] - 0s 37us/sample - loss: 0.0153 - accuracy: 0.9004\n",
      "10000/10000 [==============================] - 0s 35us/sample - loss: 0.0154 - accuracy: 0.9001\n",
      "10000/10000 [==============================] - 0s 36us/sample - loss: 0.0155 - accuracy: 0.8996\n",
      "10000/10000 [==============================] - 0s 37us/sample - loss: 0.0156 - accuracy: 0.8984\n",
      "10000/10000 [==============================] - 0s 36us/sample - loss: 0.0158 - accuracy: 0.8981\n",
      "10000/10000 [==============================] - 0s 37us/sample - loss: 0.0159 - accuracy: 0.8967\n",
      "10000/10000 [==============================] - 0s 37us/sample - loss: 0.0161 - accuracy: 0.8961\n",
      "75th iteration\n",
      "10000/10000 [==============================] - 0s 39us/sample - loss: 0.0151 - accuracy: 0.9033\n",
      "10000/10000 [==============================] - 0s 36us/sample - loss: 0.0152 - accuracy: 0.9029\n",
      "10000/10000 [==============================] - 0s 36us/sample - loss: 0.0153 - accuracy: 0.9027\n",
      "10000/10000 [==============================] - 0s 35us/sample - loss: 0.0154 - accuracy: 0.9030\n",
      "10000/10000 [==============================] - 0s 36us/sample - loss: 0.0155 - accuracy: 0.9030\n",
      "10000/10000 [==============================] - 0s 37us/sample - loss: 0.0157 - accuracy: 0.9024\n",
      "10000/10000 [==============================] - 0s 36us/sample - loss: 0.0159 - accuracy: 0.9011\n",
      "10000/10000 [==============================] - 0s 35us/sample - loss: 0.0161 - accuracy: 0.9008\n",
      "10000/10000 [==============================] - 0s 35us/sample - loss: 0.0164 - accuracy: 0.8997\n",
      "10000/10000 [==============================] - 0s 38us/sample - loss: 0.0167 - accuracy: 0.8979\n",
      "10000/10000 [==============================] - 0s 39us/sample - loss: 0.0170 - accuracy: 0.8971\n",
      "10000/10000 [==============================] - 0s 42us/sample - loss: 0.0174 - accuracy: 0.8960\n",
      "10000/10000 [==============================] - 0s 39us/sample - loss: 0.0178 - accuracy: 0.8950\n",
      "10000/10000 [==============================] - 0s 36us/sample - loss: 0.0183 - accuracy: 0.8930\n",
      "10000/10000 [==============================] - 0s 37us/sample - loss: 0.0187 - accuracy: 0.8913\n",
      "10000/10000 [==============================] - 0s 38us/sample - loss: 0.0193 - accuracy: 0.8875\n",
      "10000/10000 [==============================] - 0s 39us/sample - loss: 0.0198 - accuracy: 0.8849\n",
      "10000/10000 [==============================] - 0s 38us/sample - loss: 0.0204 - accuracy: 0.8811\n",
      "10000/10000 [==============================] - 0s 37us/sample - loss: 0.0210 - accuracy: 0.8787\n",
      "10000/10000 [==============================] - 0s 38us/sample - loss: 0.0217 - accuracy: 0.8743\n",
      "10000/10000 [==============================] - 0s 39us/sample - loss: 0.0224 - accuracy: 0.8711\n",
      "76th iteration\n",
      "10000/10000 [==============================] - 0s 40us/sample - loss: 0.0159 - accuracy: 0.8943\n",
      "10000/10000 [==============================] - 0s 37us/sample - loss: 0.0157 - accuracy: 0.8962\n",
      "10000/10000 [==============================] - 0s 40us/sample - loss: 0.0155 - accuracy: 0.8988\n",
      "10000/10000 [==============================] - 0s 39us/sample - loss: 0.0153 - accuracy: 0.9001\n",
      "10000/10000 [==============================] - 0s 38us/sample - loss: 0.0151 - accuracy: 0.9013\n",
      "10000/10000 [==============================] - 0s 44us/sample - loss: 0.0150 - accuracy: 0.9022\n",
      "10000/10000 [==============================] - 0s 40us/sample - loss: 0.0149 - accuracy: 0.9035\n",
      "10000/10000 [==============================] - 0s 38us/sample - loss: 0.0147 - accuracy: 0.9053\n",
      "10000/10000 [==============================] - 0s 39us/sample - loss: 0.0147 - accuracy: 0.9062\n",
      "10000/10000 [==============================] - 0s 37us/sample - loss: 0.0146 - accuracy: 0.9068\n",
      "10000/10000 [==============================] - 0s 36us/sample - loss: 0.0145 - accuracy: 0.9074\n",
      "10000/10000 [==============================] - 0s 37us/sample - loss: 0.0145 - accuracy: 0.9075\n",
      "10000/10000 [==============================] - 0s 37us/sample - loss: 0.0145 - accuracy: 0.9083\n",
      "10000/10000 [==============================] - 0s 37us/sample - loss: 0.0145 - accuracy: 0.9072\n",
      "10000/10000 [==============================] - 0s 36us/sample - loss: 0.0145 - accuracy: 0.9062\n",
      "10000/10000 [==============================] - 0s 43us/sample - loss: 0.0146 - accuracy: 0.9054\n",
      "10000/10000 [==============================] - 0s 39us/sample - loss: 0.0146 - accuracy: 0.9056\n",
      "10000/10000 [==============================] - 0s 36us/sample - loss: 0.0147 - accuracy: 0.9046\n",
      "10000/10000 [==============================] - 0s 37us/sample - loss: 0.0148 - accuracy: 0.9038\n",
      "10000/10000 [==============================] - 0s 37us/sample - loss: 0.0150 - accuracy: 0.9036\n",
      "10000/10000 [==============================] - 0s 37us/sample - loss: 0.0151 - accuracy: 0.9033\n",
      "77th iteration\n",
      "10000/10000 [==============================] - 0s 37us/sample - loss: 0.0159 - accuracy: 0.8943\n",
      "10000/10000 [==============================] - 0s 38us/sample - loss: 0.0157 - accuracy: 0.8958\n",
      "10000/10000 [==============================] - 0s 39us/sample - loss: 0.0155 - accuracy: 0.8968\n",
      "10000/10000 [==============================] - 0s 42us/sample - loss: 0.0154 - accuracy: 0.8981\n",
      "10000/10000 [==============================] - 0s 39us/sample - loss: 0.0152 - accuracy: 0.8991\n",
      "10000/10000 [==============================] - 0s 35us/sample - loss: 0.0151 - accuracy: 0.9008\n",
      "10000/10000 [==============================] - 0s 37us/sample - loss: 0.0150 - accuracy: 0.9013\n",
      "10000/10000 [==============================] - 0s 38us/sample - loss: 0.0150 - accuracy: 0.9019\n",
      "10000/10000 [==============================] - 0s 38us/sample - loss: 0.0149 - accuracy: 0.9024\n",
      "10000/10000 [==============================] - 0s 38us/sample - loss: 0.0149 - accuracy: 0.9030\n",
      "10000/10000 [==============================] - 0s 37us/sample - loss: 0.0149 - accuracy: 0.9028\n",
      "10000/10000 [==============================] - 0s 37us/sample - loss: 0.0149 - accuracy: 0.9032\n",
      "10000/10000 [==============================] - 0s 37us/sample - loss: 0.0149 - accuracy: 0.9026\n",
      "10000/10000 [==============================] - 0s 38us/sample - loss: 0.0150 - accuracy: 0.9020\n",
      "10000/10000 [==============================] - 0s 38us/sample - loss: 0.0151 - accuracy: 0.9018\n",
      "10000/10000 [==============================] - 0s 39us/sample - loss: 0.0152 - accuracy: 0.9013\n",
      "10000/10000 [==============================] - 0s 36us/sample - loss: 0.0153 - accuracy: 0.9002\n",
      "10000/10000 [==============================] - 0s 39us/sample - loss: 0.0155 - accuracy: 0.8995\n",
      "10000/10000 [==============================] - 0s 36us/sample - loss: 0.0157 - accuracy: 0.8983\n",
      "10000/10000 [==============================] - 0s 38us/sample - loss: 0.0159 - accuracy: 0.8973\n",
      "10000/10000 [==============================] - 0s 37us/sample - loss: 0.0161 - accuracy: 0.8961\n",
      "78th iteration\n",
      "10000/10000 [==============================] - 0s 36us/sample - loss: 0.0161 - accuracy: 0.8926\n",
      "10000/10000 [==============================] - 0s 36us/sample - loss: 0.0160 - accuracy: 0.8939\n",
      "10000/10000 [==============================] - 0s 40us/sample - loss: 0.0159 - accuracy: 0.8950\n",
      "10000/10000 [==============================] - 1s 59us/sample - loss: 0.0158 - accuracy: 0.8960\n",
      "10000/10000 [==============================] - 0s 37us/sample - loss: 0.0157 - accuracy: 0.8972\n",
      "10000/10000 [==============================] - 0s 36us/sample - loss: 0.0157 - accuracy: 0.8986\n",
      "10000/10000 [==============================] - 0s 36us/sample - loss: 0.0157 - accuracy: 0.8980\n",
      "10000/10000 [==============================] - 0s 37us/sample - loss: 0.0157 - accuracy: 0.8995\n",
      "10000/10000 [==============================] - 0s 38us/sample - loss: 0.0158 - accuracy: 0.8991\n",
      "10000/10000 [==============================] - 0s 40us/sample - loss: 0.0159 - accuracy: 0.8992\n",
      "10000/10000 [==============================] - 0s 36us/sample - loss: 0.0160 - accuracy: 0.8986\n",
      "10000/10000 [==============================] - 0s 36us/sample - loss: 0.0161 - accuracy: 0.8985\n",
      "10000/10000 [==============================] - 0s 39us/sample - loss: 0.0163 - accuracy: 0.8977\n",
      "10000/10000 [==============================] - 0s 38us/sample - loss: 0.0165 - accuracy: 0.8970\n",
      "10000/10000 [==============================] - 0s 35us/sample - loss: 0.0168 - accuracy: 0.8960\n",
      "10000/10000 [==============================] - 0s 37us/sample - loss: 0.0171 - accuracy: 0.8945\n",
      "10000/10000 [==============================] - 0s 36us/sample - loss: 0.0174 - accuracy: 0.8930\n",
      "10000/10000 [==============================] - 0s 36us/sample - loss: 0.0177 - accuracy: 0.8906\n",
      "10000/10000 [==============================] - 0s 35us/sample - loss: 0.0181 - accuracy: 0.8880\n",
      "10000/10000 [==============================] - 0s 34us/sample - loss: 0.0186 - accuracy: 0.8846\n",
      "10000/10000 [==============================] - 0s 35us/sample - loss: 0.0191 - accuracy: 0.8818\n",
      "79th iteration\n",
      "10000/10000 [==============================] - 0s 36us/sample - loss: 0.0168 - accuracy: 0.8870\n",
      "10000/10000 [==============================] - 0s 37us/sample - loss: 0.0167 - accuracy: 0.8884\n",
      "10000/10000 [==============================] - 0s 36us/sample - loss: 0.0167 - accuracy: 0.8888\n",
      "10000/10000 [==============================] - 0s 36us/sample - loss: 0.0167 - accuracy: 0.8900\n",
      "10000/10000 [==============================] - 0s 35us/sample - loss: 0.0168 - accuracy: 0.8911\n",
      "10000/10000 [==============================] - 0s 37us/sample - loss: 0.0168 - accuracy: 0.8913\n",
      "10000/10000 [==============================] - 1s 57us/sample - loss: 0.0169 - accuracy: 0.8913\n",
      "10000/10000 [==============================] - 0s 37us/sample - loss: 0.0171 - accuracy: 0.8909\n",
      "10000/10000 [==============================] - 0s 37us/sample - loss: 0.0172 - accuracy: 0.8912\n",
      "10000/10000 [==============================] - 0s 37us/sample - loss: 0.0175 - accuracy: 0.8914\n",
      "10000/10000 [==============================] - 0s 36us/sample - loss: 0.0177 - accuracy: 0.8914\n",
      "10000/10000 [==============================] - 0s 37us/sample - loss: 0.0180 - accuracy: 0.8911\n",
      "10000/10000 [==============================] - 0s 37us/sample - loss: 0.0183 - accuracy: 0.8903\n",
      "10000/10000 [==============================] - 0s 35us/sample - loss: 0.0187 - accuracy: 0.8881\n",
      "10000/10000 [==============================] - 0s 39us/sample - loss: 0.0191 - accuracy: 0.8874\n",
      "10000/10000 [==============================] - 0s 36us/sample - loss: 0.0195 - accuracy: 0.8855\n",
      "10000/10000 [==============================] - 0s 35us/sample - loss: 0.0200 - accuracy: 0.8846\n",
      "10000/10000 [==============================] - 0s 39us/sample - loss: 0.0206 - accuracy: 0.8820\n",
      "10000/10000 [==============================] - 0s 34us/sample - loss: 0.0211 - accuracy: 0.8787\n",
      "10000/10000 [==============================] - 0s 36us/sample - loss: 0.0217 - accuracy: 0.8757\n",
      "10000/10000 [==============================] - 0s 36us/sample - loss: 0.0224 - accuracy: 0.8711\n",
      "80th iteration\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000/10000 [==============================] - 0s 36us/sample - loss: 0.0163 - accuracy: 0.8942\n",
      "10000/10000 [==============================] - 0s 34us/sample - loss: 0.0163 - accuracy: 0.8938\n",
      "10000/10000 [==============================] - 0s 37us/sample - loss: 0.0164 - accuracy: 0.8932\n",
      "10000/10000 [==============================] - 0s 36us/sample - loss: 0.0166 - accuracy: 0.8930\n",
      "10000/10000 [==============================] - 0s 38us/sample - loss: 0.0167 - accuracy: 0.8931\n",
      "10000/10000 [==============================] - 0s 39us/sample - loss: 0.0169 - accuracy: 0.8938\n",
      "10000/10000 [==============================] - 0s 36us/sample - loss: 0.0172 - accuracy: 0.8939\n",
      "10000/10000 [==============================] - 0s 36us/sample - loss: 0.0175 - accuracy: 0.8928\n",
      "10000/10000 [==============================] - 0s 36us/sample - loss: 0.0178 - accuracy: 0.8918\n",
      "10000/10000 [==============================] - 0s 35us/sample - loss: 0.0182 - accuracy: 0.8907\n",
      "10000/10000 [==============================] - 0s 34us/sample - loss: 0.0187 - accuracy: 0.8893\n",
      "10000/10000 [==============================] - 0s 36us/sample - loss: 0.0192 - accuracy: 0.8880\n",
      "10000/10000 [==============================] - 1s 59us/sample - loss: 0.0198 - accuracy: 0.8867\n",
      "10000/10000 [==============================] - 0s 36us/sample - loss: 0.0204 - accuracy: 0.8858\n",
      "10000/10000 [==============================] - 0s 35us/sample - loss: 0.0211 - accuracy: 0.8834\n",
      "10000/10000 [==============================] - 0s 37us/sample - loss: 0.0218 - accuracy: 0.8820\n",
      "10000/10000 [==============================] - 0s 37us/sample - loss: 0.0226 - accuracy: 0.8773\n",
      "10000/10000 [==============================] - 0s 36us/sample - loss: 0.0234 - accuracy: 0.8733\n",
      "10000/10000 [==============================] - 0s 38us/sample - loss: 0.0243 - accuracy: 0.8693\n",
      "10000/10000 [==============================] - 0s 35us/sample - loss: 0.0253 - accuracy: 0.8652\n",
      "10000/10000 [==============================] - 0s 34us/sample - loss: 0.0263 - accuracy: 0.8606\n",
      "81th iteration\n",
      "10000/10000 [==============================] - 0s 36us/sample - loss: 0.0168 - accuracy: 0.8870\n",
      "10000/10000 [==============================] - 0s 38us/sample - loss: 0.0165 - accuracy: 0.8897\n",
      "10000/10000 [==============================] - 0s 37us/sample - loss: 0.0163 - accuracy: 0.8925\n",
      "10000/10000 [==============================] - 0s 37us/sample - loss: 0.0160 - accuracy: 0.8940\n",
      "10000/10000 [==============================] - 0s 34us/sample - loss: 0.0158 - accuracy: 0.8959\n",
      "10000/10000 [==============================] - 0s 36us/sample - loss: 0.0157 - accuracy: 0.8968\n",
      "10000/10000 [==============================] - 0s 36us/sample - loss: 0.0155 - accuracy: 0.8988\n",
      "10000/10000 [==============================] - 0s 36us/sample - loss: 0.0154 - accuracy: 0.8996\n",
      "10000/10000 [==============================] - 0s 38us/sample - loss: 0.0153 - accuracy: 0.9005\n",
      "10000/10000 [==============================] - 0s 36us/sample - loss: 0.0152 - accuracy: 0.9020\n",
      "10000/10000 [==============================] - 0s 35us/sample - loss: 0.0151 - accuracy: 0.9033\n",
      "10000/10000 [==============================] - 0s 35us/sample - loss: 0.0151 - accuracy: 0.9025\n",
      "10000/10000 [==============================] - 0s 34us/sample - loss: 0.0151 - accuracy: 0.9028\n",
      "10000/10000 [==============================] - 0s 34us/sample - loss: 0.0151 - accuracy: 0.9025\n",
      "10000/10000 [==============================] - 0s 35us/sample - loss: 0.0151 - accuracy: 0.9018\n",
      "10000/10000 [==============================] - 0s 35us/sample - loss: 0.0152 - accuracy: 0.9003\n",
      "10000/10000 [==============================] - 0s 35us/sample - loss: 0.0153 - accuracy: 0.8993\n",
      "10000/10000 [==============================] - 0s 37us/sample - loss: 0.0154 - accuracy: 0.8979\n",
      "10000/10000 [==============================] - 0s 37us/sample - loss: 0.0155 - accuracy: 0.8968\n",
      "10000/10000 [==============================] - 0s 38us/sample - loss: 0.0157 - accuracy: 0.8957\n",
      "10000/10000 [==============================] - 0s 38us/sample - loss: 0.0159 - accuracy: 0.8943\n",
      "82th iteration\n",
      "10000/10000 [==============================] - 0s 38us/sample - loss: 0.0151 - accuracy: 0.9033\n",
      "10000/10000 [==============================] - 0s 36us/sample - loss: 0.0150 - accuracy: 0.9039\n",
      "10000/10000 [==============================] - 0s 37us/sample - loss: 0.0148 - accuracy: 0.9047\n",
      "10000/10000 [==============================] - 0s 37us/sample - loss: 0.0147 - accuracy: 0.9055\n",
      "10000/10000 [==============================] - 0s 37us/sample - loss: 0.0147 - accuracy: 0.9064\n",
      "10000/10000 [==============================] - 0s 36us/sample - loss: 0.0146 - accuracy: 0.9070\n",
      "10000/10000 [==============================] - 0s 35us/sample - loss: 0.0146 - accuracy: 0.9070\n",
      "10000/10000 [==============================] - 0s 37us/sample - loss: 0.0145 - accuracy: 0.9071\n",
      "10000/10000 [==============================] - 0s 36us/sample - loss: 0.0145 - accuracy: 0.9058\n",
      "10000/10000 [==============================] - 0s 37us/sample - loss: 0.0146 - accuracy: 0.9058\n",
      "10000/10000 [==============================] - 0s 35us/sample - loss: 0.0146 - accuracy: 0.9059\n",
      "10000/10000 [==============================] - 0s 37us/sample - loss: 0.0147 - accuracy: 0.9057\n",
      "10000/10000 [==============================] - 0s 41us/sample - loss: 0.0147 - accuracy: 0.9046\n",
      "10000/10000 [==============================] - 0s 36us/sample - loss: 0.0148 - accuracy: 0.9038\n",
      "10000/10000 [==============================] - 0s 37us/sample - loss: 0.0150 - accuracy: 0.9026\n",
      "10000/10000 [==============================] - 0s 35us/sample - loss: 0.0151 - accuracy: 0.9023\n",
      "10000/10000 [==============================] - 0s 37us/sample - loss: 0.0153 - accuracy: 0.9011\n",
      "10000/10000 [==============================] - 0s 36us/sample - loss: 0.0155 - accuracy: 0.8995\n",
      "10000/10000 [==============================] - 0s 36us/sample - loss: 0.0157 - accuracy: 0.8989\n",
      "10000/10000 [==============================] - 0s 35us/sample - loss: 0.0159 - accuracy: 0.8975\n",
      "10000/10000 [==============================] - 0s 36us/sample - loss: 0.0161 - accuracy: 0.8961\n",
      "83th iteration\n",
      "10000/10000 [==============================] - 0s 37us/sample - loss: 0.0161 - accuracy: 0.8961\n",
      "10000/10000 [==============================] - 0s 35us/sample - loss: 0.0162 - accuracy: 0.8960\n",
      "10000/10000 [==============================] - 0s 36us/sample - loss: 0.0163 - accuracy: 0.8967\n",
      "10000/10000 [==============================] - 0s 34us/sample - loss: 0.0165 - accuracy: 0.8962\n",
      "10000/10000 [==============================] - 0s 36us/sample - loss: 0.0166 - accuracy: 0.8956\n",
      "10000/10000 [==============================] - 0s 38us/sample - loss: 0.0168 - accuracy: 0.8959\n",
      "10000/10000 [==============================] - 0s 35us/sample - loss: 0.0171 - accuracy: 0.8963\n",
      "10000/10000 [==============================] - 0s 36us/sample - loss: 0.0174 - accuracy: 0.8948\n",
      "10000/10000 [==============================] - 0s 34us/sample - loss: 0.0178 - accuracy: 0.8936\n",
      "10000/10000 [==============================] - 0s 35us/sample - loss: 0.0182 - accuracy: 0.8928\n",
      "10000/10000 [==============================] - 0s 38us/sample - loss: 0.0187 - accuracy: 0.8921\n",
      "10000/10000 [==============================] - 0s 36us/sample - loss: 0.0192 - accuracy: 0.8897\n",
      "10000/10000 [==============================] - 0s 35us/sample - loss: 0.0197 - accuracy: 0.8882\n",
      "10000/10000 [==============================] - 0s 34us/sample - loss: 0.0204 - accuracy: 0.8864\n",
      "10000/10000 [==============================] - 0s 35us/sample - loss: 0.0210 - accuracy: 0.8834\n",
      "10000/10000 [==============================] - 0s 36us/sample - loss: 0.0218 - accuracy: 0.8815\n",
      "10000/10000 [==============================] - 0s 34us/sample - loss: 0.0226 - accuracy: 0.8786\n",
      "10000/10000 [==============================] - 0s 38us/sample - loss: 0.0234 - accuracy: 0.8745\n",
      "10000/10000 [==============================] - 0s 36us/sample - loss: 0.0243 - accuracy: 0.8708\n",
      "10000/10000 [==============================] - 0s 37us/sample - loss: 0.0253 - accuracy: 0.8659\n",
      "10000/10000 [==============================] - 0s 36us/sample - loss: 0.0263 - accuracy: 0.8606\n",
      "84th iteration\n",
      "10000/10000 [==============================] - 0s 36us/sample - loss: 0.0168 - accuracy: 0.8870\n",
      "10000/10000 [==============================] - 0s 35us/sample - loss: 0.0165 - accuracy: 0.8891\n",
      "10000/10000 [==============================] - 0s 36us/sample - loss: 0.0162 - accuracy: 0.8919\n",
      "10000/10000 [==============================] - 0s 36us/sample - loss: 0.0160 - accuracy: 0.8946\n",
      "10000/10000 [==============================] - 0s 35us/sample - loss: 0.0158 - accuracy: 0.8963\n",
      "10000/10000 [==============================] - 0s 36us/sample - loss: 0.0156 - accuracy: 0.8980\n",
      "10000/10000 [==============================] - 0s 35us/sample - loss: 0.0155 - accuracy: 0.8993\n",
      "10000/10000 [==============================] - 1s 58us/sample - loss: 0.0153 - accuracy: 0.8991\n",
      "10000/10000 [==============================] - 0s 35us/sample - loss: 0.0153 - accuracy: 0.9005\n",
      "10000/10000 [==============================] - 0s 38us/sample - loss: 0.0152 - accuracy: 0.9001\n",
      "10000/10000 [==============================] - 0s 36us/sample - loss: 0.0151 - accuracy: 0.9004\n",
      "10000/10000 [==============================] - 0s 35us/sample - loss: 0.0151 - accuracy: 0.9010\n",
      "10000/10000 [==============================] - 0s 35us/sample - loss: 0.0151 - accuracy: 0.9007\n",
      "10000/10000 [==============================] - 0s 37us/sample - loss: 0.0152 - accuracy: 0.9009\n",
      "10000/10000 [==============================] - 0s 39us/sample - loss: 0.0153 - accuracy: 0.9006\n",
      "10000/10000 [==============================] - 0s 40us/sample - loss: 0.0154 - accuracy: 0.9001\n",
      "10000/10000 [==============================] - 0s 39us/sample - loss: 0.0155 - accuracy: 0.8990\n",
      "10000/10000 [==============================] - 0s 38us/sample - loss: 0.0156 - accuracy: 0.8983\n",
      "10000/10000 [==============================] - 0s 39us/sample - loss: 0.0158 - accuracy: 0.8967\n",
      "10000/10000 [==============================] - 0s 38us/sample - loss: 0.0160 - accuracy: 0.8944\n",
      "10000/10000 [==============================] - 0s 37us/sample - loss: 0.0163 - accuracy: 0.8942\n",
      "85th iteration\n",
      "10000/10000 [==============================] - 0s 39us/sample - loss: 0.0161 - accuracy: 0.8926\n",
      "10000/10000 [==============================] - 0s 37us/sample - loss: 0.0159 - accuracy: 0.8951\n",
      "10000/10000 [==============================] - 0s 38us/sample - loss: 0.0157 - accuracy: 0.8964\n",
      "10000/10000 [==============================] - 0s 38us/sample - loss: 0.0155 - accuracy: 0.8980\n",
      "10000/10000 [==============================] - 0s 36us/sample - loss: 0.0154 - accuracy: 0.8989\n",
      "10000/10000 [==============================] - 0s 38us/sample - loss: 0.0152 - accuracy: 0.8998\n",
      "10000/10000 [==============================] - 0s 36us/sample - loss: 0.0151 - accuracy: 0.9009\n",
      "10000/10000 [==============================] - 0s 38us/sample - loss: 0.0150 - accuracy: 0.9011\n",
      "10000/10000 [==============================] - 0s 39us/sample - loss: 0.0150 - accuracy: 0.9020\n",
      "10000/10000 [==============================] - 0s 37us/sample - loss: 0.0149 - accuracy: 0.9030\n",
      "10000/10000 [==============================] - 1s 59us/sample - loss: 0.0149 - accuracy: 0.9036\n",
      "10000/10000 [==============================] - 0s 38us/sample - loss: 0.0149 - accuracy: 0.9036\n",
      "10000/10000 [==============================] - 0s 37us/sample - loss: 0.0150 - accuracy: 0.9042\n",
      "10000/10000 [==============================] - 0s 36us/sample - loss: 0.0150 - accuracy: 0.9037\n",
      "10000/10000 [==============================] - 0s 37us/sample - loss: 0.0151 - accuracy: 0.9022\n",
      "10000/10000 [==============================] - 0s 38us/sample - loss: 0.0153 - accuracy: 0.9021\n",
      "10000/10000 [==============================] - 0s 37us/sample - loss: 0.0154 - accuracy: 0.9007\n",
      "10000/10000 [==============================] - 0s 36us/sample - loss: 0.0156 - accuracy: 0.8992\n",
      "10000/10000 [==============================] - 0s 37us/sample - loss: 0.0158 - accuracy: 0.8979\n",
      "10000/10000 [==============================] - 0s 37us/sample - loss: 0.0160 - accuracy: 0.8958\n",
      "10000/10000 [==============================] - 0s 36us/sample - loss: 0.0163 - accuracy: 0.8942\n",
      "86th iteration\n",
      "10000/10000 [==============================] - 0s 36us/sample - loss: 0.0168 - accuracy: 0.8870\n",
      "10000/10000 [==============================] - 0s 38us/sample - loss: 0.0165 - accuracy: 0.8892\n",
      "10000/10000 [==============================] - 0s 38us/sample - loss: 0.0163 - accuracy: 0.8925\n",
      "10000/10000 [==============================] - 0s 39us/sample - loss: 0.0160 - accuracy: 0.8938\n",
      "10000/10000 [==============================] - 0s 37us/sample - loss: 0.0158 - accuracy: 0.8951\n",
      "10000/10000 [==============================] - 0s 37us/sample - loss: 0.0156 - accuracy: 0.8965\n",
      "10000/10000 [==============================] - 0s 38us/sample - loss: 0.0155 - accuracy: 0.8985\n",
      "10000/10000 [==============================] - 0s 36us/sample - loss: 0.0153 - accuracy: 0.9006\n",
      "10000/10000 [==============================] - 0s 39us/sample - loss: 0.0152 - accuracy: 0.9014\n",
      "10000/10000 [==============================] - 0s 37us/sample - loss: 0.0151 - accuracy: 0.9021\n",
      "10000/10000 [==============================] - 0s 38us/sample - loss: 0.0150 - accuracy: 0.9026\n",
      "10000/10000 [==============================] - 0s 37us/sample - loss: 0.0149 - accuracy: 0.9030\n",
      "10000/10000 [==============================] - 0s 38us/sample - loss: 0.0149 - accuracy: 0.9029\n",
      "10000/10000 [==============================] - 0s 40us/sample - loss: 0.0148 - accuracy: 0.9034\n",
      "10000/10000 [==============================] - 0s 39us/sample - loss: 0.0148 - accuracy: 0.9035\n",
      "10000/10000 [==============================] - 0s 39us/sample - loss: 0.0148 - accuracy: 0.9036\n",
      "10000/10000 [==============================] - 0s 39us/sample - loss: 0.0148 - accuracy: 0.9044\n",
      "10000/10000 [==============================] - 0s 41us/sample - loss: 0.0149 - accuracy: 0.9050\n",
      "10000/10000 [==============================] - 0s 38us/sample - loss: 0.0149 - accuracy: 0.9040\n",
      "10000/10000 [==============================] - 0s 36us/sample - loss: 0.0150 - accuracy: 0.9035\n",
      "10000/10000 [==============================] - 0s 38us/sample - loss: 0.0151 - accuracy: 0.9033\n",
      "87th iteration\n",
      "10000/10000 [==============================] - 0s 38us/sample - loss: 0.0161 - accuracy: 0.8926\n",
      "10000/10000 [==============================] - 0s 38us/sample - loss: 0.0160 - accuracy: 0.8933\n",
      "10000/10000 [==============================] - 0s 39us/sample - loss: 0.0160 - accuracy: 0.8939\n",
      "10000/10000 [==============================] - 0s 38us/sample - loss: 0.0160 - accuracy: 0.8937\n",
      "10000/10000 [==============================] - 0s 38us/sample - loss: 0.0160 - accuracy: 0.8949\n",
      "10000/10000 [==============================] - 0s 38us/sample - loss: 0.0160 - accuracy: 0.8957\n",
      "10000/10000 [==============================] - 0s 39us/sample - loss: 0.0160 - accuracy: 0.8964\n",
      "10000/10000 [==============================] - 0s 36us/sample - loss: 0.0161 - accuracy: 0.8959\n",
      "10000/10000 [==============================] - 0s 38us/sample - loss: 0.0162 - accuracy: 0.8956\n",
      "10000/10000 [==============================] - 0s 38us/sample - loss: 0.0164 - accuracy: 0.8959\n",
      "10000/10000 [==============================] - 0s 38us/sample - loss: 0.0166 - accuracy: 0.8953\n",
      "10000/10000 [==============================] - 0s 38us/sample - loss: 0.0168 - accuracy: 0.8950\n",
      "10000/10000 [==============================] - 0s 37us/sample - loss: 0.0170 - accuracy: 0.8957\n",
      "10000/10000 [==============================] - 0s 41us/sample - loss: 0.0173 - accuracy: 0.8946\n",
      "10000/10000 [==============================] - 0s 38us/sample - loss: 0.0177 - accuracy: 0.8951\n",
      "10000/10000 [==============================] - 0s 36us/sample - loss: 0.0180 - accuracy: 0.8929\n",
      "10000/10000 [==============================] - 0s 38us/sample - loss: 0.0185 - accuracy: 0.8908\n",
      "10000/10000 [==============================] - 0s 41us/sample - loss: 0.0189 - accuracy: 0.8894\n",
      "10000/10000 [==============================] - 0s 37us/sample - loss: 0.0194 - accuracy: 0.8868\n",
      "10000/10000 [==============================] - 0s 39us/sample - loss: 0.0200 - accuracy: 0.8844\n",
      "10000/10000 [==============================] - 0s 39us/sample - loss: 0.0206 - accuracy: 0.8813\n",
      "88th iteration\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000/10000 [==============================] - 0s 38us/sample - loss: 0.0161 - accuracy: 0.8926\n",
      "10000/10000 [==============================] - 0s 39us/sample - loss: 0.0159 - accuracy: 0.8943\n",
      "10000/10000 [==============================] - 0s 37us/sample - loss: 0.0157 - accuracy: 0.8959\n",
      "10000/10000 [==============================] - 0s 39us/sample - loss: 0.0155 - accuracy: 0.8981\n",
      "10000/10000 [==============================] - 0s 36us/sample - loss: 0.0153 - accuracy: 0.9000\n",
      "10000/10000 [==============================] - 0s 36us/sample - loss: 0.0151 - accuracy: 0.9008\n",
      "10000/10000 [==============================] - 0s 38us/sample - loss: 0.0150 - accuracy: 0.9025\n",
      "10000/10000 [==============================] - 0s 37us/sample - loss: 0.0149 - accuracy: 0.9041\n",
      "10000/10000 [==============================] - 0s 38us/sample - loss: 0.0148 - accuracy: 0.9040\n",
      "10000/10000 [==============================] - 0s 36us/sample - loss: 0.0148 - accuracy: 0.9048\n",
      "10000/10000 [==============================] - 0s 38us/sample - loss: 0.0147 - accuracy: 0.9046\n",
      "10000/10000 [==============================] - 0s 38us/sample - loss: 0.0148 - accuracy: 0.9052\n",
      "10000/10000 [==============================] - 0s 38us/sample - loss: 0.0148 - accuracy: 0.9045\n",
      "10000/10000 [==============================] - 0s 35us/sample - loss: 0.0149 - accuracy: 0.9047\n",
      "10000/10000 [==============================] - 0s 36us/sample - loss: 0.0149 - accuracy: 0.9036\n",
      "10000/10000 [==============================] - 0s 38us/sample - loss: 0.0151 - accuracy: 0.9020\n",
      "10000/10000 [==============================] - 0s 38us/sample - loss: 0.0152 - accuracy: 0.9011\n",
      "10000/10000 [==============================] - 0s 35us/sample - loss: 0.0154 - accuracy: 0.9005\n",
      "10000/10000 [==============================] - 0s 38us/sample - loss: 0.0156 - accuracy: 0.8982\n",
      "10000/10000 [==============================] - 0s 35us/sample - loss: 0.0159 - accuracy: 0.8972\n",
      "10000/10000 [==============================] - 0s 39us/sample - loss: 0.0161 - accuracy: 0.8961\n",
      "89th iteration\n",
      "10000/10000 [==============================] - 0s 39us/sample - loss: 0.0167 - accuracy: 0.8959\n",
      "10000/10000 [==============================] - 0s 37us/sample - loss: 0.0169 - accuracy: 0.8952\n",
      "10000/10000 [==============================] - 0s 36us/sample - loss: 0.0172 - accuracy: 0.8953\n",
      "10000/10000 [==============================] - 0s 35us/sample - loss: 0.0176 - accuracy: 0.8948\n",
      "10000/10000 [==============================] - 0s 34us/sample - loss: 0.0180 - accuracy: 0.8935\n",
      "10000/10000 [==============================] - 0s 35us/sample - loss: 0.0185 - accuracy: 0.8931\n",
      "10000/10000 [==============================] - 0s 35us/sample - loss: 0.0191 - accuracy: 0.8927\n",
      "10000/10000 [==============================] - 0s 35us/sample - loss: 0.0198 - accuracy: 0.8914\n",
      "10000/10000 [==============================] - 0s 34us/sample - loss: 0.0205 - accuracy: 0.8884\n",
      "10000/10000 [==============================] - 0s 37us/sample - loss: 0.0213 - accuracy: 0.8854\n",
      "10000/10000 [==============================] - 0s 34us/sample - loss: 0.0223 - accuracy: 0.8820\n",
      "10000/10000 [==============================] - 0s 35us/sample - loss: 0.0233 - accuracy: 0.8789\n",
      "10000/10000 [==============================] - 0s 34us/sample - loss: 0.0243 - accuracy: 0.8742\n",
      "10000/10000 [==============================] - 0s 36us/sample - loss: 0.0255 - accuracy: 0.8691\n",
      "10000/10000 [==============================] - 0s 36us/sample - loss: 0.0267 - accuracy: 0.8601\n",
      "10000/10000 [==============================] - 0s 35us/sample - loss: 0.0281 - accuracy: 0.8530\n",
      "10000/10000 [==============================] - 0s 35us/sample - loss: 0.0294 - accuracy: 0.8465\n",
      "10000/10000 [==============================] - 0s 34us/sample - loss: 0.0309 - accuracy: 0.8370\n",
      "10000/10000 [==============================] - 0s 37us/sample - loss: 0.0324 - accuracy: 0.8260\n",
      "10000/10000 [==============================] - 0s 37us/sample - loss: 0.0339 - accuracy: 0.8151\n",
      "10000/10000 [==============================] - 0s 37us/sample - loss: 0.0355 - accuracy: 0.8016\n",
      "90th iteration\n",
      "10000/10000 [==============================] - 0s 37us/sample - loss: 0.0161 - accuracy: 0.8926\n",
      "10000/10000 [==============================] - 0s 36us/sample - loss: 0.0159 - accuracy: 0.8950\n",
      "10000/10000 [==============================] - 0s 36us/sample - loss: 0.0157 - accuracy: 0.8969\n",
      "10000/10000 [==============================] - 0s 36us/sample - loss: 0.0155 - accuracy: 0.8976\n",
      "10000/10000 [==============================] - 0s 34us/sample - loss: 0.0154 - accuracy: 0.8981\n",
      "10000/10000 [==============================] - 0s 36us/sample - loss: 0.0152 - accuracy: 0.8991\n",
      "10000/10000 [==============================] - 0s 35us/sample - loss: 0.0151 - accuracy: 0.9007\n",
      "10000/10000 [==============================] - 0s 36us/sample - loss: 0.0150 - accuracy: 0.9018\n",
      "10000/10000 [==============================] - 0s 35us/sample - loss: 0.0150 - accuracy: 0.9020\n",
      "10000/10000 [==============================] - 0s 36us/sample - loss: 0.0149 - accuracy: 0.9025\n",
      "10000/10000 [==============================] - 0s 35us/sample - loss: 0.0149 - accuracy: 0.9030\n",
      "10000/10000 [==============================] - 1s 58us/sample - loss: 0.0149 - accuracy: 0.9029\n",
      "10000/10000 [==============================] - 0s 37us/sample - loss: 0.0149 - accuracy: 0.9024\n",
      "10000/10000 [==============================] - 0s 36us/sample - loss: 0.0150 - accuracy: 0.9026\n",
      "10000/10000 [==============================] - 0s 36us/sample - loss: 0.0150 - accuracy: 0.9014\n",
      "10000/10000 [==============================] - 0s 36us/sample - loss: 0.0151 - accuracy: 0.9010\n",
      "10000/10000 [==============================] - 0s 37us/sample - loss: 0.0152 - accuracy: 0.8998\n",
      "10000/10000 [==============================] - 0s 38us/sample - loss: 0.0153 - accuracy: 0.8989\n",
      "10000/10000 [==============================] - 0s 36us/sample - loss: 0.0155 - accuracy: 0.8981\n",
      "10000/10000 [==============================] - 0s 37us/sample - loss: 0.0157 - accuracy: 0.8961\n",
      "10000/10000 [==============================] - 0s 34us/sample - loss: 0.0159 - accuracy: 0.8943\n",
      "91th iteration\n",
      "10000/10000 [==============================] - 0s 36us/sample - loss: 0.0161 - accuracy: 0.8926\n",
      "10000/10000 [==============================] - 0s 35us/sample - loss: 0.0159 - accuracy: 0.8944\n",
      "10000/10000 [==============================] - 0s 36us/sample - loss: 0.0157 - accuracy: 0.8956\n",
      "10000/10000 [==============================] - 0s 37us/sample - loss: 0.0155 - accuracy: 0.8969\n",
      "10000/10000 [==============================] - 0s 35us/sample - loss: 0.0154 - accuracy: 0.8974\n",
      "10000/10000 [==============================] - 0s 37us/sample - loss: 0.0153 - accuracy: 0.8995\n",
      "10000/10000 [==============================] - 0s 36us/sample - loss: 0.0152 - accuracy: 0.9007\n",
      "10000/10000 [==============================] - 0s 36us/sample - loss: 0.0151 - accuracy: 0.9010\n",
      "10000/10000 [==============================] - 0s 36us/sample - loss: 0.0150 - accuracy: 0.9019\n",
      "10000/10000 [==============================] - 0s 35us/sample - loss: 0.0150 - accuracy: 0.9027\n",
      "10000/10000 [==============================] - 0s 36us/sample - loss: 0.0150 - accuracy: 0.9025\n",
      "10000/10000 [==============================] - 0s 38us/sample - loss: 0.0150 - accuracy: 0.9017\n",
      "10000/10000 [==============================] - 0s 35us/sample - loss: 0.0151 - accuracy: 0.9019\n",
      "10000/10000 [==============================] - 0s 35us/sample - loss: 0.0152 - accuracy: 0.9021\n",
      "10000/10000 [==============================] - 1s 58us/sample - loss: 0.0153 - accuracy: 0.9025\n",
      "10000/10000 [==============================] - 0s 35us/sample - loss: 0.0155 - accuracy: 0.9017\n",
      "10000/10000 [==============================] - 0s 38us/sample - loss: 0.0156 - accuracy: 0.9016\n",
      "10000/10000 [==============================] - 0s 37us/sample - loss: 0.0158 - accuracy: 0.9007\n",
      "10000/10000 [==============================] - 0s 34us/sample - loss: 0.0161 - accuracy: 0.9000\n",
      "10000/10000 [==============================] - 0s 37us/sample - loss: 0.0164 - accuracy: 0.8972\n",
      "10000/10000 [==============================] - 0s 34us/sample - loss: 0.0167 - accuracy: 0.8959\n",
      "92th iteration\n",
      "10000/10000 [==============================] - 0s 36us/sample - loss: 0.0161 - accuracy: 0.8926\n",
      "10000/10000 [==============================] - 0s 36us/sample - loss: 0.0161 - accuracy: 0.8925\n",
      "10000/10000 [==============================] - 0s 36us/sample - loss: 0.0161 - accuracy: 0.8926\n",
      "10000/10000 [==============================] - 0s 36us/sample - loss: 0.0161 - accuracy: 0.8925\n",
      "10000/10000 [==============================] - 0s 36us/sample - loss: 0.0162 - accuracy: 0.8927\n",
      "10000/10000 [==============================] - 0s 37us/sample - loss: 0.0163 - accuracy: 0.8926\n",
      "10000/10000 [==============================] - 0s 34us/sample - loss: 0.0164 - accuracy: 0.8919\n",
      "10000/10000 [==============================] - 0s 36us/sample - loss: 0.0165 - accuracy: 0.8923\n",
      "10000/10000 [==============================] - 0s 38us/sample - loss: 0.0167 - accuracy: 0.8921\n",
      "10000/10000 [==============================] - 0s 36us/sample - loss: 0.0170 - accuracy: 0.8917\n",
      "10000/10000 [==============================] - 0s 36us/sample - loss: 0.0172 - accuracy: 0.8899\n",
      "10000/10000 [==============================] - 0s 36us/sample - loss: 0.0175 - accuracy: 0.8893\n",
      "10000/10000 [==============================] - 0s 36us/sample - loss: 0.0179 - accuracy: 0.8888\n",
      "10000/10000 [==============================] - 0s 36us/sample - loss: 0.0183 - accuracy: 0.8876\n",
      "10000/10000 [==============================] - 0s 34us/sample - loss: 0.0187 - accuracy: 0.8857\n",
      "10000/10000 [==============================] - 0s 37us/sample - loss: 0.0192 - accuracy: 0.8845\n",
      "10000/10000 [==============================] - 0s 35us/sample - loss: 0.0198 - accuracy: 0.8822\n",
      "10000/10000 [==============================] - 0s 35us/sample - loss: 0.0203 - accuracy: 0.8798\n",
      "10000/10000 [==============================] - 0s 36us/sample - loss: 0.0210 - accuracy: 0.8776\n",
      "10000/10000 [==============================] - 0s 36us/sample - loss: 0.0217 - accuracy: 0.8746\n",
      "10000/10000 [==============================] - 0s 35us/sample - loss: 0.0224 - accuracy: 0.8711\n",
      "93th iteration\n",
      "10000/10000 [==============================] - 0s 36us/sample - loss: 0.0161 - accuracy: 0.8926\n",
      "10000/10000 [==============================] - 0s 36us/sample - loss: 0.0159 - accuracy: 0.8946\n",
      "10000/10000 [==============================] - 0s 35us/sample - loss: 0.0156 - accuracy: 0.8969\n",
      "10000/10000 [==============================] - 0s 36us/sample - loss: 0.0154 - accuracy: 0.8984\n",
      "10000/10000 [==============================] - 0s 36us/sample - loss: 0.0152 - accuracy: 0.9006\n",
      "10000/10000 [==============================] - 0s 37us/sample - loss: 0.0150 - accuracy: 0.9028\n",
      "10000/10000 [==============================] - 0s 35us/sample - loss: 0.0148 - accuracy: 0.9039\n",
      "10000/10000 [==============================] - 0s 36us/sample - loss: 0.0147 - accuracy: 0.9043\n",
      "10000/10000 [==============================] - 0s 36us/sample - loss: 0.0145 - accuracy: 0.9058\n",
      "10000/10000 [==============================] - 0s 35us/sample - loss: 0.0145 - accuracy: 0.9065\n",
      "10000/10000 [==============================] - 0s 37us/sample - loss: 0.0144 - accuracy: 0.9078\n",
      "10000/10000 [==============================] - 0s 36us/sample - loss: 0.0143 - accuracy: 0.9077\n",
      "10000/10000 [==============================] - 0s 36us/sample - loss: 0.0143 - accuracy: 0.9074\n",
      "10000/10000 [==============================] - 0s 36us/sample - loss: 0.0143 - accuracy: 0.9068\n",
      "10000/10000 [==============================] - 0s 34us/sample - loss: 0.0144 - accuracy: 0.9069\n",
      "10000/10000 [==============================] - 0s 36us/sample - loss: 0.0144 - accuracy: 0.9076\n",
      "10000/10000 [==============================] - 0s 35us/sample - loss: 0.0145 - accuracy: 0.9067\n",
      "10000/10000 [==============================] - 0s 34us/sample - loss: 0.0146 - accuracy: 0.9061\n",
      "10000/10000 [==============================] - 0s 35us/sample - loss: 0.0148 - accuracy: 0.9049\n",
      "10000/10000 [==============================] - 0s 34us/sample - loss: 0.0149 - accuracy: 0.9035\n",
      "10000/10000 [==============================] - 0s 36us/sample - loss: 0.0151 - accuracy: 0.9033\n",
      "94th iteration\n",
      "10000/10000 [==============================] - 0s 35us/sample - loss: 0.0156 - accuracy: 0.8985\n",
      "10000/10000 [==============================] - 0s 36us/sample - loss: 0.0154 - accuracy: 0.8993\n",
      "10000/10000 [==============================] - 0s 36us/sample - loss: 0.0153 - accuracy: 0.9002\n",
      "10000/10000 [==============================] - 0s 36us/sample - loss: 0.0152 - accuracy: 0.9006\n",
      "10000/10000 [==============================] - 0s 37us/sample - loss: 0.0152 - accuracy: 0.9012\n",
      "10000/10000 [==============================] - 0s 35us/sample - loss: 0.0151 - accuracy: 0.9015\n",
      "10000/10000 [==============================] - 0s 37us/sample - loss: 0.0151 - accuracy: 0.9021\n",
      "10000/10000 [==============================] - 0s 36us/sample - loss: 0.0151 - accuracy: 0.9027\n",
      "10000/10000 [==============================] - 0s 36us/sample - loss: 0.0152 - accuracy: 0.9022\n",
      "10000/10000 [==============================] - 0s 37us/sample - loss: 0.0152 - accuracy: 0.9011\n",
      "10000/10000 [==============================] - 0s 34us/sample - loss: 0.0153 - accuracy: 0.9001\n",
      "10000/10000 [==============================] - 0s 35us/sample - loss: 0.0155 - accuracy: 0.8990\n",
      "10000/10000 [==============================] - 0s 36us/sample - loss: 0.0156 - accuracy: 0.8977\n",
      "10000/10000 [==============================] - 0s 36us/sample - loss: 0.0158 - accuracy: 0.8964\n",
      "10000/10000 [==============================] - 0s 36us/sample - loss: 0.0160 - accuracy: 0.8956\n",
      "10000/10000 [==============================] - 0s 36us/sample - loss: 0.0163 - accuracy: 0.8948\n",
      "10000/10000 [==============================] - 0s 36us/sample - loss: 0.0165 - accuracy: 0.8928\n",
      "10000/10000 [==============================] - 0s 34us/sample - loss: 0.0168 - accuracy: 0.8918\n",
      "10000/10000 [==============================] - 0s 37us/sample - loss: 0.0171 - accuracy: 0.8909\n",
      "10000/10000 [==============================] - 0s 36us/sample - loss: 0.0175 - accuracy: 0.8867\n",
      "10000/10000 [==============================] - 0s 36us/sample - loss: 0.0179 - accuracy: 0.8823\n",
      "95th iteration\n",
      "10000/10000 [==============================] - 0s 35us/sample - loss: 0.0159 - accuracy: 0.8943\n",
      "10000/10000 [==============================] - 0s 35us/sample - loss: 0.0159 - accuracy: 0.8945\n",
      "10000/10000 [==============================] - 0s 36us/sample - loss: 0.0160 - accuracy: 0.8949\n",
      "10000/10000 [==============================] - 0s 34us/sample - loss: 0.0161 - accuracy: 0.8947\n",
      "10000/10000 [==============================] - 0s 36us/sample - loss: 0.0163 - accuracy: 0.8945\n",
      "10000/10000 [==============================] - 0s 35us/sample - loss: 0.0165 - accuracy: 0.8949\n",
      "10000/10000 [==============================] - 0s 34us/sample - loss: 0.0168 - accuracy: 0.8946\n",
      "10000/10000 [==============================] - 0s 35us/sample - loss: 0.0170 - accuracy: 0.8946\n",
      "10000/10000 [==============================] - 0s 34us/sample - loss: 0.0174 - accuracy: 0.8933\n",
      "10000/10000 [==============================] - 0s 36us/sample - loss: 0.0178 - accuracy: 0.8927\n",
      "10000/10000 [==============================] - 0s 36us/sample - loss: 0.0182 - accuracy: 0.8920\n",
      "10000/10000 [==============================] - 0s 36us/sample - loss: 0.0188 - accuracy: 0.8914\n",
      "10000/10000 [==============================] - 0s 35us/sample - loss: 0.0193 - accuracy: 0.8894\n",
      "10000/10000 [==============================] - 0s 36us/sample - loss: 0.0200 - accuracy: 0.8872\n",
      "10000/10000 [==============================] - 0s 34us/sample - loss: 0.0207 - accuracy: 0.8853\n",
      "10000/10000 [==============================] - 0s 34us/sample - loss: 0.0214 - accuracy: 0.8815\n",
      "10000/10000 [==============================] - 0s 37us/sample - loss: 0.0223 - accuracy: 0.8781\n",
      "10000/10000 [==============================] - 0s 36us/sample - loss: 0.0232 - accuracy: 0.8737\n",
      "10000/10000 [==============================] - 0s 35us/sample - loss: 0.0241 - accuracy: 0.8710\n",
      "10000/10000 [==============================] - 0s 35us/sample - loss: 0.0252 - accuracy: 0.8661\n",
      "10000/10000 [==============================] - 0s 34us/sample - loss: 0.0263 - accuracy: 0.8606\n",
      "96th iteration\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000/10000 [==============================] - 0s 36us/sample - loss: 0.0155 - accuracy: 0.8978\n",
      "10000/10000 [==============================] - 0s 36us/sample - loss: 0.0154 - accuracy: 0.8999\n",
      "10000/10000 [==============================] - 0s 37us/sample - loss: 0.0152 - accuracy: 0.9012\n",
      "10000/10000 [==============================] - 0s 34us/sample - loss: 0.0151 - accuracy: 0.9021\n",
      "10000/10000 [==============================] - 0s 36us/sample - loss: 0.0151 - accuracy: 0.9024\n",
      "10000/10000 [==============================] - 0s 36us/sample - loss: 0.0150 - accuracy: 0.9035\n",
      "10000/10000 [==============================] - 0s 35us/sample - loss: 0.0150 - accuracy: 0.9032\n",
      "10000/10000 [==============================] - 0s 35us/sample - loss: 0.0151 - accuracy: 0.9034\n",
      "10000/10000 [==============================] - 0s 35us/sample - loss: 0.0151 - accuracy: 0.9024\n",
      "10000/10000 [==============================] - 0s 36us/sample - loss: 0.0152 - accuracy: 0.9032\n",
      "10000/10000 [==============================] - 0s 35us/sample - loss: 0.0153 - accuracy: 0.9023\n",
      "10000/10000 [==============================] - 0s 36us/sample - loss: 0.0154 - accuracy: 0.9014\n",
      "10000/10000 [==============================] - 0s 36us/sample - loss: 0.0156 - accuracy: 0.9006\n",
      "10000/10000 [==============================] - 0s 34us/sample - loss: 0.0158 - accuracy: 0.9003\n",
      "10000/10000 [==============================] - 0s 35us/sample - loss: 0.0160 - accuracy: 0.8986\n",
      "10000/10000 [==============================] - 1s 56us/sample - loss: 0.0162 - accuracy: 0.8968\n",
      "10000/10000 [==============================] - 0s 36us/sample - loss: 0.0165 - accuracy: 0.8945\n",
      "10000/10000 [==============================] - 0s 35us/sample - loss: 0.0168 - accuracy: 0.8921\n",
      "10000/10000 [==============================] - 0s 34us/sample - loss: 0.0171 - accuracy: 0.8897\n",
      "10000/10000 [==============================] - 0s 36us/sample - loss: 0.0175 - accuracy: 0.8860\n",
      "10000/10000 [==============================] - 0s 36us/sample - loss: 0.0179 - accuracy: 0.8823\n",
      "97th iteration\n",
      "10000/10000 [==============================] - 0s 37us/sample - loss: 0.0156 - accuracy: 0.8985\n",
      "10000/10000 [==============================] - 0s 34us/sample - loss: 0.0155 - accuracy: 0.8991\n",
      "10000/10000 [==============================] - 0s 36us/sample - loss: 0.0154 - accuracy: 0.8997\n",
      "10000/10000 [==============================] - 0s 35us/sample - loss: 0.0153 - accuracy: 0.8995\n",
      "10000/10000 [==============================] - 0s 34us/sample - loss: 0.0152 - accuracy: 0.9003\n",
      "10000/10000 [==============================] - 0s 36us/sample - loss: 0.0152 - accuracy: 0.9003\n",
      "10000/10000 [==============================] - 0s 34us/sample - loss: 0.0152 - accuracy: 0.9002\n",
      "10000/10000 [==============================] - 0s 36us/sample - loss: 0.0153 - accuracy: 0.9009\n",
      "10000/10000 [==============================] - 0s 34us/sample - loss: 0.0153 - accuracy: 0.9018\n",
      "10000/10000 [==============================] - 0s 36us/sample - loss: 0.0155 - accuracy: 0.9005\n",
      "10000/10000 [==============================] - 0s 39us/sample - loss: 0.0156 - accuracy: 0.8996\n",
      "10000/10000 [==============================] - 0s 37us/sample - loss: 0.0158 - accuracy: 0.8995\n",
      "10000/10000 [==============================] - 0s 37us/sample - loss: 0.0160 - accuracy: 0.8995\n",
      "10000/10000 [==============================] - 0s 38us/sample - loss: 0.0163 - accuracy: 0.8968\n",
      "10000/10000 [==============================] - 0s 38us/sample - loss: 0.0165 - accuracy: 0.8957\n",
      "10000/10000 [==============================] - 0s 38us/sample - loss: 0.0169 - accuracy: 0.8942\n",
      "10000/10000 [==============================] - 0s 38us/sample - loss: 0.0172 - accuracy: 0.8935\n",
      "10000/10000 [==============================] - 0s 36us/sample - loss: 0.0176 - accuracy: 0.8910\n",
      "10000/10000 [==============================] - 1s 60us/sample - loss: 0.0181 - accuracy: 0.8891\n",
      "10000/10000 [==============================] - 0s 35us/sample - loss: 0.0185 - accuracy: 0.8852\n",
      "10000/10000 [==============================] - 0s 38us/sample - loss: 0.0191 - accuracy: 0.8818\n",
      "98th iteration\n",
      "10000/10000 [==============================] - 0s 38us/sample - loss: 0.0180 - accuracy: 0.8781\n",
      "10000/10000 [==============================] - 0s 36us/sample - loss: 0.0177 - accuracy: 0.8802\n",
      "10000/10000 [==============================] - 0s 38us/sample - loss: 0.0175 - accuracy: 0.8821\n",
      "10000/10000 [==============================] - 0s 35us/sample - loss: 0.0173 - accuracy: 0.8836\n",
      "10000/10000 [==============================] - 0s 37us/sample - loss: 0.0171 - accuracy: 0.8855\n",
      "10000/10000 [==============================] - 0s 37us/sample - loss: 0.0169 - accuracy: 0.8866\n",
      "10000/10000 [==============================] - 0s 37us/sample - loss: 0.0168 - accuracy: 0.8868\n",
      "10000/10000 [==============================] - 0s 36us/sample - loss: 0.0167 - accuracy: 0.8875\n",
      "10000/10000 [==============================] - 0s 35us/sample - loss: 0.0166 - accuracy: 0.8880\n",
      "10000/10000 [==============================] - 0s 38us/sample - loss: 0.0165 - accuracy: 0.8886\n",
      "10000/10000 [==============================] - 0s 36us/sample - loss: 0.0165 - accuracy: 0.8893\n",
      "10000/10000 [==============================] - 0s 37us/sample - loss: 0.0165 - accuracy: 0.8912\n",
      "10000/10000 [==============================] - 0s 38us/sample - loss: 0.0165 - accuracy: 0.8912\n",
      "10000/10000 [==============================] - 0s 37us/sample - loss: 0.0166 - accuracy: 0.8907\n",
      "10000/10000 [==============================] - 0s 38us/sample - loss: 0.0167 - accuracy: 0.8909\n",
      "10000/10000 [==============================] - 0s 36us/sample - loss: 0.0169 - accuracy: 0.8906\n",
      "10000/10000 [==============================] - 0s 38us/sample - loss: 0.0170 - accuracy: 0.8897\n",
      "10000/10000 [==============================] - 0s 36us/sample - loss: 0.0172 - accuracy: 0.8902\n",
      "10000/10000 [==============================] - 0s 35us/sample - loss: 0.0175 - accuracy: 0.8907\n",
      "10000/10000 [==============================] - 0s 38us/sample - loss: 0.0178 - accuracy: 0.8882\n",
      "10000/10000 [==============================] - 0s 37us/sample - loss: 0.0181 - accuracy: 0.8868\n",
      "99th iteration\n",
      "10000/10000 [==============================] - 0s 38us/sample - loss: 0.0156 - accuracy: 0.8985\n",
      "10000/10000 [==============================] - 0s 36us/sample - loss: 0.0154 - accuracy: 0.8998\n",
      "10000/10000 [==============================] - 0s 37us/sample - loss: 0.0152 - accuracy: 0.9011\n",
      "10000/10000 [==============================] - 0s 38us/sample - loss: 0.0151 - accuracy: 0.9009\n",
      "10000/10000 [==============================] - 0s 36us/sample - loss: 0.0150 - accuracy: 0.9016\n",
      "10000/10000 [==============================] - 0s 37us/sample - loss: 0.0149 - accuracy: 0.9038\n",
      "10000/10000 [==============================] - 0s 36us/sample - loss: 0.0148 - accuracy: 0.9048\n",
      "10000/10000 [==============================] - 0s 37us/sample - loss: 0.0148 - accuracy: 0.9051\n",
      "10000/10000 [==============================] - 0s 37us/sample - loss: 0.0147 - accuracy: 0.9055\n",
      "10000/10000 [==============================] - 0s 37us/sample - loss: 0.0148 - accuracy: 0.9055\n",
      "10000/10000 [==============================] - 0s 36us/sample - loss: 0.0148 - accuracy: 0.9061\n",
      "10000/10000 [==============================] - 0s 38us/sample - loss: 0.0149 - accuracy: 0.9063\n",
      "10000/10000 [==============================] - 0s 36us/sample - loss: 0.0150 - accuracy: 0.9059\n",
      "10000/10000 [==============================] - 0s 38us/sample - loss: 0.0151 - accuracy: 0.9052\n",
      "10000/10000 [==============================] - 0s 37us/sample - loss: 0.0152 - accuracy: 0.9044\n",
      "10000/10000 [==============================] - 0s 36us/sample - loss: 0.0154 - accuracy: 0.9033\n",
      "10000/10000 [==============================] - 0s 38us/sample - loss: 0.0156 - accuracy: 0.9026\n",
      "10000/10000 [==============================] - 0s 37us/sample - loss: 0.0158 - accuracy: 0.9006\n",
      "10000/10000 [==============================] - 0s 38us/sample - loss: 0.0161 - accuracy: 0.8988\n",
      "10000/10000 [==============================] - 0s 35us/sample - loss: 0.0164 - accuracy: 0.8981\n",
      "10000/10000 [==============================] - 0s 38us/sample - loss: 0.0167 - accuracy: 0.8959\n",
      "100th iteration\n",
      "10000/10000 [==============================] - 0s 38us/sample - loss: 0.0168 - accuracy: 0.8870\n",
      "10000/10000 [==============================] - 0s 37us/sample - loss: 0.0168 - accuracy: 0.8872\n",
      "10000/10000 [==============================] - 0s 38us/sample - loss: 0.0168 - accuracy: 0.8884\n",
      "10000/10000 [==============================] - 0s 35us/sample - loss: 0.0169 - accuracy: 0.8889\n",
      "10000/10000 [==============================] - 0s 37us/sample - loss: 0.0170 - accuracy: 0.8900\n",
      "10000/10000 [==============================] - 0s 36us/sample - loss: 0.0171 - accuracy: 0.8906\n",
      "10000/10000 [==============================] - 0s 36us/sample - loss: 0.0173 - accuracy: 0.8904\n",
      "10000/10000 [==============================] - 0s 37us/sample - loss: 0.0176 - accuracy: 0.8903\n",
      "10000/10000 [==============================] - 0s 36us/sample - loss: 0.0179 - accuracy: 0.8902\n",
      "10000/10000 [==============================] - 0s 36us/sample - loss: 0.0182 - accuracy: 0.8900\n",
      "10000/10000 [==============================] - 0s 37us/sample - loss: 0.0186 - accuracy: 0.8895\n",
      "10000/10000 [==============================] - 0s 36us/sample - loss: 0.0191 - accuracy: 0.8879\n",
      "10000/10000 [==============================] - 0s 38us/sample - loss: 0.0197 - accuracy: 0.8863\n",
      "10000/10000 [==============================] - 0s 37us/sample - loss: 0.0203 - accuracy: 0.8854\n",
      "10000/10000 [==============================] - 0s 36us/sample - loss: 0.0209 - accuracy: 0.8820\n",
      "10000/10000 [==============================] - 0s 36us/sample - loss: 0.0217 - accuracy: 0.8798\n",
      "10000/10000 [==============================] - 0s 38us/sample - loss: 0.0225 - accuracy: 0.8766\n",
      "10000/10000 [==============================] - 0s 35us/sample - loss: 0.0233 - accuracy: 0.8740\n",
      "10000/10000 [==============================] - 0s 38us/sample - loss: 0.0242 - accuracy: 0.8704\n",
      "10000/10000 [==============================] - 0s 37us/sample - loss: 0.0252 - accuracy: 0.8661\n",
      "10000/10000 [==============================] - 0s 36us/sample - loss: 0.0263 - accuracy: 0.8606\n",
      "101th iteration\n",
      "10000/10000 [==============================] - 0s 38us/sample - loss: 0.0161 - accuracy: 0.8926\n",
      "10000/10000 [==============================] - 0s 36us/sample - loss: 0.0159 - accuracy: 0.8940\n",
      "10000/10000 [==============================] - 0s 38us/sample - loss: 0.0157 - accuracy: 0.8953\n",
      "10000/10000 [==============================] - 0s 36us/sample - loss: 0.0155 - accuracy: 0.8970\n",
      "10000/10000 [==============================] - 0s 38us/sample - loss: 0.0154 - accuracy: 0.8980\n",
      "10000/10000 [==============================] - 0s 37us/sample - loss: 0.0153 - accuracy: 0.8989\n",
      "10000/10000 [==============================] - 0s 35us/sample - loss: 0.0152 - accuracy: 0.8992\n",
      "10000/10000 [==============================] - 0s 37us/sample - loss: 0.0151 - accuracy: 0.9004\n",
      "10000/10000 [==============================] - 0s 37us/sample - loss: 0.0151 - accuracy: 0.9007\n",
      "10000/10000 [==============================] - 0s 37us/sample - loss: 0.0151 - accuracy: 0.9011\n",
      "10000/10000 [==============================] - 0s 36us/sample - loss: 0.0151 - accuracy: 0.9014\n",
      "10000/10000 [==============================] - 0s 38us/sample - loss: 0.0151 - accuracy: 0.9011\n",
      "10000/10000 [==============================] - 0s 38us/sample - loss: 0.0152 - accuracy: 0.9009\n",
      "10000/10000 [==============================] - 0s 36us/sample - loss: 0.0153 - accuracy: 0.8992\n",
      "10000/10000 [==============================] - 0s 38us/sample - loss: 0.0154 - accuracy: 0.8973\n",
      "10000/10000 [==============================] - 0s 35us/sample - loss: 0.0156 - accuracy: 0.8961\n",
      "10000/10000 [==============================] - 0s 37us/sample - loss: 0.0157 - accuracy: 0.8952\n",
      "10000/10000 [==============================] - 0s 38us/sample - loss: 0.0160 - accuracy: 0.8933\n",
      "10000/10000 [==============================] - 0s 36us/sample - loss: 0.0162 - accuracy: 0.8918\n",
      "10000/10000 [==============================] - 0s 36us/sample - loss: 0.0165 - accuracy: 0.8895\n",
      "10000/10000 [==============================] - 0s 37us/sample - loss: 0.0168 - accuracy: 0.8870\n",
      "102th iteration\n",
      "10000/10000 [==============================] - 0s 37us/sample - loss: 0.0155 - accuracy: 0.8978\n",
      "10000/10000 [==============================] - 0s 38us/sample - loss: 0.0153 - accuracy: 0.9003\n",
      "10000/10000 [==============================] - 0s 36us/sample - loss: 0.0151 - accuracy: 0.9022\n",
      "10000/10000 [==============================] - 0s 37us/sample - loss: 0.0150 - accuracy: 0.9033\n",
      "10000/10000 [==============================] - 0s 36us/sample - loss: 0.0148 - accuracy: 0.9048\n",
      "10000/10000 [==============================] - 0s 36us/sample - loss: 0.0147 - accuracy: 0.9062\n",
      "10000/10000 [==============================] - 0s 38us/sample - loss: 0.0147 - accuracy: 0.9067\n",
      "10000/10000 [==============================] - 0s 37us/sample - loss: 0.0146 - accuracy: 0.9066\n",
      "10000/10000 [==============================] - 0s 37us/sample - loss: 0.0146 - accuracy: 0.9076\n",
      "10000/10000 [==============================] - 0s 36us/sample - loss: 0.0146 - accuracy: 0.9067\n",
      "10000/10000 [==============================] - 0s 37us/sample - loss: 0.0146 - accuracy: 0.9063\n",
      "10000/10000 [==============================] - 0s 37us/sample - loss: 0.0147 - accuracy: 0.9060\n",
      "10000/10000 [==============================] - 0s 37us/sample - loss: 0.0148 - accuracy: 0.9066\n",
      "10000/10000 [==============================] - 0s 37us/sample - loss: 0.0149 - accuracy: 0.9055\n",
      "10000/10000 [==============================] - 0s 36us/sample - loss: 0.0150 - accuracy: 0.9043\n",
      "10000/10000 [==============================] - 0s 36us/sample - loss: 0.0152 - accuracy: 0.9037\n",
      "10000/10000 [==============================] - 0s 37us/sample - loss: 0.0153 - accuracy: 0.9016\n",
      "10000/10000 [==============================] - 0s 38us/sample - loss: 0.0155 - accuracy: 0.8993\n",
      "10000/10000 [==============================] - 0s 36us/sample - loss: 0.0157 - accuracy: 0.8978\n",
      "10000/10000 [==============================] - 1s 58us/sample - loss: 0.0160 - accuracy: 0.8955\n",
      "10000/10000 [==============================] - 0s 37us/sample - loss: 0.0163 - accuracy: 0.8942\n",
      "103th iteration\n",
      "10000/10000 [==============================] - 0s 37us/sample - loss: 0.0151 - accuracy: 0.9033\n",
      "10000/10000 [==============================] - 0s 36us/sample - loss: 0.0152 - accuracy: 0.9025\n",
      "10000/10000 [==============================] - 0s 37us/sample - loss: 0.0153 - accuracy: 0.9016\n",
      "10000/10000 [==============================] - 0s 36us/sample - loss: 0.0155 - accuracy: 0.9013\n",
      "10000/10000 [==============================] - 0s 38us/sample - loss: 0.0157 - accuracy: 0.9011\n",
      "10000/10000 [==============================] - 0s 37us/sample - loss: 0.0160 - accuracy: 0.9010\n",
      "10000/10000 [==============================] - 0s 37us/sample - loss: 0.0163 - accuracy: 0.8994\n",
      "10000/10000 [==============================] - 0s 37us/sample - loss: 0.0166 - accuracy: 0.8990\n",
      "10000/10000 [==============================] - 0s 36us/sample - loss: 0.0170 - accuracy: 0.8980\n",
      "10000/10000 [==============================] - 0s 37us/sample - loss: 0.0174 - accuracy: 0.8966\n",
      "10000/10000 [==============================] - 0s 36us/sample - loss: 0.0179 - accuracy: 0.8958\n",
      "10000/10000 [==============================] - 0s 37us/sample - loss: 0.0185 - accuracy: 0.8945\n",
      "10000/10000 [==============================] - 0s 36us/sample - loss: 0.0191 - accuracy: 0.8931\n",
      "10000/10000 [==============================] - 0s 36us/sample - loss: 0.0198 - accuracy: 0.8919\n",
      "10000/10000 [==============================] - 0s 37us/sample - loss: 0.0205 - accuracy: 0.8886\n",
      "10000/10000 [==============================] - 0s 36us/sample - loss: 0.0213 - accuracy: 0.8849\n",
      "10000/10000 [==============================] - 0s 38us/sample - loss: 0.0222 - accuracy: 0.8802\n",
      "10000/10000 [==============================] - 0s 36us/sample - loss: 0.0231 - accuracy: 0.8755\n",
      "10000/10000 [==============================] - 0s 37us/sample - loss: 0.0241 - accuracy: 0.8714\n",
      "10000/10000 [==============================] - 0s 35us/sample - loss: 0.0252 - accuracy: 0.8664\n",
      "10000/10000 [==============================] - 0s 37us/sample - loss: 0.0263 - accuracy: 0.8606\n",
      "104th iteration\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000/10000 [==============================] - 0s 38us/sample - loss: 0.0155 - accuracy: 0.8978\n",
      "10000/10000 [==============================] - 1s 60us/sample - loss: 0.0154 - accuracy: 0.9007\n",
      "10000/10000 [==============================] - 0s 37us/sample - loss: 0.0153 - accuracy: 0.9017\n",
      "10000/10000 [==============================] - 0s 36us/sample - loss: 0.0152 - accuracy: 0.9020\n",
      "10000/10000 [==============================] - 0s 38us/sample - loss: 0.0152 - accuracy: 0.9024\n",
      "10000/10000 [==============================] - 0s 36us/sample - loss: 0.0152 - accuracy: 0.9024\n",
      "10000/10000 [==============================] - 0s 38us/sample - loss: 0.0152 - accuracy: 0.9028\n",
      "10000/10000 [==============================] - 0s 35us/sample - loss: 0.0152 - accuracy: 0.9028\n",
      "10000/10000 [==============================] - 0s 37us/sample - loss: 0.0153 - accuracy: 0.9026\n",
      "10000/10000 [==============================] - 0s 38us/sample - loss: 0.0154 - accuracy: 0.9025\n",
      "10000/10000 [==============================] - 0s 35us/sample - loss: 0.0155 - accuracy: 0.9012\n",
      "10000/10000 [==============================] - 0s 36us/sample - loss: 0.0157 - accuracy: 0.9003\n",
      "10000/10000 [==============================] - 0s 36us/sample - loss: 0.0158 - accuracy: 0.8997\n",
      "10000/10000 [==============================] - 0s 38us/sample - loss: 0.0160 - accuracy: 0.8986\n",
      "10000/10000 [==============================] - 0s 37us/sample - loss: 0.0162 - accuracy: 0.8972\n",
      "10000/10000 [==============================] - 0s 36us/sample - loss: 0.0165 - accuracy: 0.8965\n",
      "10000/10000 [==============================] - 0s 38us/sample - loss: 0.0167 - accuracy: 0.8951\n",
      "10000/10000 [==============================] - 0s 36us/sample - loss: 0.0170 - accuracy: 0.8928\n",
      "10000/10000 [==============================] - 0s 37us/sample - loss: 0.0174 - accuracy: 0.8911\n",
      "10000/10000 [==============================] - 0s 37us/sample - loss: 0.0177 - accuracy: 0.8894\n",
      "10000/10000 [==============================] - 0s 37us/sample - loss: 0.0181 - accuracy: 0.8868\n",
      "105th iteration\n",
      "10000/10000 [==============================] - 0s 36us/sample - loss: 0.0156 - accuracy: 0.8985\n",
      "10000/10000 [==============================] - 0s 36us/sample - loss: 0.0154 - accuracy: 0.8998\n",
      "10000/10000 [==============================] - 0s 40us/sample - loss: 0.0151 - accuracy: 0.9012\n",
      "10000/10000 [==============================] - 0s 36us/sample - loss: 0.0150 - accuracy: 0.9017\n",
      "10000/10000 [==============================] - 0s 36us/sample - loss: 0.0148 - accuracy: 0.9025\n",
      "10000/10000 [==============================] - 0s 36us/sample - loss: 0.0147 - accuracy: 0.9031\n",
      "10000/10000 [==============================] - 0s 37us/sample - loss: 0.0146 - accuracy: 0.9041\n",
      "10000/10000 [==============================] - 0s 36us/sample - loss: 0.0145 - accuracy: 0.9049\n",
      "10000/10000 [==============================] - 0s 37us/sample - loss: 0.0145 - accuracy: 0.9047\n",
      "10000/10000 [==============================] - 0s 36us/sample - loss: 0.0145 - accuracy: 0.9048\n",
      "10000/10000 [==============================] - 0s 36us/sample - loss: 0.0145 - accuracy: 0.9044\n",
      "10000/10000 [==============================] - 0s 35us/sample - loss: 0.0145 - accuracy: 0.9037\n",
      "10000/10000 [==============================] - 0s 37us/sample - loss: 0.0146 - accuracy: 0.9029\n",
      "10000/10000 [==============================] - 0s 36us/sample - loss: 0.0146 - accuracy: 0.9026\n",
      "10000/10000 [==============================] - 0s 37us/sample - loss: 0.0148 - accuracy: 0.9017\n",
      "10000/10000 [==============================] - 0s 35us/sample - loss: 0.0149 - accuracy: 0.9010\n",
      "10000/10000 [==============================] - 0s 35us/sample - loss: 0.0150 - accuracy: 0.9001\n",
      "10000/10000 [==============================] - 0s 37us/sample - loss: 0.0152 - accuracy: 0.8997\n",
      "10000/10000 [==============================] - 0s 37us/sample - loss: 0.0154 - accuracy: 0.8980\n",
      "10000/10000 [==============================] - 0s 37us/sample - loss: 0.0156 - accuracy: 0.8965\n",
      "10000/10000 [==============================] - 0s 36us/sample - loss: 0.0159 - accuracy: 0.8943\n",
      "106th iteration\n",
      "10000/10000 [==============================] - 0s 39us/sample - loss: 0.0156 - accuracy: 0.8985\n",
      "10000/10000 [==============================] - 0s 38us/sample - loss: 0.0154 - accuracy: 0.8998\n",
      "10000/10000 [==============================] - 0s 38us/sample - loss: 0.0153 - accuracy: 0.9009\n",
      "10000/10000 [==============================] - 0s 38us/sample - loss: 0.0152 - accuracy: 0.9019\n",
      "10000/10000 [==============================] - 0s 38us/sample - loss: 0.0151 - accuracy: 0.9018\n",
      "10000/10000 [==============================] - 0s 38us/sample - loss: 0.0151 - accuracy: 0.9025\n",
      "10000/10000 [==============================] - 0s 38us/sample - loss: 0.0151 - accuracy: 0.9040\n",
      "10000/10000 [==============================] - 0s 39us/sample - loss: 0.0151 - accuracy: 0.9040\n",
      "10000/10000 [==============================] - 0s 37us/sample - loss: 0.0151 - accuracy: 0.9033\n",
      "10000/10000 [==============================] - 0s 38us/sample - loss: 0.0152 - accuracy: 0.9036\n",
      "10000/10000 [==============================] - 0s 38us/sample - loss: 0.0153 - accuracy: 0.9025\n",
      "10000/10000 [==============================] - 0s 35us/sample - loss: 0.0155 - accuracy: 0.9025\n",
      "10000/10000 [==============================] - 0s 38us/sample - loss: 0.0156 - accuracy: 0.9015\n",
      "10000/10000 [==============================] - 0s 38us/sample - loss: 0.0158 - accuracy: 0.9001\n",
      "10000/10000 [==============================] - 0s 35us/sample - loss: 0.0161 - accuracy: 0.8990\n",
      "10000/10000 [==============================] - 0s 37us/sample - loss: 0.0163 - accuracy: 0.8979\n",
      "10000/10000 [==============================] - 0s 36us/sample - loss: 0.0166 - accuracy: 0.8962\n",
      "10000/10000 [==============================] - 0s 37us/sample - loss: 0.0169 - accuracy: 0.8944\n",
      "10000/10000 [==============================] - 0s 36us/sample - loss: 0.0173 - accuracy: 0.8929\n",
      "10000/10000 [==============================] - 0s 36us/sample - loss: 0.0177 - accuracy: 0.8900\n",
      "10000/10000 [==============================] - 0s 36us/sample - loss: 0.0181 - accuracy: 0.8868\n",
      "107th iteration\n",
      "10000/10000 [==============================] - 0s 36us/sample - loss: 0.0180 - accuracy: 0.8781\n",
      "10000/10000 [==============================] - 0s 39us/sample - loss: 0.0177 - accuracy: 0.8795\n",
      "10000/10000 [==============================] - 0s 39us/sample - loss: 0.0175 - accuracy: 0.8809\n",
      "10000/10000 [==============================] - 0s 40us/sample - loss: 0.0173 - accuracy: 0.8832\n",
      "10000/10000 [==============================] - 0s 36us/sample - loss: 0.0171 - accuracy: 0.8835\n",
      "10000/10000 [==============================] - 0s 36us/sample - loss: 0.0170 - accuracy: 0.8846\n",
      "10000/10000 [==============================] - 0s 38us/sample - loss: 0.0169 - accuracy: 0.8866\n",
      "10000/10000 [==============================] - 0s 34us/sample - loss: 0.0168 - accuracy: 0.8869\n",
      "10000/10000 [==============================] - 0s 36us/sample - loss: 0.0167 - accuracy: 0.8888\n",
      "10000/10000 [==============================] - 0s 36us/sample - loss: 0.0167 - accuracy: 0.8893\n",
      "10000/10000 [==============================] - 0s 37us/sample - loss: 0.0167 - accuracy: 0.8893\n",
      "10000/10000 [==============================] - 0s 36us/sample - loss: 0.0168 - accuracy: 0.8898\n",
      "10000/10000 [==============================] - 0s 36us/sample - loss: 0.0169 - accuracy: 0.8906\n",
      "10000/10000 [==============================] - 0s 35us/sample - loss: 0.0170 - accuracy: 0.8909\n",
      "10000/10000 [==============================] - 0s 36us/sample - loss: 0.0171 - accuracy: 0.8905\n",
      "10000/10000 [==============================] - 0s 36us/sample - loss: 0.0173 - accuracy: 0.8909\n",
      "10000/10000 [==============================] - ETA: 0s - loss: 0.0187 - accuracy: 0.88 - 0s 34us/sample - loss: 0.0176 - accuracy: 0.8901\n",
      "10000/10000 [==============================] - 0s 35us/sample - loss: 0.0179 - accuracy: 0.8888\n",
      "10000/10000 [==============================] - 0s 34us/sample - loss: 0.0182 - accuracy: 0.8883\n",
      "10000/10000 [==============================] - 0s 34us/sample - loss: 0.0185 - accuracy: 0.8859\n",
      "10000/10000 [==============================] - 0s 36us/sample - loss: 0.0190 - accuracy: 0.8848\n",
      "108th iteration\n",
      "10000/10000 [==============================] - 0s 35us/sample - loss: 0.0155 - accuracy: 0.8978\n",
      "10000/10000 [==============================] - 0s 41us/sample - loss: 0.0154 - accuracy: 0.8997\n",
      "10000/10000 [==============================] - 0s 36us/sample - loss: 0.0154 - accuracy: 0.9012\n",
      "10000/10000 [==============================] - 0s 36us/sample - loss: 0.0153 - accuracy: 0.9014\n",
      "10000/10000 [==============================] - 0s 37us/sample - loss: 0.0153 - accuracy: 0.9013\n",
      "10000/10000 [==============================] - 0s 37us/sample - loss: 0.0153 - accuracy: 0.9014\n",
      "10000/10000 [==============================] - 0s 37us/sample - loss: 0.0154 - accuracy: 0.9027\n",
      "10000/10000 [==============================] - 0s 36us/sample - loss: 0.0154 - accuracy: 0.9031\n",
      "10000/10000 [==============================] - 0s 38us/sample - loss: 0.0155 - accuracy: 0.9028\n",
      "10000/10000 [==============================] - 0s 38us/sample - loss: 0.0156 - accuracy: 0.9018\n",
      "10000/10000 [==============================] - 0s 36us/sample - loss: 0.0158 - accuracy: 0.9015\n",
      "10000/10000 [==============================] - 0s 36us/sample - loss: 0.0160 - accuracy: 0.9008\n",
      "10000/10000 [==============================] - 0s 37us/sample - loss: 0.0162 - accuracy: 0.8996\n",
      "10000/10000 [==============================] - 0s 37us/sample - loss: 0.0164 - accuracy: 0.8994\n",
      "10000/10000 [==============================] - 0s 36us/sample - loss: 0.0167 - accuracy: 0.8978\n",
      "10000/10000 [==============================] - 0s 36us/sample - loss: 0.0170 - accuracy: 0.8959\n",
      "10000/10000 [==============================] - 0s 37us/sample - loss: 0.0173 - accuracy: 0.8935\n",
      "10000/10000 [==============================] - 0s 36us/sample - loss: 0.0177 - accuracy: 0.8916\n",
      "10000/10000 [==============================] - 0s 39us/sample - loss: 0.0181 - accuracy: 0.8900\n",
      "10000/10000 [==============================] - 0s 36us/sample - loss: 0.0185 - accuracy: 0.8880\n",
      "10000/10000 [==============================] - 0s 40us/sample - loss: 0.0190 - accuracy: 0.8848\n",
      "109th iteration\n",
      "10000/10000 [==============================] - 0s 36us/sample - loss: 0.0168 - accuracy: 0.8853\n",
      "10000/10000 [==============================] - 0s 39us/sample - loss: 0.0165 - accuracy: 0.8868\n",
      "10000/10000 [==============================] - 1s 59us/sample - loss: 0.0163 - accuracy: 0.8885\n",
      "10000/10000 [==============================] - 0s 38us/sample - loss: 0.0161 - accuracy: 0.8911\n",
      "10000/10000 [==============================] - 0s 38us/sample - loss: 0.0160 - accuracy: 0.8934\n",
      "10000/10000 [==============================] - 0s 38us/sample - loss: 0.0159 - accuracy: 0.8939\n",
      "10000/10000 [==============================] - 0s 40us/sample - loss: 0.0158 - accuracy: 0.8950\n",
      "10000/10000 [==============================] - 0s 38us/sample - loss: 0.0157 - accuracy: 0.8955\n",
      "10000/10000 [==============================] - 0s 38us/sample - loss: 0.0157 - accuracy: 0.8963\n",
      "10000/10000 [==============================] - 0s 37us/sample - loss: 0.0157 - accuracy: 0.8968\n",
      "10000/10000 [==============================] - 0s 38us/sample - loss: 0.0157 - accuracy: 0.8973\n",
      "10000/10000 [==============================] - 0s 39us/sample - loss: 0.0158 - accuracy: 0.8972\n",
      "10000/10000 [==============================] - 0s 41us/sample - loss: 0.0159 - accuracy: 0.8980\n",
      "10000/10000 [==============================] - 0s 44us/sample - loss: 0.0160 - accuracy: 0.8962\n",
      "10000/10000 [==============================] - 0s 40us/sample - loss: 0.0162 - accuracy: 0.8944\n",
      "10000/10000 [==============================] - 0s 41us/sample - loss: 0.0164 - accuracy: 0.8937\n",
      "10000/10000 [==============================] - 0s 39us/sample - loss: 0.0166 - accuracy: 0.8925\n",
      "10000/10000 [==============================] - 0s 42us/sample - loss: 0.0169 - accuracy: 0.8898\n",
      "10000/10000 [==============================] - 0s 39us/sample - loss: 0.0172 - accuracy: 0.8878\n",
      "10000/10000 [==============================] - 0s 39us/sample - loss: 0.0175 - accuracy: 0.8856\n",
      "10000/10000 [==============================] - 0s 39us/sample - loss: 0.0179 - accuracy: 0.8823\n",
      "110th iteration\n",
      "10000/10000 [==============================] - 0s 40us/sample - loss: 0.0155 - accuracy: 0.8978\n",
      "10000/10000 [==============================] - 0s 40us/sample - loss: 0.0153 - accuracy: 0.9010\n",
      "10000/10000 [==============================] - 0s 40us/sample - loss: 0.0151 - accuracy: 0.9028\n",
      "10000/10000 [==============================] - 0s 39us/sample - loss: 0.0149 - accuracy: 0.9043\n",
      "10000/10000 [==============================] - 0s 38us/sample - loss: 0.0148 - accuracy: 0.9052\n",
      "10000/10000 [==============================] - 1s 60us/sample - loss: 0.0147 - accuracy: 0.9064\n",
      "10000/10000 [==============================] - 0s 40us/sample - loss: 0.0146 - accuracy: 0.9069\n",
      "10000/10000 [==============================] - 0s 38us/sample - loss: 0.0146 - accuracy: 0.9069\n",
      "10000/10000 [==============================] - 0s 37us/sample - loss: 0.0145 - accuracy: 0.9056\n",
      "10000/10000 [==============================] - 0s 39us/sample - loss: 0.0145 - accuracy: 0.9056\n",
      "10000/10000 [==============================] - 0s 37us/sample - loss: 0.0145 - accuracy: 0.9057\n",
      "10000/10000 [==============================] - 0s 36us/sample - loss: 0.0146 - accuracy: 0.9056\n",
      "10000/10000 [==============================] - 0s 38us/sample - loss: 0.0146 - accuracy: 0.9050\n",
      "10000/10000 [==============================] - 0s 35us/sample - loss: 0.0147 - accuracy: 0.9045\n",
      "10000/10000 [==============================] - 0s 36us/sample - loss: 0.0148 - accuracy: 0.9026\n",
      "10000/10000 [==============================] - 0s 35us/sample - loss: 0.0149 - accuracy: 0.9018\n",
      "10000/10000 [==============================] - 0s 35us/sample - loss: 0.0151 - accuracy: 0.9005\n",
      "10000/10000 [==============================] - 0s 36us/sample - loss: 0.0153 - accuracy: 0.8993\n",
      "10000/10000 [==============================] - 0s 37us/sample - loss: 0.0154 - accuracy: 0.8985\n",
      "10000/10000 [==============================] - 0s 36us/sample - loss: 0.0157 - accuracy: 0.8965\n",
      "10000/10000 [==============================] - 0s 37us/sample - loss: 0.0159 - accuracy: 0.8943\n",
      "111th iteration\n",
      "10000/10000 [==============================] - 0s 36us/sample - loss: 0.0180 - accuracy: 0.8781\n",
      "10000/10000 [==============================] - 0s 35us/sample - loss: 0.0177 - accuracy: 0.8807\n",
      "10000/10000 [==============================] - 0s 36us/sample - loss: 0.0174 - accuracy: 0.8839\n",
      "10000/10000 [==============================] - 0s 38us/sample - loss: 0.0171 - accuracy: 0.8856\n",
      "10000/10000 [==============================] - 0s 35us/sample - loss: 0.0169 - accuracy: 0.8875\n",
      "10000/10000 [==============================] - 0s 34us/sample - loss: 0.0167 - accuracy: 0.8893\n",
      "10000/10000 [==============================] - 0s 37us/sample - loss: 0.0165 - accuracy: 0.8893\n",
      "10000/10000 [==============================] - 0s 35us/sample - loss: 0.0164 - accuracy: 0.8908\n",
      "10000/10000 [==============================] - 0s 38us/sample - loss: 0.0163 - accuracy: 0.8923\n",
      "10000/10000 [==============================] - 0s 34us/sample - loss: 0.0162 - accuracy: 0.8936\n",
      "10000/10000 [==============================] - 0s 38us/sample - loss: 0.0161 - accuracy: 0.8946\n",
      "10000/10000 [==============================] - 0s 37us/sample - loss: 0.0161 - accuracy: 0.8956\n",
      "10000/10000 [==============================] - 0s 38us/sample - loss: 0.0162 - accuracy: 0.8960\n",
      "10000/10000 [==============================] - 0s 37us/sample - loss: 0.0162 - accuracy: 0.8955\n",
      "10000/10000 [==============================] - 0s 37us/sample - loss: 0.0164 - accuracy: 0.8943\n",
      "10000/10000 [==============================] - 0s 39us/sample - loss: 0.0165 - accuracy: 0.8939\n",
      "10000/10000 [==============================] - 0s 36us/sample - loss: 0.0167 - accuracy: 0.8919\n",
      "10000/10000 [==============================] - 0s 38us/sample - loss: 0.0169 - accuracy: 0.8908\n",
      "10000/10000 [==============================] - 0s 37us/sample - loss: 0.0172 - accuracy: 0.8880\n",
      "10000/10000 [==============================] - 0s 38us/sample - loss: 0.0175 - accuracy: 0.8853\n",
      "10000/10000 [==============================] - 0s 36us/sample - loss: 0.0179 - accuracy: 0.8823\n",
      "112th iteration\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000/10000 [==============================] - 0s 36us/sample - loss: 0.0156 - accuracy: 0.8985\n",
      "10000/10000 [==============================] - 0s 41us/sample - loss: 0.0155 - accuracy: 0.8993\n",
      "10000/10000 [==============================] - 0s 37us/sample - loss: 0.0154 - accuracy: 0.8996\n",
      "10000/10000 [==============================] - 0s 38us/sample - loss: 0.0153 - accuracy: 0.8987\n",
      "10000/10000 [==============================] - 0s 35us/sample - loss: 0.0153 - accuracy: 0.8991\n",
      "10000/10000 [==============================] - 0s 41us/sample - loss: 0.0153 - accuracy: 0.8997\n",
      "10000/10000 [==============================] - 0s 37us/sample - loss: 0.0153 - accuracy: 0.8999\n",
      "10000/10000 [==============================] - 0s 37us/sample - loss: 0.0154 - accuracy: 0.9003\n",
      "10000/10000 [==============================] - 0s 36us/sample - loss: 0.0155 - accuracy: 0.9004\n",
      "10000/10000 [==============================] - 0s 39us/sample - loss: 0.0156 - accuracy: 0.9012\n",
      "10000/10000 [==============================] - 0s 38us/sample - loss: 0.0157 - accuracy: 0.9010\n",
      "10000/10000 [==============================] - 0s 37us/sample - loss: 0.0159 - accuracy: 0.9005\n",
      "10000/10000 [==============================] - 0s 39us/sample - loss: 0.0161 - accuracy: 0.9001\n",
      "10000/10000 [==============================] - 0s 38us/sample - loss: 0.0164 - accuracy: 0.8983\n",
      "10000/10000 [==============================] - 0s 39us/sample - loss: 0.0166 - accuracy: 0.8973\n",
      "10000/10000 [==============================] - 0s 37us/sample - loss: 0.0169 - accuracy: 0.8953\n",
      "10000/10000 [==============================] - 0s 38us/sample - loss: 0.0173 - accuracy: 0.8931\n",
      "10000/10000 [==============================] - 0s 39us/sample - loss: 0.0176 - accuracy: 0.8914\n",
      "10000/10000 [==============================] - 0s 36us/sample - loss: 0.0180 - accuracy: 0.8900\n",
      "10000/10000 [==============================] - 0s 38us/sample - loss: 0.0185 - accuracy: 0.8875\n",
      "10000/10000 [==============================] - 0s 37us/sample - loss: 0.0190 - accuracy: 0.8848\n",
      "113th iteration\n",
      "10000/10000 [==============================] - 0s 36us/sample - loss: 0.0155 - accuracy: 0.8978\n",
      "10000/10000 [==============================] - 0s 37us/sample - loss: 0.0154 - accuracy: 0.9006\n",
      "10000/10000 [==============================] - 0s 38us/sample - loss: 0.0153 - accuracy: 0.9024\n",
      "10000/10000 [==============================] - 0s 38us/sample - loss: 0.0152 - accuracy: 0.9023\n",
      "10000/10000 [==============================] - 0s 37us/sample - loss: 0.0151 - accuracy: 0.9026\n",
      "10000/10000 [==============================] - 0s 35us/sample - loss: 0.0151 - accuracy: 0.9031\n",
      "10000/10000 [==============================] - 0s 36us/sample - loss: 0.0151 - accuracy: 0.9035\n",
      "10000/10000 [==============================] - 0s 36us/sample - loss: 0.0151 - accuracy: 0.9038\n",
      "10000/10000 [==============================] - 0s 36us/sample - loss: 0.0152 - accuracy: 0.9046\n",
      "10000/10000 [==============================] - 0s 37us/sample - loss: 0.0153 - accuracy: 0.9055\n",
      "10000/10000 [==============================] - 0s 34us/sample - loss: 0.0155 - accuracy: 0.9044\n",
      "10000/10000 [==============================] - 0s 39us/sample - loss: 0.0156 - accuracy: 0.9043\n",
      "10000/10000 [==============================] - 0s 37us/sample - loss: 0.0159 - accuracy: 0.9030\n",
      "10000/10000 [==============================] - 0s 35us/sample - loss: 0.0161 - accuracy: 0.9014\n",
      "10000/10000 [==============================] - 0s 36us/sample - loss: 0.0164 - accuracy: 0.8998\n",
      "10000/10000 [==============================] - 0s 38us/sample - loss: 0.0168 - accuracy: 0.8974\n",
      "10000/10000 [==============================] - 0s 37us/sample - loss: 0.0171 - accuracy: 0.8958\n",
      "10000/10000 [==============================] - 0s 41us/sample - loss: 0.0176 - accuracy: 0.8928\n",
      "10000/10000 [==============================] - 0s 36us/sample - loss: 0.0180 - accuracy: 0.8892\n",
      "10000/10000 [==============================] - 0s 37us/sample - loss: 0.0185 - accuracy: 0.8859\n",
      "10000/10000 [==============================] - 0s 36us/sample - loss: 0.0191 - accuracy: 0.8818\n",
      "114th iteration\n",
      "10000/10000 [==============================] - 0s 38us/sample - loss: 0.0180 - accuracy: 0.8781\n",
      "10000/10000 [==============================] - 0s 38us/sample - loss: 0.0176 - accuracy: 0.8820\n",
      "10000/10000 [==============================] - 0s 36us/sample - loss: 0.0172 - accuracy: 0.8839\n",
      "10000/10000 [==============================] - 0s 38us/sample - loss: 0.0169 - accuracy: 0.8859\n",
      "10000/10000 [==============================] - 0s 37us/sample - loss: 0.0166 - accuracy: 0.8875\n",
      "10000/10000 [==============================] - 0s 38us/sample - loss: 0.0163 - accuracy: 0.8893\n",
      "10000/10000 [==============================] - 0s 37us/sample - loss: 0.0161 - accuracy: 0.8912\n",
      "10000/10000 [==============================] - 0s 39us/sample - loss: 0.0158 - accuracy: 0.8927\n",
      "10000/10000 [==============================] - 0s 38us/sample - loss: 0.0157 - accuracy: 0.8942\n",
      "10000/10000 [==============================] - 0s 36us/sample - loss: 0.0155 - accuracy: 0.8961\n",
      "10000/10000 [==============================] - 0s 39us/sample - loss: 0.0154 - accuracy: 0.8978\n",
      "10000/10000 [==============================] - 0s 40us/sample - loss: 0.0153 - accuracy: 0.8978\n",
      "10000/10000 [==============================] - 0s 40us/sample - loss: 0.0152 - accuracy: 0.8974\n",
      "10000/10000 [==============================] - 0s 38us/sample - loss: 0.0152 - accuracy: 0.8980\n",
      "10000/10000 [==============================] - 0s 37us/sample - loss: 0.0152 - accuracy: 0.8982\n",
      "10000/10000 [==============================] - 0s 36us/sample - loss: 0.0152 - accuracy: 0.8983\n",
      "10000/10000 [==============================] - 0s 38us/sample - loss: 0.0153 - accuracy: 0.8990\n",
      "10000/10000 [==============================] - 0s 38us/sample - loss: 0.0154 - accuracy: 0.8981\n",
      "10000/10000 [==============================] - 0s 39us/sample - loss: 0.0155 - accuracy: 0.8966\n",
      "10000/10000 [==============================] - 0s 36us/sample - loss: 0.0157 - accuracy: 0.8964\n",
      "10000/10000 [==============================] - 0s 37us/sample - loss: 0.0159 - accuracy: 0.8943\n",
      "115th iteration\n",
      "10000/10000 [==============================] - 0s 36us/sample - loss: 0.0168 - accuracy: 0.8853\n",
      "10000/10000 [==============================] - 0s 37us/sample - loss: 0.0165 - accuracy: 0.8871\n",
      "10000/10000 [==============================] - 0s 38us/sample - loss: 0.0164 - accuracy: 0.8881\n",
      "10000/10000 [==============================] - 0s 38us/sample - loss: 0.0162 - accuracy: 0.8907\n",
      "10000/10000 [==============================] - 0s 39us/sample - loss: 0.0161 - accuracy: 0.8917\n",
      "10000/10000 [==============================] - 0s 38us/sample - loss: 0.0160 - accuracy: 0.8929\n",
      "10000/10000 [==============================] - 1s 62us/sample - loss: 0.0159 - accuracy: 0.8942\n",
      "10000/10000 [==============================] - 0s 37us/sample - loss: 0.0159 - accuracy: 0.8949\n",
      "10000/10000 [==============================] - 0s 37us/sample - loss: 0.0159 - accuracy: 0.8954\n",
      "10000/10000 [==============================] - 0s 37us/sample - loss: 0.0159 - accuracy: 0.8960\n",
      "10000/10000 [==============================] - 0s 39us/sample - loss: 0.0159 - accuracy: 0.8960\n",
      "10000/10000 [==============================] - 0s 37us/sample - loss: 0.0160 - accuracy: 0.8954\n",
      "10000/10000 [==============================] - 0s 39us/sample - loss: 0.0161 - accuracy: 0.8955\n",
      "10000/10000 [==============================] - 0s 36us/sample - loss: 0.0162 - accuracy: 0.8950\n",
      "10000/10000 [==============================] - 0s 37us/sample - loss: 0.0164 - accuracy: 0.8934\n",
      "10000/10000 [==============================] - 0s 38us/sample - loss: 0.0166 - accuracy: 0.8931\n",
      "10000/10000 [==============================] - 0s 38us/sample - loss: 0.0168 - accuracy: 0.8923\n",
      "10000/10000 [==============================] - 0s 36us/sample - loss: 0.0171 - accuracy: 0.8917\n",
      "10000/10000 [==============================] - 0s 39us/sample - loss: 0.0174 - accuracy: 0.8902\n",
      "10000/10000 [==============================] - 0s 38us/sample - loss: 0.0177 - accuracy: 0.8887\n",
      "10000/10000 [==============================] - 0s 38us/sample - loss: 0.0181 - accuracy: 0.8868\n",
      "116th iteration\n",
      "10000/10000 [==============================] - 0s 37us/sample - loss: 0.0156 - accuracy: 0.8985\n",
      "10000/10000 [==============================] - 0s 36us/sample - loss: 0.0154 - accuracy: 0.8990\n",
      "10000/10000 [==============================] - 0s 38us/sample - loss: 0.0152 - accuracy: 0.9007\n",
      "10000/10000 [==============================] - 0s 38us/sample - loss: 0.0150 - accuracy: 0.9030\n",
      "10000/10000 [==============================] - 0s 38us/sample - loss: 0.0148 - accuracy: 0.9048\n",
      "10000/10000 [==============================] - 0s 37us/sample - loss: 0.0147 - accuracy: 0.9046\n",
      "10000/10000 [==============================] - 0s 36us/sample - loss: 0.0146 - accuracy: 0.9056\n",
      "10000/10000 [==============================] - 0s 37us/sample - loss: 0.0145 - accuracy: 0.9058\n",
      "10000/10000 [==============================] - 0s 38us/sample - loss: 0.0145 - accuracy: 0.9066\n",
      "10000/10000 [==============================] - 1s 60us/sample - loss: 0.0145 - accuracy: 0.9058\n",
      "10000/10000 [==============================] - 0s 38us/sample - loss: 0.0145 - accuracy: 0.9056\n",
      "10000/10000 [==============================] - 0s 38us/sample - loss: 0.0145 - accuracy: 0.9056\n",
      "10000/10000 [==============================] - 0s 41us/sample - loss: 0.0146 - accuracy: 0.9050\n",
      "10000/10000 [==============================] - 0s 38us/sample - loss: 0.0147 - accuracy: 0.9034\n",
      "10000/10000 [==============================] - 0s 36us/sample - loss: 0.0148 - accuracy: 0.9037\n",
      "10000/10000 [==============================] - 0s 35us/sample - loss: 0.0150 - accuracy: 0.9035\n",
      "10000/10000 [==============================] - 0s 37us/sample - loss: 0.0151 - accuracy: 0.9027\n",
      "10000/10000 [==============================] - 0s 35us/sample - loss: 0.0153 - accuracy: 0.9014\n",
      "10000/10000 [==============================] - 0s 36us/sample - loss: 0.0156 - accuracy: 0.8995\n",
      "10000/10000 [==============================] - 0s 35us/sample - loss: 0.0159 - accuracy: 0.8981\n",
      "10000/10000 [==============================] - 0s 35us/sample - loss: 0.0161 - accuracy: 0.8961\n",
      "117th iteration\n",
      "10000/10000 [==============================] - 0s 35us/sample - loss: 0.0156 - accuracy: 0.8985\n",
      "10000/10000 [==============================] - 0s 35us/sample - loss: 0.0154 - accuracy: 0.8998\n",
      "10000/10000 [==============================] - 0s 36us/sample - loss: 0.0152 - accuracy: 0.9013\n",
      "10000/10000 [==============================] - 0s 37us/sample - loss: 0.0150 - accuracy: 0.9025\n",
      "10000/10000 [==============================] - 0s 34us/sample - loss: 0.0149 - accuracy: 0.9035\n",
      "10000/10000 [==============================] - 0s 37us/sample - loss: 0.0148 - accuracy: 0.9041\n",
      "10000/10000 [==============================] - 0s 36us/sample - loss: 0.0147 - accuracy: 0.9049\n",
      "10000/10000 [==============================] - 0s 36us/sample - loss: 0.0146 - accuracy: 0.9051\n",
      "10000/10000 [==============================] - 0s 34us/sample - loss: 0.0146 - accuracy: 0.9051\n",
      "10000/10000 [==============================] - 0s 35us/sample - loss: 0.0146 - accuracy: 0.9065\n",
      "10000/10000 [==============================] - 0s 36us/sample - loss: 0.0146 - accuracy: 0.9072\n",
      "10000/10000 [==============================] - 0s 37us/sample - loss: 0.0147 - accuracy: 0.9069\n",
      "10000/10000 [==============================] - 0s 36us/sample - loss: 0.0147 - accuracy: 0.9065\n",
      "10000/10000 [==============================] - 0s 35us/sample - loss: 0.0148 - accuracy: 0.9051\n",
      "10000/10000 [==============================] - 0s 36us/sample - loss: 0.0149 - accuracy: 0.9043\n",
      "10000/10000 [==============================] - 0s 35us/sample - loss: 0.0151 - accuracy: 0.9029\n",
      "10000/10000 [==============================] - 0s 34us/sample - loss: 0.0153 - accuracy: 0.9020\n",
      "10000/10000 [==============================] - 0s 35us/sample - loss: 0.0155 - accuracy: 0.8994\n",
      "10000/10000 [==============================] - 0s 35us/sample - loss: 0.0157 - accuracy: 0.8975\n",
      "10000/10000 [==============================] - 0s 34us/sample - loss: 0.0160 - accuracy: 0.8955\n",
      "10000/10000 [==============================] - 0s 35us/sample - loss: 0.0163 - accuracy: 0.8942\n",
      "118th iteration\n",
      "10000/10000 [==============================] - 0s 36us/sample - loss: 0.0168 - accuracy: 0.8853\n",
      "10000/10000 [==============================] - 0s 35us/sample - loss: 0.0166 - accuracy: 0.8863\n",
      "10000/10000 [==============================] - 0s 36us/sample - loss: 0.0165 - accuracy: 0.8879\n",
      "10000/10000 [==============================] - 0s 36us/sample - loss: 0.0164 - accuracy: 0.8893\n",
      "10000/10000 [==============================] - 0s 36us/sample - loss: 0.0163 - accuracy: 0.8904\n",
      "10000/10000 [==============================] - 0s 36us/sample - loss: 0.0162 - accuracy: 0.8916\n",
      "10000/10000 [==============================] - 0s 35us/sample - loss: 0.0162 - accuracy: 0.8928\n",
      "10000/10000 [==============================] - 0s 35us/sample - loss: 0.0162 - accuracy: 0.8937\n",
      "10000/10000 [==============================] - 0s 35us/sample - loss: 0.0162 - accuracy: 0.8935\n",
      "10000/10000 [==============================] - 0s 34us/sample - loss: 0.0163 - accuracy: 0.8933\n",
      "10000/10000 [==============================] - 0s 36us/sample - loss: 0.0164 - accuracy: 0.8943\n",
      "10000/10000 [==============================] - 0s 35us/sample - loss: 0.0165 - accuracy: 0.8939\n",
      "10000/10000 [==============================] - 0s 34us/sample - loss: 0.0166 - accuracy: 0.8935\n",
      "10000/10000 [==============================] - 0s 35us/sample - loss: 0.0168 - accuracy: 0.8924\n",
      "10000/10000 [==============================] - 0s 36us/sample - loss: 0.0170 - accuracy: 0.8913\n",
      "10000/10000 [==============================] - 0s 36us/sample - loss: 0.0172 - accuracy: 0.8906\n",
      "10000/10000 [==============================] - 0s 35us/sample - loss: 0.0175 - accuracy: 0.8899\n",
      "10000/10000 [==============================] - 0s 35us/sample - loss: 0.0178 - accuracy: 0.8883\n",
      "10000/10000 [==============================] - 0s 36us/sample - loss: 0.0182 - accuracy: 0.8883\n",
      "10000/10000 [==============================] - 0s 34us/sample - loss: 0.0185 - accuracy: 0.8871\n",
      "10000/10000 [==============================] - 0s 36us/sample - loss: 0.0190 - accuracy: 0.8848\n",
      "119th iteration\n",
      "10000/10000 [==============================] - 0s 34us/sample - loss: 0.0156 - accuracy: 0.8985\n",
      "10000/10000 [==============================] - 0s 36us/sample - loss: 0.0153 - accuracy: 0.8998\n",
      "10000/10000 [==============================] - 0s 38us/sample - loss: 0.0151 - accuracy: 0.9003\n",
      "10000/10000 [==============================] - 0s 36us/sample - loss: 0.0149 - accuracy: 0.9018\n",
      "10000/10000 [==============================] - 0s 34us/sample - loss: 0.0147 - accuracy: 0.9034\n",
      "10000/10000 [==============================] - 0s 35us/sample - loss: 0.0146 - accuracy: 0.9045\n",
      "10000/10000 [==============================] - 0s 35us/sample - loss: 0.0144 - accuracy: 0.9062\n",
      "10000/10000 [==============================] - 0s 36us/sample - loss: 0.0143 - accuracy: 0.9070\n",
      "10000/10000 [==============================] - 0s 36us/sample - loss: 0.0143 - accuracy: 0.9080\n",
      "10000/10000 [==============================] - 0s 35us/sample - loss: 0.0142 - accuracy: 0.9082\n",
      "10000/10000 [==============================] - 0s 34us/sample - loss: 0.0142 - accuracy: 0.9086\n",
      "10000/10000 [==============================] - 0s 37us/sample - loss: 0.0142 - accuracy: 0.9090\n",
      "10000/10000 [==============================] - 0s 36us/sample - loss: 0.0142 - accuracy: 0.9091\n",
      "10000/10000 [==============================] - 0s 36us/sample - loss: 0.0142 - accuracy: 0.9080\n",
      "10000/10000 [==============================] - 0s 34us/sample - loss: 0.0143 - accuracy: 0.9078\n",
      "10000/10000 [==============================] - 0s 34us/sample - loss: 0.0143 - accuracy: 0.9071\n",
      "10000/10000 [==============================] - 0s 34us/sample - loss: 0.0145 - accuracy: 0.9067\n",
      "10000/10000 [==============================] - 0s 34us/sample - loss: 0.0146 - accuracy: 0.9062\n",
      "10000/10000 [==============================] - 0s 35us/sample - loss: 0.0147 - accuracy: 0.9048\n",
      "10000/10000 [==============================] - 0s 35us/sample - loss: 0.0149 - accuracy: 0.9041\n",
      "10000/10000 [==============================] - 0s 34us/sample - loss: 0.0151 - accuracy: 0.9033\n",
      "120th iteration\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000/10000 [==============================] - 0s 35us/sample - loss: 0.0156 - accuracy: 0.8985\n",
      "10000/10000 [==============================] - 0s 35us/sample - loss: 0.0155 - accuracy: 0.8980\n",
      "10000/10000 [==============================] - 0s 36us/sample - loss: 0.0155 - accuracy: 0.8978\n",
      "10000/10000 [==============================] - 0s 36us/sample - loss: 0.0155 - accuracy: 0.8979\n",
      "10000/10000 [==============================] - 0s 34us/sample - loss: 0.0155 - accuracy: 0.8980\n",
      "10000/10000 [==============================] - 0s 35us/sample - loss: 0.0156 - accuracy: 0.8989\n",
      "10000/10000 [==============================] - 0s 36us/sample - loss: 0.0156 - accuracy: 0.8998\n",
      "10000/10000 [==============================] - 0s 36us/sample - loss: 0.0158 - accuracy: 0.8995\n",
      "10000/10000 [==============================] - 0s 36us/sample - loss: 0.0159 - accuracy: 0.9004\n",
      "10000/10000 [==============================] - 0s 37us/sample - loss: 0.0161 - accuracy: 0.8999\n",
      "10000/10000 [==============================] - 0s 34us/sample - loss: 0.0163 - accuracy: 0.8994\n",
      "10000/10000 [==============================] - 0s 36us/sample - loss: 0.0166 - accuracy: 0.8979\n",
      "10000/10000 [==============================] - 0s 36us/sample - loss: 0.0168 - accuracy: 0.8966\n",
      "10000/10000 [==============================] - 0s 35us/sample - loss: 0.0172 - accuracy: 0.8955\n",
      "10000/10000 [==============================] - 0s 35us/sample - loss: 0.0175 - accuracy: 0.8948\n",
      "10000/10000 [==============================] - 0s 34us/sample - loss: 0.0179 - accuracy: 0.8929\n",
      "10000/10000 [==============================] - 0s 36us/sample - loss: 0.0184 - accuracy: 0.8910\n",
      "10000/10000 [==============================] - 0s 36us/sample - loss: 0.0189 - accuracy: 0.8887\n",
      "10000/10000 [==============================] - 0s 36us/sample - loss: 0.0194 - accuracy: 0.8865\n",
      "10000/10000 [==============================] - 0s 38us/sample - loss: 0.0200 - accuracy: 0.8847\n",
      "10000/10000 [==============================] - 0s 36us/sample - loss: 0.0206 - accuracy: 0.8813\n",
      "121th iteration\n",
      "10000/10000 [==============================] - 0s 37us/sample - loss: 0.0180 - accuracy: 0.8781\n",
      "10000/10000 [==============================] - 0s 36us/sample - loss: 0.0176 - accuracy: 0.8807\n",
      "10000/10000 [==============================] - 0s 36us/sample - loss: 0.0172 - accuracy: 0.8842\n",
      "10000/10000 [==============================] - 0s 35us/sample - loss: 0.0169 - accuracy: 0.8859\n",
      "10000/10000 [==============================] - 0s 36us/sample - loss: 0.0166 - accuracy: 0.8877\n",
      "10000/10000 [==============================] - 0s 37us/sample - loss: 0.0163 - accuracy: 0.8895\n",
      "10000/10000 [==============================] - 0s 34us/sample - loss: 0.0161 - accuracy: 0.8918\n",
      "10000/10000 [==============================] - 0s 37us/sample - loss: 0.0158 - accuracy: 0.8938\n",
      "10000/10000 [==============================] - 0s 34us/sample - loss: 0.0156 - accuracy: 0.8953\n",
      "10000/10000 [==============================] - 0s 36us/sample - loss: 0.0155 - accuracy: 0.8969\n",
      "10000/10000 [==============================] - 1s 57us/sample - loss: 0.0154 - accuracy: 0.8989\n",
      "10000/10000 [==============================] - 0s 34us/sample - loss: 0.0153 - accuracy: 0.8992\n",
      "10000/10000 [==============================] - 0s 35us/sample - loss: 0.0152 - accuracy: 0.8989\n",
      "10000/10000 [==============================] - 0s 36us/sample - loss: 0.0152 - accuracy: 0.8994\n",
      "10000/10000 [==============================] - 0s 36us/sample - loss: 0.0153 - accuracy: 0.9006\n",
      "10000/10000 [==============================] - 0s 36us/sample - loss: 0.0153 - accuracy: 0.9001\n",
      "10000/10000 [==============================] - 0s 36us/sample - loss: 0.0154 - accuracy: 0.8993\n",
      "10000/10000 [==============================] - 0s 35us/sample - loss: 0.0156 - accuracy: 0.8987\n",
      "10000/10000 [==============================] - 0s 36us/sample - loss: 0.0158 - accuracy: 0.8965\n",
      "10000/10000 [==============================] - 0s 37us/sample - loss: 0.0160 - accuracy: 0.8952\n",
      "10000/10000 [==============================] - 0s 34us/sample - loss: 0.0163 - accuracy: 0.8942\n",
      "122th iteration\n",
      "10000/10000 [==============================] - 0s 36us/sample - loss: 0.0161 - accuracy: 0.8926\n",
      "10000/10000 [==============================] - 0s 36us/sample - loss: 0.0161 - accuracy: 0.8927\n",
      "10000/10000 [==============================] - 0s 36us/sample - loss: 0.0162 - accuracy: 0.8930\n",
      "10000/10000 [==============================] - 0s 36us/sample - loss: 0.0163 - accuracy: 0.8936\n",
      "10000/10000 [==============================] - 0s 34us/sample - loss: 0.0164 - accuracy: 0.8930\n",
      "10000/10000 [==============================] - 0s 36us/sample - loss: 0.0166 - accuracy: 0.8932\n",
      "10000/10000 [==============================] - 0s 36us/sample - loss: 0.0168 - accuracy: 0.8936\n",
      "10000/10000 [==============================] - 0s 35us/sample - loss: 0.0171 - accuracy: 0.8924\n",
      "10000/10000 [==============================] - 0s 36us/sample - loss: 0.0174 - accuracy: 0.8925\n",
      "10000/10000 [==============================] - 0s 36us/sample - loss: 0.0177 - accuracy: 0.8911\n",
      "10000/10000 [==============================] - 0s 36us/sample - loss: 0.0182 - accuracy: 0.8900\n",
      "10000/10000 [==============================] - 0s 35us/sample - loss: 0.0186 - accuracy: 0.8879\n",
      "10000/10000 [==============================] - 0s 34us/sample - loss: 0.0192 - accuracy: 0.8863\n",
      "10000/10000 [==============================] - 1s 56us/sample - loss: 0.0198 - accuracy: 0.8839\n",
      "10000/10000 [==============================] - 0s 36us/sample - loss: 0.0205 - accuracy: 0.8825\n",
      "10000/10000 [==============================] - 0s 36us/sample - loss: 0.0213 - accuracy: 0.8798\n",
      "10000/10000 [==============================] - 0s 34us/sample - loss: 0.0221 - accuracy: 0.8772\n",
      "10000/10000 [==============================] - 0s 36us/sample - loss: 0.0231 - accuracy: 0.8732\n",
      "10000/10000 [==============================] - 0s 36us/sample - loss: 0.0241 - accuracy: 0.8702\n",
      "10000/10000 [==============================] - 0s 37us/sample - loss: 0.0251 - accuracy: 0.8658\n",
      "10000/10000 [==============================] - 0s 36us/sample - loss: 0.0263 - accuracy: 0.8606\n",
      "123th iteration\n",
      "10000/10000 [==============================] - 0s 36us/sample - loss: 0.0155 - accuracy: 0.8978\n",
      "10000/10000 [==============================] - 0s 35us/sample - loss: 0.0153 - accuracy: 0.9008\n",
      "10000/10000 [==============================] - 0s 34us/sample - loss: 0.0151 - accuracy: 0.9027\n",
      "10000/10000 [==============================] - 0s 35us/sample - loss: 0.0149 - accuracy: 0.9042\n",
      "10000/10000 [==============================] - 0s 35us/sample - loss: 0.0148 - accuracy: 0.9052\n",
      "10000/10000 [==============================] - 0s 36us/sample - loss: 0.0147 - accuracy: 0.9061\n",
      "10000/10000 [==============================] - 0s 35us/sample - loss: 0.0146 - accuracy: 0.9063\n",
      "10000/10000 [==============================] - 0s 37us/sample - loss: 0.0146 - accuracy: 0.9070\n",
      "10000/10000 [==============================] - 0s 37us/sample - loss: 0.0146 - accuracy: 0.9072\n",
      "10000/10000 [==============================] - 0s 35us/sample - loss: 0.0146 - accuracy: 0.9086\n",
      "10000/10000 [==============================] - 0s 34us/sample - loss: 0.0146 - accuracy: 0.9082\n",
      "10000/10000 [==============================] - 0s 35us/sample - loss: 0.0147 - accuracy: 0.9077\n",
      "10000/10000 [==============================] - 0s 36us/sample - loss: 0.0148 - accuracy: 0.9067\n",
      "10000/10000 [==============================] - 0s 37us/sample - loss: 0.0149 - accuracy: 0.9064\n",
      "10000/10000 [==============================] - 0s 34us/sample - loss: 0.0150 - accuracy: 0.9053\n",
      "10000/10000 [==============================] - 0s 36us/sample - loss: 0.0152 - accuracy: 0.9039\n",
      "10000/10000 [==============================] - 0s 37us/sample - loss: 0.0154 - accuracy: 0.9027\n",
      "10000/10000 [==============================] - 0s 36us/sample - loss: 0.0157 - accuracy: 0.9014\n",
      "10000/10000 [==============================] - 0s 37us/sample - loss: 0.0160 - accuracy: 0.8994\n",
      "10000/10000 [==============================] - 0s 35us/sample - loss: 0.0163 - accuracy: 0.8974\n",
      "10000/10000 [==============================] - 0s 36us/sample - loss: 0.0167 - accuracy: 0.8959\n",
      "124th iteration\n",
      "10000/10000 [==============================] - 0s 36us/sample - loss: 0.0168 - accuracy: 0.8853\n",
      "10000/10000 [==============================] - 0s 36us/sample - loss: 0.0165 - accuracy: 0.8876\n",
      "10000/10000 [==============================] - 0s 36us/sample - loss: 0.0162 - accuracy: 0.8903\n",
      "10000/10000 [==============================] - 0s 34us/sample - loss: 0.0159 - accuracy: 0.8933\n",
      "10000/10000 [==============================] - 0s 35us/sample - loss: 0.0157 - accuracy: 0.8947\n",
      "10000/10000 [==============================] - 0s 34us/sample - loss: 0.0155 - accuracy: 0.8965\n",
      "10000/10000 [==============================] - 0s 36us/sample - loss: 0.0154 - accuracy: 0.8977\n",
      "10000/10000 [==============================] - 0s 36us/sample - loss: 0.0152 - accuracy: 0.8990\n",
      "10000/10000 [==============================] - 0s 36us/sample - loss: 0.0151 - accuracy: 0.8999\n",
      "10000/10000 [==============================] - 0s 35us/sample - loss: 0.0151 - accuracy: 0.9001\n",
      "10000/10000 [==============================] - 0s 35us/sample - loss: 0.0150 - accuracy: 0.9002\n",
      "10000/10000 [==============================] - 0s 38us/sample - loss: 0.0150 - accuracy: 0.9010\n",
      "10000/10000 [==============================] - 0s 34us/sample - loss: 0.0150 - accuracy: 0.9005\n",
      "10000/10000 [==============================] - 0s 35us/sample - loss: 0.0151 - accuracy: 0.9009\n",
      "10000/10000 [==============================] - 0s 35us/sample - loss: 0.0151 - accuracy: 0.9013\n",
      "10000/10000 [==============================] - 0s 34us/sample - loss: 0.0152 - accuracy: 0.9001\n",
      "10000/10000 [==============================] - 0s 35us/sample - loss: 0.0154 - accuracy: 0.8998\n",
      "10000/10000 [==============================] - 0s 36us/sample - loss: 0.0156 - accuracy: 0.8988\n",
      "10000/10000 [==============================] - 0s 36us/sample - loss: 0.0158 - accuracy: 0.8970\n",
      "10000/10000 [==============================] - 0s 34us/sample - loss: 0.0160 - accuracy: 0.8953\n",
      "10000/10000 [==============================] - 0s 36us/sample - loss: 0.0163 - accuracy: 0.8942\n",
      "125th iteration\n",
      "10000/10000 [==============================] - 0s 34us/sample - loss: 0.0168 - accuracy: 0.8853\n",
      "10000/10000 [==============================] - 0s 34us/sample - loss: 0.0165 - accuracy: 0.8881\n",
      "10000/10000 [==============================] - 0s 36us/sample - loss: 0.0162 - accuracy: 0.8905\n",
      "10000/10000 [==============================] - 0s 35us/sample - loss: 0.0159 - accuracy: 0.8930\n",
      "10000/10000 [==============================] - 0s 36us/sample - loss: 0.0157 - accuracy: 0.8942\n",
      "10000/10000 [==============================] - 0s 34us/sample - loss: 0.0155 - accuracy: 0.8951\n",
      "10000/10000 [==============================] - 0s 34us/sample - loss: 0.0154 - accuracy: 0.8966\n",
      "10000/10000 [==============================] - 0s 35us/sample - loss: 0.0152 - accuracy: 0.8983\n",
      "10000/10000 [==============================] - 0s 34us/sample - loss: 0.0151 - accuracy: 0.8993\n",
      "10000/10000 [==============================] - 0s 35us/sample - loss: 0.0151 - accuracy: 0.9010\n",
      "10000/10000 [==============================] - 0s 36us/sample - loss: 0.0150 - accuracy: 0.9013\n",
      "10000/10000 [==============================] - 0s 37us/sample - loss: 0.0150 - accuracy: 0.9021\n",
      "10000/10000 [==============================] - 0s 36us/sample - loss: 0.0150 - accuracy: 0.9016\n",
      "10000/10000 [==============================] - 0s 36us/sample - loss: 0.0150 - accuracy: 0.9017\n",
      "10000/10000 [==============================] - 0s 37us/sample - loss: 0.0151 - accuracy: 0.9019\n",
      "10000/10000 [==============================] - 0s 36us/sample - loss: 0.0152 - accuracy: 0.9008\n",
      "10000/10000 [==============================] - 0s 34us/sample - loss: 0.0153 - accuracy: 0.9000\n",
      "10000/10000 [==============================] - 0s 35us/sample - loss: 0.0155 - accuracy: 0.8998\n",
      "10000/10000 [==============================] - 0s 36us/sample - loss: 0.0157 - accuracy: 0.8993\n",
      "10000/10000 [==============================] - 0s 35us/sample - loss: 0.0159 - accuracy: 0.8980\n",
      "10000/10000 [==============================] - 0s 34us/sample - loss: 0.0161 - accuracy: 0.8961\n",
      "126th iteration\n",
      "10000/10000 [==============================] - 0s 36us/sample - loss: 0.0168 - accuracy: 0.8853\n",
      "10000/10000 [==============================] - 0s 34us/sample - loss: 0.0165 - accuracy: 0.8876\n",
      "10000/10000 [==============================] - 0s 34us/sample - loss: 0.0163 - accuracy: 0.8902\n",
      "10000/10000 [==============================] - 0s 34us/sample - loss: 0.0161 - accuracy: 0.8909\n",
      "10000/10000 [==============================] - 0s 34us/sample - loss: 0.0160 - accuracy: 0.8929\n",
      "10000/10000 [==============================] - 0s 36us/sample - loss: 0.0159 - accuracy: 0.8942\n",
      "10000/10000 [==============================] - 0s 36us/sample - loss: 0.0158 - accuracy: 0.8951\n",
      "10000/10000 [==============================] - 0s 37us/sample - loss: 0.0158 - accuracy: 0.8967\n",
      "10000/10000 [==============================] - 0s 36us/sample - loss: 0.0157 - accuracy: 0.8972\n",
      "10000/10000 [==============================] - 0s 34us/sample - loss: 0.0158 - accuracy: 0.8971\n",
      "10000/10000 [==============================] - 0s 36us/sample - loss: 0.0158 - accuracy: 0.8973\n",
      "10000/10000 [==============================] - 0s 36us/sample - loss: 0.0160 - accuracy: 0.8983\n",
      "10000/10000 [==============================] - 0s 36us/sample - loss: 0.0161 - accuracy: 0.8984\n",
      "10000/10000 [==============================] - 0s 36us/sample - loss: 0.0163 - accuracy: 0.8974\n",
      "10000/10000 [==============================] - 0s 37us/sample - loss: 0.0166 - accuracy: 0.8959\n",
      "10000/10000 [==============================] - 0s 35us/sample - loss: 0.0168 - accuracy: 0.8946\n",
      "10000/10000 [==============================] - 0s 36us/sample - loss: 0.0172 - accuracy: 0.8935\n",
      "10000/10000 [==============================] - 0s 36us/sample - loss: 0.0176 - accuracy: 0.8910\n",
      "10000/10000 [==============================] - 0s 36us/sample - loss: 0.0180 - accuracy: 0.8897\n",
      "10000/10000 [==============================] - 0s 34us/sample - loss: 0.0185 - accuracy: 0.8858\n",
      "10000/10000 [==============================] - 0s 34us/sample - loss: 0.0191 - accuracy: 0.8818\n",
      "127th iteration\n",
      "10000/10000 [==============================] - 0s 36us/sample - loss: 0.0168 - accuracy: 0.8853\n",
      "10000/10000 [==============================] - 0s 34us/sample - loss: 0.0165 - accuracy: 0.8876\n",
      "10000/10000 [==============================] - 0s 34us/sample - loss: 0.0162 - accuracy: 0.8895\n",
      "10000/10000 [==============================] - 0s 36us/sample - loss: 0.0160 - accuracy: 0.8922\n",
      "10000/10000 [==============================] - 0s 35us/sample - loss: 0.0158 - accuracy: 0.8932\n",
      "10000/10000 [==============================] - 0s 34us/sample - loss: 0.0156 - accuracy: 0.8947\n",
      "10000/10000 [==============================] - 0s 34us/sample - loss: 0.0155 - accuracy: 0.8953\n",
      "10000/10000 [==============================] - 0s 35us/sample - loss: 0.0153 - accuracy: 0.8965\n",
      "10000/10000 [==============================] - 0s 36us/sample - loss: 0.0152 - accuracy: 0.8966\n",
      "10000/10000 [==============================] - 0s 36us/sample - loss: 0.0152 - accuracy: 0.8975\n",
      "10000/10000 [==============================] - 0s 35us/sample - loss: 0.0151 - accuracy: 0.8988\n",
      "10000/10000 [==============================] - 0s 35us/sample - loss: 0.0151 - accuracy: 0.8985\n",
      "10000/10000 [==============================] - 0s 36us/sample - loss: 0.0151 - accuracy: 0.8996\n",
      "10000/10000 [==============================] - 0s 36us/sample - loss: 0.0151 - accuracy: 0.8990\n",
      "10000/10000 [==============================] - 0s 36us/sample - loss: 0.0152 - accuracy: 0.8991\n",
      "10000/10000 [==============================] - 0s 37us/sample - loss: 0.0153 - accuracy: 0.8988\n",
      "10000/10000 [==============================] - 0s 34us/sample - loss: 0.0154 - accuracy: 0.8979\n",
      "10000/10000 [==============================] - 0s 35us/sample - loss: 0.0155 - accuracy: 0.8969\n",
      "10000/10000 [==============================] - 0s 35us/sample - loss: 0.0157 - accuracy: 0.8965\n",
      "10000/10000 [==============================] - 0s 36us/sample - loss: 0.0159 - accuracy: 0.8949\n",
      "10000/10000 [==============================] - 0s 37us/sample - loss: 0.0161 - accuracy: 0.8926\n",
      "128th iteration\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000/10000 [==============================] - 0s 35us/sample - loss: 0.0168 - accuracy: 0.8853\n",
      "10000/10000 [==============================] - 0s 36us/sample - loss: 0.0166 - accuracy: 0.8866\n",
      "10000/10000 [==============================] - 0s 35us/sample - loss: 0.0165 - accuracy: 0.8882\n",
      "10000/10000 [==============================] - 0s 36us/sample - loss: 0.0164 - accuracy: 0.8897\n",
      "10000/10000 [==============================] - 0s 37us/sample - loss: 0.0163 - accuracy: 0.8902\n",
      "10000/10000 [==============================] - 0s 35us/sample - loss: 0.0163 - accuracy: 0.8913\n",
      "10000/10000 [==============================] - 0s 37us/sample - loss: 0.0163 - accuracy: 0.8922\n",
      "10000/10000 [==============================] - 0s 34us/sample - loss: 0.0163 - accuracy: 0.8930\n",
      "10000/10000 [==============================] - 0s 36us/sample - loss: 0.0164 - accuracy: 0.8928\n",
      "10000/10000 [==============================] - 0s 34us/sample - loss: 0.0165 - accuracy: 0.8930\n",
      "10000/10000 [==============================] - 0s 34us/sample - loss: 0.0166 - accuracy: 0.8938\n",
      "10000/10000 [==============================] - 0s 35us/sample - loss: 0.0168 - accuracy: 0.8946\n",
      "10000/10000 [==============================] - 0s 34us/sample - loss: 0.0170 - accuracy: 0.8943\n",
      "10000/10000 [==============================] - 0s 34us/sample - loss: 0.0173 - accuracy: 0.8932\n",
      "10000/10000 [==============================] - 0s 35us/sample - loss: 0.0176 - accuracy: 0.8930\n",
      "10000/10000 [==============================] - 0s 36us/sample - loss: 0.0180 - accuracy: 0.8931\n",
      "10000/10000 [==============================] - 0s 35us/sample - loss: 0.0184 - accuracy: 0.8910\n",
      "10000/10000 [==============================] - 1s 58us/sample - loss: 0.0189 - accuracy: 0.8884\n",
      "10000/10000 [==============================] - 0s 35us/sample - loss: 0.0194 - accuracy: 0.8858\n",
      "10000/10000 [==============================] - 0s 34us/sample - loss: 0.0200 - accuracy: 0.8841\n",
      "10000/10000 [==============================] - 0s 36us/sample - loss: 0.0206 - accuracy: 0.8813\n",
      "129th iteration\n",
      "10000/10000 [==============================] - 0s 34us/sample - loss: 0.0168 - accuracy: 0.8853\n",
      "10000/10000 [==============================] - 0s 34us/sample - loss: 0.0164 - accuracy: 0.8884\n",
      "10000/10000 [==============================] - 0s 36us/sample - loss: 0.0162 - accuracy: 0.8907\n",
      "10000/10000 [==============================] - 0s 35us/sample - loss: 0.0159 - accuracy: 0.8933\n",
      "10000/10000 [==============================] - 0s 35us/sample - loss: 0.0157 - accuracy: 0.8948\n",
      "10000/10000 [==============================] - 0s 35us/sample - loss: 0.0155 - accuracy: 0.8970\n",
      "10000/10000 [==============================] - 0s 45us/sample - loss: 0.0153 - accuracy: 0.8979\n",
      "10000/10000 [==============================] - 0s 35us/sample - loss: 0.0152 - accuracy: 0.8987\n",
      "10000/10000 [==============================] - 0s 34us/sample - loss: 0.0151 - accuracy: 0.8985\n",
      "10000/10000 [==============================] - 0s 36us/sample - loss: 0.0150 - accuracy: 0.9001\n",
      "10000/10000 [==============================] - 0s 36us/sample - loss: 0.0150 - accuracy: 0.9003\n",
      "10000/10000 [==============================] - 0s 36us/sample - loss: 0.0150 - accuracy: 0.9010\n",
      "10000/10000 [==============================] - 0s 34us/sample - loss: 0.0150 - accuracy: 0.9012\n",
      "10000/10000 [==============================] - 0s 36us/sample - loss: 0.0151 - accuracy: 0.9020\n",
      "10000/10000 [==============================] - 0s 35us/sample - loss: 0.0152 - accuracy: 0.9031\n",
      "10000/10000 [==============================] - 0s 34us/sample - loss: 0.0153 - accuracy: 0.9024\n",
      "10000/10000 [==============================] - 0s 36us/sample - loss: 0.0155 - accuracy: 0.9017\n",
      "10000/10000 [==============================] - 0s 34us/sample - loss: 0.0158 - accuracy: 0.9009\n",
      "10000/10000 [==============================] - 0s 34us/sample - loss: 0.0160 - accuracy: 0.8992\n",
      "10000/10000 [==============================] - 0s 35us/sample - loss: 0.0163 - accuracy: 0.8973\n",
      "10000/10000 [==============================] - 0s 36us/sample - loss: 0.0167 - accuracy: 0.8959\n",
      "130th iteration\n",
      "10000/10000 [==============================] - 0s 38us/sample - loss: 0.0156 - accuracy: 0.8985\n",
      "10000/10000 [==============================] - 0s 35us/sample - loss: 0.0154 - accuracy: 0.8986\n",
      "10000/10000 [==============================] - 0s 34us/sample - loss: 0.0152 - accuracy: 0.8994\n",
      "10000/10000 [==============================] - 0s 35us/sample - loss: 0.0151 - accuracy: 0.9005\n",
      "10000/10000 [==============================] - 0s 36us/sample - loss: 0.0149 - accuracy: 0.9013\n",
      "10000/10000 [==============================] - 0s 36us/sample - loss: 0.0148 - accuracy: 0.9019\n",
      "10000/10000 [==============================] - 0s 34us/sample - loss: 0.0148 - accuracy: 0.9023\n",
      "10000/10000 [==============================] - 0s 36us/sample - loss: 0.0147 - accuracy: 0.9024\n",
      "10000/10000 [==============================] - 0s 34us/sample - loss: 0.0147 - accuracy: 0.9015\n",
      "10000/10000 [==============================] - 0s 36us/sample - loss: 0.0148 - accuracy: 0.9027\n",
      "10000/10000 [==============================] - 0s 34us/sample - loss: 0.0148 - accuracy: 0.9025\n",
      "10000/10000 [==============================] - 0s 35us/sample - loss: 0.0149 - accuracy: 0.9025\n",
      "10000/10000 [==============================] - 0s 35us/sample - loss: 0.0150 - accuracy: 0.9007\n",
      "10000/10000 [==============================] - 0s 36us/sample - loss: 0.0151 - accuracy: 0.9000\n",
      "10000/10000 [==============================] - 0s 37us/sample - loss: 0.0153 - accuracy: 0.8991\n",
      "10000/10000 [==============================] - 0s 34us/sample - loss: 0.0155 - accuracy: 0.8974\n",
      "10000/10000 [==============================] - 0s 34us/sample - loss: 0.0157 - accuracy: 0.8957\n",
      "10000/10000 [==============================] - 0s 35us/sample - loss: 0.0159 - accuracy: 0.8942\n",
      "10000/10000 [==============================] - 0s 34us/sample - loss: 0.0162 - accuracy: 0.8914\n",
      "10000/10000 [==============================] - 0s 36us/sample - loss: 0.0165 - accuracy: 0.8894\n",
      "10000/10000 [==============================] - 0s 35us/sample - loss: 0.0168 - accuracy: 0.8870\n",
      "131th iteration\n",
      "10000/10000 [==============================] - 0s 36us/sample - loss: 0.0156 - accuracy: 0.8985\n",
      "10000/10000 [==============================] - 0s 34us/sample - loss: 0.0156 - accuracy: 0.8980\n",
      "10000/10000 [==============================] - 0s 34us/sample - loss: 0.0156 - accuracy: 0.8984\n",
      "10000/10000 [==============================] - 0s 35us/sample - loss: 0.0156 - accuracy: 0.8982\n",
      "10000/10000 [==============================] - 0s 36us/sample - loss: 0.0157 - accuracy: 0.8984\n",
      "10000/10000 [==============================] - 0s 37us/sample - loss: 0.0158 - accuracy: 0.8987\n",
      "10000/10000 [==============================] - 0s 35us/sample - loss: 0.0160 - accuracy: 0.8999\n",
      "10000/10000 [==============================] - 0s 36us/sample - loss: 0.0161 - accuracy: 0.8999\n",
      "10000/10000 [==============================] - 0s 36us/sample - loss: 0.0164 - accuracy: 0.8990\n",
      "10000/10000 [==============================] - 0s 36us/sample - loss: 0.0166 - accuracy: 0.8983\n",
      "10000/10000 [==============================] - 0s 36us/sample - loss: 0.0169 - accuracy: 0.8969\n",
      "10000/10000 [==============================] - 0s 34us/sample - loss: 0.0173 - accuracy: 0.8955\n",
      "10000/10000 [==============================] - 0s 37us/sample - loss: 0.0177 - accuracy: 0.8945\n",
      "10000/10000 [==============================] - 0s 35us/sample - loss: 0.0181 - accuracy: 0.8929\n",
      "10000/10000 [==============================] - 0s 37us/sample - loss: 0.0186 - accuracy: 0.8896\n",
      "10000/10000 [==============================] - 0s 34us/sample - loss: 0.0191 - accuracy: 0.8874\n",
      "10000/10000 [==============================] - 0s 36us/sample - loss: 0.0197 - accuracy: 0.8851\n",
      "10000/10000 [==============================] - 0s 37us/sample - loss: 0.0203 - accuracy: 0.8822\n",
      "10000/10000 [==============================] - 0s 36us/sample - loss: 0.0209 - accuracy: 0.8793\n",
      "10000/10000 [==============================] - 0s 37us/sample - loss: 0.0217 - accuracy: 0.8744\n",
      "10000/10000 [==============================] - 0s 35us/sample - loss: 0.0224 - accuracy: 0.8711\n",
      "132th iteration\n",
      "10000/10000 [==============================] - 0s 36us/sample - loss: 0.0156 - accuracy: 0.8985\n",
      "10000/10000 [==============================] - 0s 36us/sample - loss: 0.0153 - accuracy: 0.8994\n",
      "10000/10000 [==============================] - 0s 36us/sample - loss: 0.0151 - accuracy: 0.9007\n",
      "10000/10000 [==============================] - 0s 37us/sample - loss: 0.0149 - accuracy: 0.9010\n",
      "10000/10000 [==============================] - 0s 34us/sample - loss: 0.0147 - accuracy: 0.9021\n",
      "10000/10000 [==============================] - 0s 34us/sample - loss: 0.0146 - accuracy: 0.9036\n",
      "10000/10000 [==============================] - 0s 36us/sample - loss: 0.0145 - accuracy: 0.9050\n",
      "10000/10000 [==============================] - 0s 34us/sample - loss: 0.0144 - accuracy: 0.9052\n",
      "10000/10000 [==============================] - 0s 36us/sample - loss: 0.0144 - accuracy: 0.9051\n",
      "10000/10000 [==============================] - 0s 36us/sample - loss: 0.0144 - accuracy: 0.9052\n",
      "10000/10000 [==============================] - 0s 34us/sample - loss: 0.0144 - accuracy: 0.9053\n",
      "10000/10000 [==============================] - 0s 35us/sample - loss: 0.0144 - accuracy: 0.9048\n",
      "10000/10000 [==============================] - 0s 34us/sample - loss: 0.0145 - accuracy: 0.9041\n",
      "10000/10000 [==============================] - 0s 35us/sample - loss: 0.0146 - accuracy: 0.9042\n",
      "10000/10000 [==============================] - 0s 35us/sample - loss: 0.0147 - accuracy: 0.9030\n",
      "10000/10000 [==============================] - 0s 35us/sample - loss: 0.0149 - accuracy: 0.9019\n",
      "10000/10000 [==============================] - 0s 34us/sample - loss: 0.0151 - accuracy: 0.9012\n",
      "10000/10000 [==============================] - 0s 35us/sample - loss: 0.0153 - accuracy: 0.8989\n",
      "10000/10000 [==============================] - 0s 36us/sample - loss: 0.0156 - accuracy: 0.8971\n",
      "10000/10000 [==============================] - 0s 36us/sample - loss: 0.0158 - accuracy: 0.8948\n",
      "10000/10000 [==============================] - 0s 34us/sample - loss: 0.0161 - accuracy: 0.8926\n",
      "133th iteration\n",
      "10000/10000 [==============================] - 0s 36us/sample - loss: 0.0155 - accuracy: 0.8978\n",
      "10000/10000 [==============================] - 0s 36us/sample - loss: 0.0154 - accuracy: 0.9005\n",
      "10000/10000 [==============================] - 0s 36us/sample - loss: 0.0154 - accuracy: 0.9012\n",
      "10000/10000 [==============================] - 0s 36us/sample - loss: 0.0154 - accuracy: 0.9017\n",
      "10000/10000 [==============================] - 0s 35us/sample - loss: 0.0154 - accuracy: 0.9021\n",
      "10000/10000 [==============================] - 0s 34us/sample - loss: 0.0154 - accuracy: 0.9024\n",
      "10000/10000 [==============================] - 0s 37us/sample - loss: 0.0155 - accuracy: 0.9016\n",
      "10000/10000 [==============================] - 0s 36us/sample - loss: 0.0156 - accuracy: 0.9011\n",
      "10000/10000 [==============================] - 0s 36us/sample - loss: 0.0157 - accuracy: 0.9016\n",
      "10000/10000 [==============================] - 0s 35us/sample - loss: 0.0159 - accuracy: 0.9011\n",
      "10000/10000 [==============================] - 0s 34us/sample - loss: 0.0161 - accuracy: 0.9016\n",
      "10000/10000 [==============================] - 0s 35us/sample - loss: 0.0164 - accuracy: 0.9008\n",
      "10000/10000 [==============================] - 0s 36us/sample - loss: 0.0166 - accuracy: 0.9005\n",
      "10000/10000 [==============================] - 0s 35us/sample - loss: 0.0170 - accuracy: 0.8999\n",
      "10000/10000 [==============================] - 0s 37us/sample - loss: 0.0174 - accuracy: 0.8991\n",
      "10000/10000 [==============================] - 0s 36us/sample - loss: 0.0178 - accuracy: 0.8961\n",
      "10000/10000 [==============================] - 0s 36us/sample - loss: 0.0183 - accuracy: 0.8945\n",
      "10000/10000 [==============================] - 0s 36us/sample - loss: 0.0188 - accuracy: 0.8923\n",
      "10000/10000 [==============================] - 1s 58us/sample - loss: 0.0193 - accuracy: 0.8891\n",
      "10000/10000 [==============================] - 0s 36us/sample - loss: 0.0199 - accuracy: 0.8852\n",
      "10000/10000 [==============================] - 0s 38us/sample - loss: 0.0206 - accuracy: 0.8813\n",
      "134th iteration\n",
      "10000/10000 [==============================] - 0s 35us/sample - loss: 0.0168 - accuracy: 0.8853\n",
      "10000/10000 [==============================] - 0s 36us/sample - loss: 0.0164 - accuracy: 0.8881\n",
      "10000/10000 [==============================] - 0s 35us/sample - loss: 0.0161 - accuracy: 0.8900\n",
      "10000/10000 [==============================] - 0s 36us/sample - loss: 0.0158 - accuracy: 0.8920\n",
      "10000/10000 [==============================] - 0s 35us/sample - loss: 0.0156 - accuracy: 0.8944\n",
      "10000/10000 [==============================] - 0s 36us/sample - loss: 0.0154 - accuracy: 0.8975\n",
      "10000/10000 [==============================] - 0s 37us/sample - loss: 0.0152 - accuracy: 0.8983\n",
      "10000/10000 [==============================] - 0s 36us/sample - loss: 0.0150 - accuracy: 0.8999\n",
      "10000/10000 [==============================] - 0s 36us/sample - loss: 0.0149 - accuracy: 0.9006\n",
      "10000/10000 [==============================] - 0s 35us/sample - loss: 0.0148 - accuracy: 0.9010\n",
      "10000/10000 [==============================] - 0s 34us/sample - loss: 0.0148 - accuracy: 0.9009\n",
      "10000/10000 [==============================] - 0s 35us/sample - loss: 0.0147 - accuracy: 0.9007\n",
      "10000/10000 [==============================] - 0s 36us/sample - loss: 0.0148 - accuracy: 0.9020\n",
      "10000/10000 [==============================] - 0s 36us/sample - loss: 0.0148 - accuracy: 0.9024\n",
      "10000/10000 [==============================] - 0s 37us/sample - loss: 0.0148 - accuracy: 0.9020\n",
      "10000/10000 [==============================] - 0s 34us/sample - loss: 0.0149 - accuracy: 0.9019\n",
      "10000/10000 [==============================] - 0s 35us/sample - loss: 0.0151 - accuracy: 0.9008\n",
      "10000/10000 [==============================] - 0s 34us/sample - loss: 0.0152 - accuracy: 0.9004\n",
      "10000/10000 [==============================] - 0s 35us/sample - loss: 0.0154 - accuracy: 0.8994\n",
      "10000/10000 [==============================] - 0s 36us/sample - loss: 0.0156 - accuracy: 0.8970\n",
      "10000/10000 [==============================] - 0s 35us/sample - loss: 0.0159 - accuracy: 0.8943\n",
      "135th iteration\n",
      "10000/10000 [==============================] - 1s 57us/sample - loss: 0.0180 - accuracy: 0.8781\n",
      "10000/10000 [==============================] - 0s 34us/sample - loss: 0.0177 - accuracy: 0.8807\n",
      "10000/10000 [==============================] - 0s 37us/sample - loss: 0.0174 - accuracy: 0.8831\n",
      "10000/10000 [==============================] - 0s 36us/sample - loss: 0.0171 - accuracy: 0.8850\n",
      "10000/10000 [==============================] - 0s 35us/sample - loss: 0.0168 - accuracy: 0.8864\n",
      "10000/10000 [==============================] - 0s 36us/sample - loss: 0.0166 - accuracy: 0.8881\n",
      "10000/10000 [==============================] - 0s 34us/sample - loss: 0.0165 - accuracy: 0.8886\n",
      "10000/10000 [==============================] - 0s 35us/sample - loss: 0.0163 - accuracy: 0.8906\n",
      "10000/10000 [==============================] - 0s 36us/sample - loss: 0.0162 - accuracy: 0.8928\n",
      "10000/10000 [==============================] - 0s 37us/sample - loss: 0.0162 - accuracy: 0.8948\n",
      "10000/10000 [==============================] - 0s 36us/sample - loss: 0.0162 - accuracy: 0.8950\n",
      "10000/10000 [==============================] - 0s 36us/sample - loss: 0.0162 - accuracy: 0.8955\n",
      "10000/10000 [==============================] - 0s 35us/sample - loss: 0.0163 - accuracy: 0.8970\n",
      "10000/10000 [==============================] - 0s 36us/sample - loss: 0.0165 - accuracy: 0.8963\n",
      "10000/10000 [==============================] - 0s 35us/sample - loss: 0.0167 - accuracy: 0.8961\n",
      "10000/10000 [==============================] - 0s 34us/sample - loss: 0.0169 - accuracy: 0.8950\n",
      "10000/10000 [==============================] - 0s 34us/sample - loss: 0.0173 - accuracy: 0.8941\n",
      "10000/10000 [==============================] - 0s 34us/sample - loss: 0.0176 - accuracy: 0.8916\n",
      "10000/10000 [==============================] - 0s 34us/sample - loss: 0.0180 - accuracy: 0.8882\n",
      "10000/10000 [==============================] - 0s 35us/sample - loss: 0.0185 - accuracy: 0.8849\n",
      "10000/10000 [==============================] - 0s 36us/sample - loss: 0.0191 - accuracy: 0.8818\n",
      "136th iteration\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000/10000 [==============================] - 0s 36us/sample - loss: 0.0163 - accuracy: 0.8942\n",
      "10000/10000 [==============================] - 0s 36us/sample - loss: 0.0164 - accuracy: 0.8931\n",
      "10000/10000 [==============================] - 0s 34us/sample - loss: 0.0166 - accuracy: 0.8933\n",
      "10000/10000 [==============================] - 0s 36us/sample - loss: 0.0169 - accuracy: 0.8926\n",
      "10000/10000 [==============================] - 0s 34us/sample - loss: 0.0172 - accuracy: 0.8930\n",
      "10000/10000 [==============================] - 0s 36us/sample - loss: 0.0176 - accuracy: 0.8920\n",
      "10000/10000 [==============================] - 0s 34us/sample - loss: 0.0182 - accuracy: 0.8908\n",
      "10000/10000 [==============================] - 0s 34us/sample - loss: 0.0188 - accuracy: 0.8887\n",
      "10000/10000 [==============================] - 0s 35us/sample - loss: 0.0195 - accuracy: 0.8878\n",
      "10000/10000 [==============================] - 0s 36us/sample - loss: 0.0203 - accuracy: 0.8843\n",
      "10000/10000 [==============================] - 0s 37us/sample - loss: 0.0212 - accuracy: 0.8814\n",
      "10000/10000 [==============================] - 0s 36us/sample - loss: 0.0222 - accuracy: 0.8784\n",
      "10000/10000 [==============================] - 0s 35us/sample - loss: 0.0233 - accuracy: 0.8738\n",
      "10000/10000 [==============================] - 0s 36us/sample - loss: 0.0245 - accuracy: 0.8689\n",
      "10000/10000 [==============================] - 0s 36us/sample - loss: 0.0259 - accuracy: 0.8610\n",
      "10000/10000 [==============================] - 0s 34us/sample - loss: 0.0273 - accuracy: 0.8516\n",
      "10000/10000 [==============================] - 0s 34us/sample - loss: 0.0288 - accuracy: 0.8449\n",
      "10000/10000 [==============================] - 0s 35us/sample - loss: 0.0303 - accuracy: 0.8370\n",
      "10000/10000 [==============================] - 0s 36us/sample - loss: 0.0320 - accuracy: 0.8250\n",
      "10000/10000 [==============================] - 0s 36us/sample - loss: 0.0337 - accuracy: 0.8145\n",
      "10000/10000 [==============================] - 0s 34us/sample - loss: 0.0355 - accuracy: 0.8016\n",
      "137th iteration\n",
      "10000/10000 [==============================] - 0s 36us/sample - loss: 0.0155 - accuracy: 0.8978\n",
      "10000/10000 [==============================] - 0s 37us/sample - loss: 0.0153 - accuracy: 0.9009\n",
      "10000/10000 [==============================] - 0s 35us/sample - loss: 0.0150 - accuracy: 0.9028\n",
      "10000/10000 [==============================] - 0s 36us/sample - loss: 0.0148 - accuracy: 0.9040\n",
      "10000/10000 [==============================] - 0s 34us/sample - loss: 0.0146 - accuracy: 0.9058\n",
      "10000/10000 [==============================] - 0s 34us/sample - loss: 0.0145 - accuracy: 0.9065\n",
      "10000/10000 [==============================] - 0s 37us/sample - loss: 0.0144 - accuracy: 0.9077\n",
      "10000/10000 [==============================] - 0s 34us/sample - loss: 0.0143 - accuracy: 0.9084\n",
      "10000/10000 [==============================] - 0s 34us/sample - loss: 0.0142 - accuracy: 0.9088\n",
      "10000/10000 [==============================] - 0s 34us/sample - loss: 0.0142 - accuracy: 0.9095\n",
      "10000/10000 [==============================] - 0s 34us/sample - loss: 0.0141 - accuracy: 0.9095\n",
      "10000/10000 [==============================] - 0s 34us/sample - loss: 0.0141 - accuracy: 0.9096\n",
      "10000/10000 [==============================] - 0s 34us/sample - loss: 0.0141 - accuracy: 0.9108\n",
      "10000/10000 [==============================] - 0s 36us/sample - loss: 0.0142 - accuracy: 0.9101\n",
      "10000/10000 [==============================] - 0s 35us/sample - loss: 0.0142 - accuracy: 0.9103\n",
      "10000/10000 [==============================] - 0s 36us/sample - loss: 0.0143 - accuracy: 0.9099\n",
      "10000/10000 [==============================] - 0s 35us/sample - loss: 0.0144 - accuracy: 0.9088\n",
      "10000/10000 [==============================] - 0s 35us/sample - loss: 0.0146 - accuracy: 0.9071\n",
      "10000/10000 [==============================] - 0s 34us/sample - loss: 0.0147 - accuracy: 0.9056\n",
      "10000/10000 [==============================] - 0s 36us/sample - loss: 0.0149 - accuracy: 0.9044\n",
      "10000/10000 [==============================] - 0s 36us/sample - loss: 0.0151 - accuracy: 0.9033\n",
      "138th iteration\n",
      "10000/10000 [==============================] - 0s 36us/sample - loss: 0.0155 - accuracy: 0.8978\n",
      "10000/10000 [==============================] - 0s 38us/sample - loss: 0.0153 - accuracy: 0.9008\n",
      "10000/10000 [==============================] - 0s 36us/sample - loss: 0.0151 - accuracy: 0.9028\n",
      "10000/10000 [==============================] - ETA: 0s - loss: 0.0157 - accuracy: 0.90 - 0s 35us/sample - loss: 0.0149 - accuracy: 0.9053\n",
      "10000/10000 [==============================] - 0s 35us/sample - loss: 0.0147 - accuracy: 0.9060\n",
      "10000/10000 [==============================] - 0s 36us/sample - loss: 0.0146 - accuracy: 0.9067\n",
      "10000/10000 [==============================] - 0s 37us/sample - loss: 0.0145 - accuracy: 0.9076\n",
      "10000/10000 [==============================] - 0s 34us/sample - loss: 0.0145 - accuracy: 0.9084\n",
      "10000/10000 [==============================] - 0s 36us/sample - loss: 0.0144 - accuracy: 0.9090\n",
      "10000/10000 [==============================] - 0s 34us/sample - loss: 0.0144 - accuracy: 0.9085\n",
      "10000/10000 [==============================] - 0s 36us/sample - loss: 0.0144 - accuracy: 0.9088\n",
      "10000/10000 [==============================] - 0s 36us/sample - loss: 0.0145 - accuracy: 0.9074\n",
      "10000/10000 [==============================] - 0s 34us/sample - loss: 0.0146 - accuracy: 0.9074\n",
      "10000/10000 [==============================] - 0s 34us/sample - loss: 0.0147 - accuracy: 0.9076\n",
      "10000/10000 [==============================] - 0s 37us/sample - loss: 0.0148 - accuracy: 0.9060\n",
      "10000/10000 [==============================] - 0s 36us/sample - loss: 0.0149 - accuracy: 0.9043\n",
      "10000/10000 [==============================] - 0s 35us/sample - loss: 0.0151 - accuracy: 0.9021\n",
      "10000/10000 [==============================] - 0s 34us/sample - loss: 0.0153 - accuracy: 0.9012\n",
      "10000/10000 [==============================] - 0s 36us/sample - loss: 0.0156 - accuracy: 0.9001\n",
      "10000/10000 [==============================] - 0s 35us/sample - loss: 0.0158 - accuracy: 0.8985\n",
      "10000/10000 [==============================] - 0s 34us/sample - loss: 0.0161 - accuracy: 0.8961\n",
      "139th iteration\n",
      "10000/10000 [==============================] - 0s 34us/sample - loss: 0.0168 - accuracy: 0.8853\n",
      "10000/10000 [==============================] - 0s 34us/sample - loss: 0.0164 - accuracy: 0.8888\n",
      "10000/10000 [==============================] - 0s 36us/sample - loss: 0.0160 - accuracy: 0.8920\n",
      "10000/10000 [==============================] - 0s 35us/sample - loss: 0.0157 - accuracy: 0.8943\n",
      "10000/10000 [==============================] - 0s 35us/sample - loss: 0.0154 - accuracy: 0.8975\n",
      "10000/10000 [==============================] - 0s 34us/sample - loss: 0.0152 - accuracy: 0.9005\n",
      "10000/10000 [==============================] - 0s 35us/sample - loss: 0.0149 - accuracy: 0.9018\n",
      "10000/10000 [==============================] - 0s 37us/sample - loss: 0.0147 - accuracy: 0.9037\n",
      "10000/10000 [==============================] - 0s 34us/sample - loss: 0.0146 - accuracy: 0.9052\n",
      "10000/10000 [==============================] - 0s 36us/sample - loss: 0.0145 - accuracy: 0.9062\n",
      "10000/10000 [==============================] - 0s 34us/sample - loss: 0.0144 - accuracy: 0.9067\n",
      "10000/10000 [==============================] - 0s 34us/sample - loss: 0.0143 - accuracy: 0.9070\n",
      "10000/10000 [==============================] - 0s 36us/sample - loss: 0.0143 - accuracy: 0.9073\n",
      "10000/10000 [==============================] - 0s 36us/sample - loss: 0.0143 - accuracy: 0.9082\n",
      "10000/10000 [==============================] - 0s 36us/sample - loss: 0.0143 - accuracy: 0.9086\n",
      "10000/10000 [==============================] - 0s 36us/sample - loss: 0.0143 - accuracy: 0.9089\n",
      "10000/10000 [==============================] - 0s 36us/sample - loss: 0.0144 - accuracy: 0.9072\n",
      "10000/10000 [==============================] - 0s 37us/sample - loss: 0.0146 - accuracy: 0.9061\n",
      "10000/10000 [==============================] - 0s 36us/sample - loss: 0.0147 - accuracy: 0.9045\n",
      "10000/10000 [==============================] - 0s 35us/sample - loss: 0.0149 - accuracy: 0.9037\n",
      "10000/10000 [==============================] - 0s 34us/sample - loss: 0.0151 - accuracy: 0.9033\n",
      "140th iteration\n",
      "10000/10000 [==============================] - 0s 34us/sample - loss: 0.0180 - accuracy: 0.8781\n",
      "10000/10000 [==============================] - 1s 56us/sample - loss: 0.0176 - accuracy: 0.8808\n",
      "10000/10000 [==============================] - 0s 34us/sample - loss: 0.0173 - accuracy: 0.8830\n",
      "10000/10000 [==============================] - 0s 35us/sample - loss: 0.0170 - accuracy: 0.8857\n",
      "10000/10000 [==============================] - 0s 36us/sample - loss: 0.0167 - accuracy: 0.8867\n",
      "10000/10000 [==============================] - 0s 36us/sample - loss: 0.0164 - accuracy: 0.8880\n",
      "10000/10000 [==============================] - 0s 34us/sample - loss: 0.0161 - accuracy: 0.8892\n",
      "10000/10000 [==============================] - 0s 35us/sample - loss: 0.0159 - accuracy: 0.8911\n",
      "10000/10000 [==============================] - 0s 34us/sample - loss: 0.0157 - accuracy: 0.8935\n",
      "10000/10000 [==============================] - 0s 36us/sample - loss: 0.0156 - accuracy: 0.8962\n",
      "10000/10000 [==============================] - 0s 36us/sample - loss: 0.0154 - accuracy: 0.8977\n",
      "10000/10000 [==============================] - 0s 35us/sample - loss: 0.0154 - accuracy: 0.8986\n",
      "10000/10000 [==============================] - 0s 36us/sample - loss: 0.0153 - accuracy: 0.8996\n",
      "10000/10000 [==============================] - 0s 36us/sample - loss: 0.0153 - accuracy: 0.8994\n",
      "10000/10000 [==============================] - 0s 36us/sample - loss: 0.0153 - accuracy: 0.8987\n",
      "10000/10000 [==============================] - 0s 36us/sample - loss: 0.0153 - accuracy: 0.8987\n",
      "10000/10000 [==============================] - 0s 34us/sample - loss: 0.0154 - accuracy: 0.8983\n",
      "10000/10000 [==============================] - 0s 36us/sample - loss: 0.0155 - accuracy: 0.8992\n",
      "10000/10000 [==============================] - 0s 33us/sample - loss: 0.0157 - accuracy: 0.8988\n",
      "10000/10000 [==============================] - 0s 34us/sample - loss: 0.0159 - accuracy: 0.8977\n",
      "10000/10000 [==============================] - 0s 38us/sample - loss: 0.0161 - accuracy: 0.8961\n",
      "141th iteration\n",
      "10000/10000 [==============================] - 0s 35us/sample - loss: 0.0180 - accuracy: 0.8781\n",
      "10000/10000 [==============================] - 0s 35us/sample - loss: 0.0178 - accuracy: 0.8795\n",
      "10000/10000 [==============================] - 0s 34us/sample - loss: 0.0175 - accuracy: 0.8820\n",
      "10000/10000 [==============================] - 0s 34us/sample - loss: 0.0174 - accuracy: 0.8830\n",
      "10000/10000 [==============================] - 1s 59us/sample - loss: 0.0172 - accuracy: 0.8834\n",
      "10000/10000 [==============================] - 0s 34us/sample - loss: 0.0171 - accuracy: 0.8854\n",
      "10000/10000 [==============================] - 0s 35us/sample - loss: 0.0170 - accuracy: 0.8865\n",
      "10000/10000 [==============================] - 0s 36us/sample - loss: 0.0169 - accuracy: 0.8875\n",
      "10000/10000 [==============================] - 0s 35us/sample - loss: 0.0169 - accuracy: 0.8885\n",
      "10000/10000 [==============================] - 0s 36us/sample - loss: 0.0169 - accuracy: 0.8902\n",
      "10000/10000 [==============================] - 0s 34us/sample - loss: 0.0170 - accuracy: 0.8909\n",
      "10000/10000 [==============================] - 0s 33us/sample - loss: 0.0171 - accuracy: 0.8912\n",
      "10000/10000 [==============================] - 0s 34us/sample - loss: 0.0173 - accuracy: 0.8917\n",
      "10000/10000 [==============================] - 0s 36us/sample - loss: 0.0175 - accuracy: 0.8907\n",
      "10000/10000 [==============================] - 0s 34us/sample - loss: 0.0178 - accuracy: 0.8915\n",
      "10000/10000 [==============================] - 0s 36us/sample - loss: 0.0181 - accuracy: 0.8902\n",
      "10000/10000 [==============================] - 0s 34us/sample - loss: 0.0185 - accuracy: 0.8886\n",
      "10000/10000 [==============================] - 0s 36us/sample - loss: 0.0189 - accuracy: 0.8867\n",
      "10000/10000 [==============================] - 0s 36us/sample - loss: 0.0194 - accuracy: 0.8846\n",
      "10000/10000 [==============================] - 0s 36us/sample - loss: 0.0200 - accuracy: 0.8832\n",
      "10000/10000 [==============================] - 0s 36us/sample - loss: 0.0206 - accuracy: 0.8813\n",
      "142th iteration\n",
      "10000/10000 [==============================] - 0s 34us/sample - loss: 0.0180 - accuracy: 0.8781\n",
      "10000/10000 [==============================] - 0s 34us/sample - loss: 0.0177 - accuracy: 0.8797\n",
      "10000/10000 [==============================] - 0s 35us/sample - loss: 0.0173 - accuracy: 0.8827\n",
      "10000/10000 [==============================] - 0s 36us/sample - loss: 0.0171 - accuracy: 0.8845\n",
      "10000/10000 [==============================] - 0s 35us/sample - loss: 0.0168 - accuracy: 0.8862\n",
      "10000/10000 [==============================] - 0s 34us/sample - loss: 0.0165 - accuracy: 0.8882\n",
      "10000/10000 [==============================] - 0s 34us/sample - loss: 0.0163 - accuracy: 0.8899\n",
      "10000/10000 [==============================] - 0s 34us/sample - loss: 0.0161 - accuracy: 0.8911\n",
      "10000/10000 [==============================] - 0s 34us/sample - loss: 0.0160 - accuracy: 0.8929\n",
      "10000/10000 [==============================] - 0s 35us/sample - loss: 0.0158 - accuracy: 0.8950\n",
      "10000/10000 [==============================] - 0s 34us/sample - loss: 0.0157 - accuracy: 0.8957\n",
      "10000/10000 [==============================] - 0s 36us/sample - loss: 0.0156 - accuracy: 0.8974\n",
      "10000/10000 [==============================] - 0s 35us/sample - loss: 0.0155 - accuracy: 0.8973\n",
      "10000/10000 [==============================] - 0s 34us/sample - loss: 0.0155 - accuracy: 0.8978\n",
      "10000/10000 [==============================] - 0s 36us/sample - loss: 0.0155 - accuracy: 0.8968\n",
      "10000/10000 [==============================] - 0s 34us/sample - loss: 0.0155 - accuracy: 0.8972\n",
      "10000/10000 [==============================] - 0s 37us/sample - loss: 0.0156 - accuracy: 0.8971\n",
      "10000/10000 [==============================] - 0s 36us/sample - loss: 0.0157 - accuracy: 0.8961\n",
      "10000/10000 [==============================] - 0s 35us/sample - loss: 0.0158 - accuracy: 0.8960\n",
      "10000/10000 [==============================] - 0s 35us/sample - loss: 0.0159 - accuracy: 0.8944\n",
      "10000/10000 [==============================] - 0s 36us/sample - loss: 0.0161 - accuracy: 0.8926\n",
      "143th iteration\n",
      "10000/10000 [==============================] - 0s 35us/sample - loss: 0.0161 - accuracy: 0.8961\n",
      "10000/10000 [==============================] - 0s 36us/sample - loss: 0.0163 - accuracy: 0.8955\n",
      "10000/10000 [==============================] - 0s 36us/sample - loss: 0.0165 - accuracy: 0.8960\n",
      "10000/10000 [==============================] - 0s 37us/sample - loss: 0.0168 - accuracy: 0.8942\n",
      "10000/10000 [==============================] - 0s 36us/sample - loss: 0.0172 - accuracy: 0.8938\n",
      "10000/10000 [==============================] - 0s 35us/sample - loss: 0.0176 - accuracy: 0.8925\n",
      "10000/10000 [==============================] - 0s 36us/sample - loss: 0.0182 - accuracy: 0.8907\n",
      "10000/10000 [==============================] - 0s 36us/sample - loss: 0.0188 - accuracy: 0.8895\n",
      "10000/10000 [==============================] - 0s 34us/sample - loss: 0.0195 - accuracy: 0.8881\n",
      "10000/10000 [==============================] - 0s 34us/sample - loss: 0.0203 - accuracy: 0.8860\n",
      "10000/10000 [==============================] - 0s 36us/sample - loss: 0.0213 - accuracy: 0.8824\n",
      "10000/10000 [==============================] - 0s 36us/sample - loss: 0.0223 - accuracy: 0.8789\n",
      "10000/10000 [==============================] - 0s 37us/sample - loss: 0.0234 - accuracy: 0.8735\n",
      "10000/10000 [==============================] - 0s 34us/sample - loss: 0.0246 - accuracy: 0.8696\n",
      "10000/10000 [==============================] - 0s 36us/sample - loss: 0.0260 - accuracy: 0.8638\n",
      "10000/10000 [==============================] - 0s 36us/sample - loss: 0.0274 - accuracy: 0.8559\n",
      "10000/10000 [==============================] - 0s 36us/sample - loss: 0.0289 - accuracy: 0.8457\n",
      "10000/10000 [==============================] - 0s 35us/sample - loss: 0.0304 - accuracy: 0.8368\n",
      "10000/10000 [==============================] - 0s 34us/sample - loss: 0.0321 - accuracy: 0.8273\n",
      "10000/10000 [==============================] - 0s 34us/sample - loss: 0.0338 - accuracy: 0.8148\n",
      "10000/10000 [==============================] - 0s 35us/sample - loss: 0.0355 - accuracy: 0.8016\n",
      "144th iteration\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000/10000 [==============================] - 0s 36us/sample - loss: 0.0180 - accuracy: 0.8781\n",
      "10000/10000 [==============================] - 0s 34us/sample - loss: 0.0176 - accuracy: 0.8803\n",
      "10000/10000 [==============================] - 0s 34us/sample - loss: 0.0173 - accuracy: 0.8837\n",
      "10000/10000 [==============================] - 0s 36us/sample - loss: 0.0169 - accuracy: 0.8864\n",
      "10000/10000 [==============================] - 0s 34us/sample - loss: 0.0166 - accuracy: 0.8879\n",
      "10000/10000 [==============================] - 0s 36us/sample - loss: 0.0164 - accuracy: 0.8892\n",
      "10000/10000 [==============================] - 0s 34us/sample - loss: 0.0161 - accuracy: 0.8907\n",
      "10000/10000 [==============================] - 0s 36us/sample - loss: 0.0159 - accuracy: 0.8923\n",
      "10000/10000 [==============================] - 0s 37us/sample - loss: 0.0157 - accuracy: 0.8932\n",
      "10000/10000 [==============================] - 0s 34us/sample - loss: 0.0156 - accuracy: 0.8963\n",
      "10000/10000 [==============================] - 0s 35us/sample - loss: 0.0154 - accuracy: 0.8974\n",
      "10000/10000 [==============================] - 0s 35us/sample - loss: 0.0154 - accuracy: 0.9000\n",
      "10000/10000 [==============================] - 0s 34us/sample - loss: 0.0153 - accuracy: 0.9014\n",
      "10000/10000 [==============================] - 0s 38us/sample - loss: 0.0154 - accuracy: 0.9015\n",
      "10000/10000 [==============================] - 0s 36us/sample - loss: 0.0154 - accuracy: 0.9019\n",
      "10000/10000 [==============================] - 0s 36us/sample - loss: 0.0155 - accuracy: 0.9011\n",
      "10000/10000 [==============================] - 0s 34us/sample - loss: 0.0157 - accuracy: 0.9012\n",
      "10000/10000 [==============================] - 0s 34us/sample - loss: 0.0158 - accuracy: 0.9011\n",
      "10000/10000 [==============================] - 0s 35us/sample - loss: 0.0161 - accuracy: 0.8997\n",
      "10000/10000 [==============================] - 0s 34us/sample - loss: 0.0163 - accuracy: 0.8979\n",
      "10000/10000 [==============================] - 0s 36us/sample - loss: 0.0167 - accuracy: 0.8959\n",
      "145th iteration\n",
      "10000/10000 [==============================] - 0s 36us/sample - loss: 0.0155 - accuracy: 0.8978\n",
      "10000/10000 [==============================] - 0s 34us/sample - loss: 0.0153 - accuracy: 0.9011\n",
      "10000/10000 [==============================] - 0s 35us/sample - loss: 0.0151 - accuracy: 0.9030\n",
      "10000/10000 [==============================] - 0s 36us/sample - loss: 0.0149 - accuracy: 0.9033\n",
      "10000/10000 [==============================] - 0s 35us/sample - loss: 0.0148 - accuracy: 0.9045\n",
      "10000/10000 [==============================] - 0s 36us/sample - loss: 0.0147 - accuracy: 0.9049\n",
      "10000/10000 [==============================] - 0s 36us/sample - loss: 0.0146 - accuracy: 0.9051\n",
      "10000/10000 [==============================] - 0s 36us/sample - loss: 0.0145 - accuracy: 0.9049\n",
      "10000/10000 [==============================] - 0s 36us/sample - loss: 0.0145 - accuracy: 0.9060\n",
      "10000/10000 [==============================] - 0s 34us/sample - loss: 0.0145 - accuracy: 0.9058\n",
      "10000/10000 [==============================] - 0s 34us/sample - loss: 0.0146 - accuracy: 0.9065\n",
      "10000/10000 [==============================] - 0s 36us/sample - loss: 0.0147 - accuracy: 0.9057\n",
      "10000/10000 [==============================] - 0s 34us/sample - loss: 0.0148 - accuracy: 0.9040\n",
      "10000/10000 [==============================] - 0s 35us/sample - loss: 0.0149 - accuracy: 0.9032\n",
      "10000/10000 [==============================] - 0s 34us/sample - loss: 0.0151 - accuracy: 0.9018\n",
      "10000/10000 [==============================] - 0s 36us/sample - loss: 0.0153 - accuracy: 0.8997\n",
      "10000/10000 [==============================] - 0s 36us/sample - loss: 0.0155 - accuracy: 0.8988\n",
      "10000/10000 [==============================] - 0s 36us/sample - loss: 0.0158 - accuracy: 0.8959\n",
      "10000/10000 [==============================] - 0s 35us/sample - loss: 0.0161 - accuracy: 0.8932\n",
      "10000/10000 [==============================] - 0s 34us/sample - loss: 0.0164 - accuracy: 0.8903\n",
      "10000/10000 [==============================] - 0s 36us/sample - loss: 0.0168 - accuracy: 0.8870\n",
      "146th iteration\n",
      "10000/10000 [==============================] - 0s 36us/sample - loss: 0.0180 - accuracy: 0.8781\n",
      "10000/10000 [==============================] - 0s 36us/sample - loss: 0.0178 - accuracy: 0.8791\n",
      "10000/10000 [==============================] - 0s 37us/sample - loss: 0.0176 - accuracy: 0.8814\n",
      "10000/10000 [==============================] - 0s 34us/sample - loss: 0.0175 - accuracy: 0.8827\n",
      "10000/10000 [==============================] - 0s 35us/sample - loss: 0.0174 - accuracy: 0.8835\n",
      "10000/10000 [==============================] - 1s 59us/sample - loss: 0.0173 - accuracy: 0.8837\n",
      "10000/10000 [==============================] - 0s 34us/sample - loss: 0.0173 - accuracy: 0.8845\n",
      "10000/10000 [==============================] - 0s 34us/sample - loss: 0.0173 - accuracy: 0.8853\n",
      "10000/10000 [==============================] - 0s 34us/sample - loss: 0.0173 - accuracy: 0.8846\n",
      "10000/10000 [==============================] - 0s 36us/sample - loss: 0.0174 - accuracy: 0.8853\n",
      "10000/10000 [==============================] - 0s 36us/sample - loss: 0.0176 - accuracy: 0.8863\n",
      "10000/10000 [==============================] - 0s 35us/sample - loss: 0.0178 - accuracy: 0.8873\n",
      "10000/10000 [==============================] - 0s 34us/sample - loss: 0.0180 - accuracy: 0.8869\n",
      "10000/10000 [==============================] - 0s 36us/sample - loss: 0.0184 - accuracy: 0.8855\n",
      "10000/10000 [==============================] - 0s 36us/sample - loss: 0.0187 - accuracy: 0.8848\n",
      "10000/10000 [==============================] - 0s 36us/sample - loss: 0.0192 - accuracy: 0.8853\n",
      "10000/10000 [==============================] - 0s 37us/sample - loss: 0.0197 - accuracy: 0.8845\n",
      "10000/10000 [==============================] - 0s 35us/sample - loss: 0.0203 - accuracy: 0.8818\n",
      "10000/10000 [==============================] - 0s 36us/sample - loss: 0.0209 - accuracy: 0.8787\n",
      "10000/10000 [==============================] - 0s 36us/sample - loss: 0.0216 - accuracy: 0.8753\n",
      "10000/10000 [==============================] - 0s 34us/sample - loss: 0.0224 - accuracy: 0.8711\n",
      "147th iteration\n",
      "10000/10000 [==============================] - 0s 35us/sample - loss: 0.0155 - accuracy: 0.8978\n",
      "10000/10000 [==============================] - 0s 36us/sample - loss: 0.0153 - accuracy: 0.9003\n",
      "10000/10000 [==============================] - 0s 34us/sample - loss: 0.0150 - accuracy: 0.9029\n",
      "10000/10000 [==============================] - 0s 34us/sample - loss: 0.0148 - accuracy: 0.9032\n",
      "10000/10000 [==============================] - 0s 34us/sample - loss: 0.0147 - accuracy: 0.9045\n",
      "10000/10000 [==============================] - 0s 35us/sample - loss: 0.0146 - accuracy: 0.9058\n",
      "10000/10000 [==============================] - 0s 36us/sample - loss: 0.0145 - accuracy: 0.9068\n",
      "10000/10000 [==============================] - 0s 36us/sample - loss: 0.0144 - accuracy: 0.9073\n",
      "10000/10000 [==============================] - 1s 57us/sample - loss: 0.0144 - accuracy: 0.9068\n",
      "10000/10000 [==============================] - 0s 34us/sample - loss: 0.0144 - accuracy: 0.9066\n",
      "10000/10000 [==============================] - 0s 35us/sample - loss: 0.0144 - accuracy: 0.9067\n",
      "10000/10000 [==============================] - 0s 34us/sample - loss: 0.0145 - accuracy: 0.9058\n",
      "10000/10000 [==============================] - 0s 36us/sample - loss: 0.0146 - accuracy: 0.9057\n",
      "10000/10000 [==============================] - 0s 35us/sample - loss: 0.0147 - accuracy: 0.9041\n",
      "10000/10000 [==============================] - 0s 34us/sample - loss: 0.0148 - accuracy: 0.9027\n",
      "10000/10000 [==============================] - 0s 34us/sample - loss: 0.0150 - accuracy: 0.9006\n",
      "10000/10000 [==============================] - 0s 34us/sample - loss: 0.0152 - accuracy: 0.8995\n",
      "10000/10000 [==============================] - 0s 35us/sample - loss: 0.0154 - accuracy: 0.8987\n",
      "10000/10000 [==============================] - 0s 36us/sample - loss: 0.0156 - accuracy: 0.8960\n",
      "10000/10000 [==============================] - 0s 36us/sample - loss: 0.0158 - accuracy: 0.8944\n",
      "10000/10000 [==============================] - 0s 35us/sample - loss: 0.0161 - accuracy: 0.8926\n",
      "148th iteration\n",
      "10000/10000 [==============================] - 0s 35us/sample - loss: 0.0180 - accuracy: 0.8781\n",
      "10000/10000 [==============================] - 0s 34us/sample - loss: 0.0176 - accuracy: 0.8808\n",
      "10000/10000 [==============================] - 0s 34us/sample - loss: 0.0172 - accuracy: 0.8852\n",
      "10000/10000 [==============================] - 0s 36us/sample - loss: 0.0168 - accuracy: 0.8867\n",
      "10000/10000 [==============================] - 0s 35us/sample - loss: 0.0164 - accuracy: 0.8884\n",
      "10000/10000 [==============================] - 0s 36us/sample - loss: 0.0161 - accuracy: 0.8903\n",
      "10000/10000 [==============================] - 0s 38us/sample - loss: 0.0158 - accuracy: 0.8927\n",
      "10000/10000 [==============================] - 0s 35us/sample - loss: 0.0155 - accuracy: 0.8949\n",
      "10000/10000 [==============================] - 0s 36us/sample - loss: 0.0153 - accuracy: 0.8981\n",
      "10000/10000 [==============================] - 0s 34us/sample - loss: 0.0151 - accuracy: 0.9006\n",
      "10000/10000 [==============================] - 0s 34us/sample - loss: 0.0149 - accuracy: 0.9037\n",
      "10000/10000 [==============================] - 0s 35us/sample - loss: 0.0148 - accuracy: 0.9036\n",
      "10000/10000 [==============================] - 0s 36us/sample - loss: 0.0147 - accuracy: 0.9042\n",
      "10000/10000 [==============================] - 0s 34us/sample - loss: 0.0146 - accuracy: 0.9046\n",
      "10000/10000 [==============================] - 0s 35us/sample - loss: 0.0146 - accuracy: 0.9047\n",
      "10000/10000 [==============================] - 0s 34us/sample - loss: 0.0146 - accuracy: 0.9056\n",
      "10000/10000 [==============================] - 0s 35us/sample - loss: 0.0146 - accuracy: 0.9063\n",
      "10000/10000 [==============================] - 0s 34us/sample - loss: 0.0147 - accuracy: 0.9061\n",
      "10000/10000 [==============================] - 0s 35us/sample - loss: 0.0148 - accuracy: 0.9058\n",
      "10000/10000 [==============================] - 0s 34us/sample - loss: 0.0149 - accuracy: 0.9038\n",
      "10000/10000 [==============================] - 0s 36us/sample - loss: 0.0151 - accuracy: 0.9033\n",
      "149th iteration\n",
      "10000/10000 [==============================] - 0s 34us/sample - loss: 0.0155 - accuracy: 0.8978\n",
      "10000/10000 [==============================] - 0s 34us/sample - loss: 0.0155 - accuracy: 0.9001\n",
      "10000/10000 [==============================] - 0s 34us/sample - loss: 0.0155 - accuracy: 0.9017\n",
      "10000/10000 [==============================] - 0s 36us/sample - loss: 0.0155 - accuracy: 0.9017\n",
      "10000/10000 [==============================] - 0s 36us/sample - loss: 0.0155 - accuracy: 0.9015\n",
      "10000/10000 [==============================] - 0s 35us/sample - loss: 0.0156 - accuracy: 0.9013\n",
      "10000/10000 [==============================] - 0s 36us/sample - loss: 0.0157 - accuracy: 0.9007\n",
      "10000/10000 [==============================] - 0s 34us/sample - loss: 0.0159 - accuracy: 0.9006\n",
      "10000/10000 [==============================] - 0s 34us/sample - loss: 0.0161 - accuracy: 0.8996\n",
      "10000/10000 [==============================] - 0s 36us/sample - loss: 0.0164 - accuracy: 0.8998\n",
      "10000/10000 [==============================] - 0s 36us/sample - loss: 0.0167 - accuracy: 0.8983\n",
      "10000/10000 [==============================] - 0s 34us/sample - loss: 0.0170 - accuracy: 0.8969\n",
      "10000/10000 [==============================] - 0s 34us/sample - loss: 0.0174 - accuracy: 0.8958\n",
      "10000/10000 [==============================] - 0s 34us/sample - loss: 0.0179 - accuracy: 0.8940\n",
      "10000/10000 [==============================] - 0s 35us/sample - loss: 0.0184 - accuracy: 0.8922\n",
      "10000/10000 [==============================] - 0s 34us/sample - loss: 0.0189 - accuracy: 0.8887\n",
      "10000/10000 [==============================] - 0s 36us/sample - loss: 0.0195 - accuracy: 0.8855\n",
      "10000/10000 [==============================] - 0s 35us/sample - loss: 0.0202 - accuracy: 0.8833\n",
      "10000/10000 [==============================] - 0s 34us/sample - loss: 0.0209 - accuracy: 0.8798\n",
      "10000/10000 [==============================] - 0s 35us/sample - loss: 0.0216 - accuracy: 0.8750\n",
      "10000/10000 [==============================] - 0s 35us/sample - loss: 0.0224 - accuracy: 0.8711\n",
      "150th iteration\n",
      "10000/10000 [==============================] - 0s 35us/sample - loss: 0.0168 - accuracy: 0.8853\n",
      "10000/10000 [==============================] - 0s 34us/sample - loss: 0.0166 - accuracy: 0.8868\n",
      "10000/10000 [==============================] - 0s 34us/sample - loss: 0.0165 - accuracy: 0.8884\n",
      "10000/10000 [==============================] - 0s 36us/sample - loss: 0.0164 - accuracy: 0.8896\n",
      "10000/10000 [==============================] - 0s 34us/sample - loss: 0.0164 - accuracy: 0.8909\n",
      "10000/10000 [==============================] - 0s 34us/sample - loss: 0.0164 - accuracy: 0.8922\n",
      "10000/10000 [==============================] - 0s 34us/sample - loss: 0.0164 - accuracy: 0.8924\n",
      "10000/10000 [==============================] - 0s 35us/sample - loss: 0.0165 - accuracy: 0.8922\n",
      "10000/10000 [==============================] - 0s 35us/sample - loss: 0.0167 - accuracy: 0.8918\n",
      "10000/10000 [==============================] - 0s 34us/sample - loss: 0.0168 - accuracy: 0.8915\n",
      "10000/10000 [==============================] - 0s 37us/sample - loss: 0.0170 - accuracy: 0.8915\n",
      "10000/10000 [==============================] - 0s 36us/sample - loss: 0.0173 - accuracy: 0.8911\n",
      "10000/10000 [==============================] - 0s 36us/sample - loss: 0.0176 - accuracy: 0.8901\n",
      "10000/10000 [==============================] - 0s 34us/sample - loss: 0.0180 - accuracy: 0.8895\n",
      "10000/10000 [==============================] - 0s 34us/sample - loss: 0.0185 - accuracy: 0.8887\n",
      "10000/10000 [==============================] - 0s 34us/sample - loss: 0.0190 - accuracy: 0.8868\n",
      "10000/10000 [==============================] - 0s 34us/sample - loss: 0.0195 - accuracy: 0.8844\n",
      "10000/10000 [==============================] - 0s 36us/sample - loss: 0.0202 - accuracy: 0.8817\n",
      "10000/10000 [==============================] - 0s 34us/sample - loss: 0.0208 - accuracy: 0.8785\n",
      "10000/10000 [==============================] - 0s 36us/sample - loss: 0.0216 - accuracy: 0.8753\n",
      "10000/10000 [==============================] - 0s 35us/sample - loss: 0.0224 - accuracy: 0.8711\n",
      "151th iteration\n",
      "10000/10000 [==============================] - 0s 34us/sample - loss: 0.0180 - accuracy: 0.8781\n",
      "10000/10000 [==============================] - 0s 35us/sample - loss: 0.0176 - accuracy: 0.8812\n",
      "10000/10000 [==============================] - 0s 36us/sample - loss: 0.0173 - accuracy: 0.8834\n",
      "10000/10000 [==============================] - 0s 37us/sample - loss: 0.0170 - accuracy: 0.8855\n",
      "10000/10000 [==============================] - 0s 34us/sample - loss: 0.0167 - accuracy: 0.8866\n",
      "10000/10000 [==============================] - 0s 36us/sample - loss: 0.0165 - accuracy: 0.8881\n",
      "10000/10000 [==============================] - 0s 35us/sample - loss: 0.0162 - accuracy: 0.8901\n",
      "10000/10000 [==============================] - 0s 34us/sample - loss: 0.0160 - accuracy: 0.8911\n",
      "10000/10000 [==============================] - 0s 36us/sample - loss: 0.0159 - accuracy: 0.8935\n",
      "10000/10000 [==============================] - 0s 36us/sample - loss: 0.0158 - accuracy: 0.8943\n",
      "10000/10000 [==============================] - 0s 35us/sample - loss: 0.0157 - accuracy: 0.8955\n",
      "10000/10000 [==============================] - 0s 35us/sample - loss: 0.0156 - accuracy: 0.8970\n",
      "10000/10000 [==============================] - 0s 36us/sample - loss: 0.0156 - accuracy: 0.8973\n",
      "10000/10000 [==============================] - 0s 35us/sample - loss: 0.0156 - accuracy: 0.8972\n",
      "10000/10000 [==============================] - 0s 34us/sample - loss: 0.0157 - accuracy: 0.8958\n",
      "10000/10000 [==============================] - 0s 36us/sample - loss: 0.0158 - accuracy: 0.8954\n",
      "10000/10000 [==============================] - 0s 34us/sample - loss: 0.0159 - accuracy: 0.8944\n",
      "10000/10000 [==============================] - 0s 35us/sample - loss: 0.0161 - accuracy: 0.8940\n",
      "10000/10000 [==============================] - 0s 34us/sample - loss: 0.0163 - accuracy: 0.8912\n",
      "10000/10000 [==============================] - 0s 36us/sample - loss: 0.0165 - accuracy: 0.8892\n",
      "10000/10000 [==============================] - 0s 37us/sample - loss: 0.0168 - accuracy: 0.8870\n",
      "152th iteration\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000/10000 [==============================] - 0s 36us/sample - loss: 0.0151 - accuracy: 0.8978\n",
      "10000/10000 [==============================] - 0s 36us/sample - loss: 0.0149 - accuracy: 0.9002\n",
      "10000/10000 [==============================] - 0s 34us/sample - loss: 0.0148 - accuracy: 0.9015\n",
      "10000/10000 [==============================] - 0s 34us/sample - loss: 0.0147 - accuracy: 0.9039\n",
      "10000/10000 [==============================] - 0s 37us/sample - loss: 0.0145 - accuracy: 0.9048\n",
      "10000/10000 [==============================] - 0s 34us/sample - loss: 0.0145 - accuracy: 0.9057\n",
      "10000/10000 [==============================] - 0s 36us/sample - loss: 0.0144 - accuracy: 0.9058\n",
      "10000/10000 [==============================] - 0s 35us/sample - loss: 0.0144 - accuracy: 0.9065\n",
      "10000/10000 [==============================] - 0s 36us/sample - loss: 0.0144 - accuracy: 0.9070\n",
      "10000/10000 [==============================] - 1s 57us/sample - loss: 0.0144 - accuracy: 0.9076\n",
      "10000/10000 [==============================] - 0s 34us/sample - loss: 0.0144 - accuracy: 0.9081\n",
      "10000/10000 [==============================] - 0s 35us/sample - loss: 0.0145 - accuracy: 0.9083\n",
      "10000/10000 [==============================] - 0s 36us/sample - loss: 0.0146 - accuracy: 0.9068\n",
      "10000/10000 [==============================] - 0s 36us/sample - loss: 0.0147 - accuracy: 0.9061\n",
      "10000/10000 [==============================] - 0s 34us/sample - loss: 0.0149 - accuracy: 0.9056\n",
      "10000/10000 [==============================] - 0s 34us/sample - loss: 0.0150 - accuracy: 0.9040\n",
      "10000/10000 [==============================] - 0s 34us/sample - loss: 0.0152 - accuracy: 0.9024\n",
      "10000/10000 [==============================] - 0s 34us/sample - loss: 0.0155 - accuracy: 0.9002\n",
      "10000/10000 [==============================] - 0s 36us/sample - loss: 0.0157 - accuracy: 0.8979\n",
      "10000/10000 [==============================] - 0s 36us/sample - loss: 0.0160 - accuracy: 0.8968\n",
      "10000/10000 [==============================] - 0s 36us/sample - loss: 0.0163 - accuracy: 0.8942\n",
      "153th iteration\n",
      "10000/10000 [==============================] - 0s 34us/sample - loss: 0.0180 - accuracy: 0.8781\n",
      "10000/10000 [==============================] - 0s 34us/sample - loss: 0.0177 - accuracy: 0.8803\n",
      "10000/10000 [==============================] - 0s 36us/sample - loss: 0.0174 - accuracy: 0.8817\n",
      "10000/10000 [==============================] - 0s 34us/sample - loss: 0.0172 - accuracy: 0.8834\n",
      "10000/10000 [==============================] - 0s 35us/sample - loss: 0.0169 - accuracy: 0.8848\n",
      "10000/10000 [==============================] - 0s 35us/sample - loss: 0.0167 - accuracy: 0.8861\n",
      "10000/10000 [==============================] - 0s 34us/sample - loss: 0.0165 - accuracy: 0.8870\n",
      "10000/10000 [==============================] - 0s 36us/sample - loss: 0.0164 - accuracy: 0.8887\n",
      "10000/10000 [==============================] - 0s 34us/sample - loss: 0.0162 - accuracy: 0.8893\n",
      "10000/10000 [==============================] - 0s 35us/sample - loss: 0.0161 - accuracy: 0.8911\n",
      "10000/10000 [==============================] - 0s 34us/sample - loss: 0.0160 - accuracy: 0.8927\n",
      "10000/10000 [==============================] - 0s 34us/sample - loss: 0.0160 - accuracy: 0.8930\n",
      "10000/10000 [==============================] - 1s 59us/sample - loss: 0.0159 - accuracy: 0.8934\n",
      "10000/10000 [==============================] - 0s 36us/sample - loss: 0.0159 - accuracy: 0.8942\n",
      "10000/10000 [==============================] - 0s 36us/sample - loss: 0.0160 - accuracy: 0.8942\n",
      "10000/10000 [==============================] - 0s 36us/sample - loss: 0.0160 - accuracy: 0.8931\n",
      "10000/10000 [==============================] - 0s 38us/sample - loss: 0.0161 - accuracy: 0.8920\n",
      "10000/10000 [==============================] - 0s 37us/sample - loss: 0.0162 - accuracy: 0.8912\n",
      "10000/10000 [==============================] - 0s 36us/sample - loss: 0.0164 - accuracy: 0.8892\n",
      "10000/10000 [==============================] - 0s 34us/sample - loss: 0.0165 - accuracy: 0.8875\n",
      "10000/10000 [==============================] - 0s 34us/sample - loss: 0.0168 - accuracy: 0.8853\n",
      "154th iteration\n",
      "10000/10000 [==============================] - 0s 37us/sample - loss: 0.0151 - accuracy: 0.8978\n",
      "10000/10000 [==============================] - 0s 35us/sample - loss: 0.0150 - accuracy: 0.8995\n",
      "10000/10000 [==============================] - 0s 35us/sample - loss: 0.0149 - accuracy: 0.9008\n",
      "10000/10000 [==============================] - 0s 35us/sample - loss: 0.0148 - accuracy: 0.9010\n",
      "10000/10000 [==============================] - 0s 36us/sample - loss: 0.0148 - accuracy: 0.9029\n",
      "10000/10000 [==============================] - 0s 35us/sample - loss: 0.0148 - accuracy: 0.9032\n",
      "10000/10000 [==============================] - 0s 35us/sample - loss: 0.0148 - accuracy: 0.9033\n",
      "10000/10000 [==============================] - 0s 36us/sample - loss: 0.0148 - accuracy: 0.9035\n",
      "10000/10000 [==============================] - 0s 34us/sample - loss: 0.0149 - accuracy: 0.9025\n",
      "10000/10000 [==============================] - 0s 34us/sample - loss: 0.0150 - accuracy: 0.9011\n",
      "10000/10000 [==============================] - 0s 35us/sample - loss: 0.0151 - accuracy: 0.9013\n",
      "10000/10000 [==============================] - 0s 36us/sample - loss: 0.0152 - accuracy: 0.9013\n",
      "10000/10000 [==============================] - 0s 35us/sample - loss: 0.0154 - accuracy: 0.8992\n",
      "10000/10000 [==============================] - 0s 34us/sample - loss: 0.0156 - accuracy: 0.8984\n",
      "10000/10000 [==============================] - 0s 34us/sample - loss: 0.0159 - accuracy: 0.8975\n",
      "10000/10000 [==============================] - 0s 36us/sample - loss: 0.0161 - accuracy: 0.8953\n",
      "10000/10000 [==============================] - 0s 34us/sample - loss: 0.0164 - accuracy: 0.8935\n",
      "10000/10000 [==============================] - 0s 37us/sample - loss: 0.0167 - accuracy: 0.8921\n",
      "10000/10000 [==============================] - 0s 35us/sample - loss: 0.0171 - accuracy: 0.8894\n",
      "10000/10000 [==============================] - 0s 36us/sample - loss: 0.0175 - accuracy: 0.8863\n",
      "10000/10000 [==============================] - 0s 36us/sample - loss: 0.0179 - accuracy: 0.8823\n",
      "155th iteration\n",
      "10000/10000 [==============================] - 0s 36us/sample - loss: 0.0151 - accuracy: 0.8978\n",
      "10000/10000 [==============================] - 0s 36us/sample - loss: 0.0149 - accuracy: 0.8998\n",
      "10000/10000 [==============================] - 0s 36us/sample - loss: 0.0147 - accuracy: 0.9010\n",
      "10000/10000 [==============================] - 0s 38us/sample - loss: 0.0145 - accuracy: 0.9023\n",
      "10000/10000 [==============================] - 0s 36us/sample - loss: 0.0144 - accuracy: 0.9038\n",
      "10000/10000 [==============================] - 0s 36us/sample - loss: 0.0143 - accuracy: 0.9053\n",
      "10000/10000 [==============================] - 0s 35us/sample - loss: 0.0142 - accuracy: 0.9064\n",
      "10000/10000 [==============================] - 0s 34us/sample - loss: 0.0142 - accuracy: 0.9070\n",
      "10000/10000 [==============================] - 0s 35us/sample - loss: 0.0141 - accuracy: 0.9059\n",
      "10000/10000 [==============================] - 0s 34us/sample - loss: 0.0142 - accuracy: 0.9060\n",
      "10000/10000 [==============================] - 0s 35us/sample - loss: 0.0142 - accuracy: 0.9062\n",
      "10000/10000 [==============================] - 0s 35us/sample - loss: 0.0142 - accuracy: 0.9055\n",
      "10000/10000 [==============================] - 0s 34us/sample - loss: 0.0143 - accuracy: 0.9056\n",
      "10000/10000 [==============================] - 0s 37us/sample - loss: 0.0144 - accuracy: 0.9038\n",
      "10000/10000 [==============================] - 0s 35us/sample - loss: 0.0146 - accuracy: 0.9038\n",
      "10000/10000 [==============================] - 0s 36us/sample - loss: 0.0147 - accuracy: 0.9028\n",
      "10000/10000 [==============================] - 0s 34us/sample - loss: 0.0149 - accuracy: 0.9016\n",
      "10000/10000 [==============================] - 0s 34us/sample - loss: 0.0151 - accuracy: 0.8997\n",
      "10000/10000 [==============================] - 0s 34us/sample - loss: 0.0153 - accuracy: 0.8984\n",
      "10000/10000 [==============================] - 0s 34us/sample - loss: 0.0156 - accuracy: 0.8962\n",
      "10000/10000 [==============================] - 0s 35us/sample - loss: 0.0159 - accuracy: 0.8943\n",
      "156th iteration\n",
      "10000/10000 [==============================] - 0s 36us/sample - loss: 0.0168 - accuracy: 0.8853\n",
      "10000/10000 [==============================] - 0s 34us/sample - loss: 0.0164 - accuracy: 0.8880\n",
      "10000/10000 [==============================] - 0s 34us/sample - loss: 0.0161 - accuracy: 0.8898\n",
      "10000/10000 [==============================] - 0s 34us/sample - loss: 0.0159 - accuracy: 0.8918\n",
      "10000/10000 [==============================] - 0s 35us/sample - loss: 0.0156 - accuracy: 0.8945\n",
      "10000/10000 [==============================] - 0s 36us/sample - loss: 0.0154 - accuracy: 0.8961\n",
      "10000/10000 [==============================] - 0s 36us/sample - loss: 0.0153 - accuracy: 0.8980\n",
      "10000/10000 [==============================] - 0s 36us/sample - loss: 0.0151 - accuracy: 0.9001\n",
      "10000/10000 [==============================] - 0s 36us/sample - loss: 0.0151 - accuracy: 0.9001\n",
      "10000/10000 [==============================] - 0s 35us/sample - loss: 0.0150 - accuracy: 0.9017\n",
      "10000/10000 [==============================] - 0s 36us/sample - loss: 0.0150 - accuracy: 0.9030\n",
      "10000/10000 [==============================] - 0s 35us/sample - loss: 0.0150 - accuracy: 0.9020\n",
      "10000/10000 [==============================] - 0s 36us/sample - loss: 0.0151 - accuracy: 0.9016\n",
      "10000/10000 [==============================] - 0s 35us/sample - loss: 0.0152 - accuracy: 0.9009\n",
      "10000/10000 [==============================] - 0s 34us/sample - loss: 0.0153 - accuracy: 0.8995\n",
      "10000/10000 [==============================] - 0s 34us/sample - loss: 0.0154 - accuracy: 0.8983\n",
      "10000/10000 [==============================] - 0s 34us/sample - loss: 0.0156 - accuracy: 0.8969\n",
      "10000/10000 [==============================] - 0s 34us/sample - loss: 0.0159 - accuracy: 0.8953\n",
      "10000/10000 [==============================] - 0s 36us/sample - loss: 0.0161 - accuracy: 0.8932\n",
      "10000/10000 [==============================] - 0s 34us/sample - loss: 0.0164 - accuracy: 0.8911\n",
      "10000/10000 [==============================] - 0s 34us/sample - loss: 0.0168 - accuracy: 0.8870\n",
      "157th iteration\n",
      "10000/10000 [==============================] - 0s 35us/sample - loss: 0.0155 - accuracy: 0.8978\n",
      "10000/10000 [==============================] - 0s 34us/sample - loss: 0.0152 - accuracy: 0.9005\n",
      "10000/10000 [==============================] - 0s 35us/sample - loss: 0.0150 - accuracy: 0.9027\n",
      "10000/10000 [==============================] - 0s 35us/sample - loss: 0.0148 - accuracy: 0.9040\n",
      "10000/10000 [==============================] - 0s 34us/sample - loss: 0.0146 - accuracy: 0.9059\n",
      "10000/10000 [==============================] - 0s 35us/sample - loss: 0.0144 - accuracy: 0.9069\n",
      "10000/10000 [==============================] - 0s 34us/sample - loss: 0.0143 - accuracy: 0.9081\n",
      "10000/10000 [==============================] - 0s 35us/sample - loss: 0.0142 - accuracy: 0.9075\n",
      "10000/10000 [==============================] - 0s 34us/sample - loss: 0.0141 - accuracy: 0.9070\n",
      "10000/10000 [==============================] - 0s 34us/sample - loss: 0.0141 - accuracy: 0.9070\n",
      "10000/10000 [==============================] - 0s 37us/sample - loss: 0.0141 - accuracy: 0.9075\n",
      "10000/10000 [==============================] - 0s 37us/sample - loss: 0.0141 - accuracy: 0.9080\n",
      "10000/10000 [==============================] - 0s 34us/sample - loss: 0.0142 - accuracy: 0.9086\n",
      "10000/10000 [==============================] - 0s 36us/sample - loss: 0.0143 - accuracy: 0.9075\n",
      "10000/10000 [==============================] - 0s 36us/sample - loss: 0.0144 - accuracy: 0.9058\n",
      "10000/10000 [==============================] - 0s 35us/sample - loss: 0.0145 - accuracy: 0.9054\n",
      "10000/10000 [==============================] - 0s 35us/sample - loss: 0.0147 - accuracy: 0.9039\n",
      "10000/10000 [==============================] - 0s 35us/sample - loss: 0.0149 - accuracy: 0.9028\n",
      "10000/10000 [==============================] - 0s 36us/sample - loss: 0.0151 - accuracy: 0.9008\n",
      "10000/10000 [==============================] - 0s 36us/sample - loss: 0.0153 - accuracy: 0.8999\n",
      "10000/10000 [==============================] - 0s 36us/sample - loss: 0.0156 - accuracy: 0.8985\n",
      "158th iteration\n",
      "10000/10000 [==============================] - 0s 37us/sample - loss: 0.0151 - accuracy: 0.8978\n",
      "10000/10000 [==============================] - 0s 35us/sample - loss: 0.0149 - accuracy: 0.8994\n",
      "10000/10000 [==============================] - 0s 36us/sample - loss: 0.0147 - accuracy: 0.9024\n",
      "10000/10000 [==============================] - 0s 36us/sample - loss: 0.0145 - accuracy: 0.9042\n",
      "10000/10000 [==============================] - 0s 34us/sample - loss: 0.0143 - accuracy: 0.9059\n",
      "10000/10000 [==============================] - 0s 36us/sample - loss: 0.0142 - accuracy: 0.9061\n",
      "10000/10000 [==============================] - 0s 34us/sample - loss: 0.0141 - accuracy: 0.9073\n",
      "10000/10000 [==============================] - 0s 34us/sample - loss: 0.0140 - accuracy: 0.9083\n",
      "10000/10000 [==============================] - 0s 35us/sample - loss: 0.0139 - accuracy: 0.9090\n",
      "10000/10000 [==============================] - 0s 34us/sample - loss: 0.0139 - accuracy: 0.9103\n",
      "10000/10000 [==============================] - 0s 36us/sample - loss: 0.0139 - accuracy: 0.9108\n",
      "10000/10000 [==============================] - 0s 34us/sample - loss: 0.0139 - accuracy: 0.9117\n",
      "10000/10000 [==============================] - 0s 35us/sample - loss: 0.0139 - accuracy: 0.9113\n",
      "10000/10000 [==============================] - 1s 57us/sample - loss: 0.0140 - accuracy: 0.9115\n",
      "10000/10000 [==============================] - 0s 37us/sample - loss: 0.0141 - accuracy: 0.9103\n",
      "10000/10000 [==============================] - 0s 36us/sample - loss: 0.0142 - accuracy: 0.9090\n",
      "10000/10000 [==============================] - 0s 34us/sample - loss: 0.0143 - accuracy: 0.9078\n",
      "10000/10000 [==============================] - 0s 35us/sample - loss: 0.0145 - accuracy: 0.9065\n",
      "10000/10000 [==============================] - 0s 34us/sample - loss: 0.0147 - accuracy: 0.9056\n",
      "10000/10000 [==============================] - 0s 36us/sample - loss: 0.0149 - accuracy: 0.9048\n",
      "10000/10000 [==============================] - 0s 34us/sample - loss: 0.0151 - accuracy: 0.9033\n",
      "159th iteration\n",
      "10000/10000 [==============================] - 0s 34us/sample - loss: 0.0151 - accuracy: 0.8978\n",
      "10000/10000 [==============================] - 0s 35us/sample - loss: 0.0150 - accuracy: 0.8999\n",
      "10000/10000 [==============================] - 0s 36us/sample - loss: 0.0149 - accuracy: 0.9012\n",
      "10000/10000 [==============================] - 0s 37us/sample - loss: 0.0148 - accuracy: 0.9017\n",
      "10000/10000 [==============================] - 0s 35us/sample - loss: 0.0147 - accuracy: 0.9027\n",
      "10000/10000 [==============================] - 0s 36us/sample - loss: 0.0147 - accuracy: 0.9034\n",
      "10000/10000 [==============================] - 0s 37us/sample - loss: 0.0147 - accuracy: 0.9038\n",
      "10000/10000 [==============================] - 0s 36us/sample - loss: 0.0148 - accuracy: 0.9034\n",
      "10000/10000 [==============================] - 0s 37us/sample - loss: 0.0149 - accuracy: 0.9030\n",
      "10000/10000 [==============================] - 0s 35us/sample - loss: 0.0150 - accuracy: 0.9029\n",
      "10000/10000 [==============================] - 0s 37us/sample - loss: 0.0151 - accuracy: 0.9026\n",
      "10000/10000 [==============================] - 0s 36us/sample - loss: 0.0152 - accuracy: 0.9021\n",
      "10000/10000 [==============================] - 0s 36us/sample - loss: 0.0154 - accuracy: 0.9007\n",
      "10000/10000 [==============================] - 0s 36us/sample - loss: 0.0157 - accuracy: 0.8990\n",
      "10000/10000 [==============================] - 0s 34us/sample - loss: 0.0159 - accuracy: 0.8985\n",
      "10000/10000 [==============================] - 0s 35us/sample - loss: 0.0162 - accuracy: 0.8972\n",
      "10000/10000 [==============================] - 1s 56us/sample - loss: 0.0165 - accuracy: 0.8965\n",
      "10000/10000 [==============================] - 0s 35us/sample - loss: 0.0169 - accuracy: 0.8952\n",
      "10000/10000 [==============================] - 0s 36us/sample - loss: 0.0172 - accuracy: 0.8920\n",
      "10000/10000 [==============================] - 0s 36us/sample - loss: 0.0176 - accuracy: 0.8897\n",
      "10000/10000 [==============================] - 0s 36us/sample - loss: 0.0181 - accuracy: 0.8868\n",
      "160th iteration\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000/10000 [==============================] - 0s 34us/sample - loss: 0.0151 - accuracy: 0.8978\n",
      "10000/10000 [==============================] - 0s 35us/sample - loss: 0.0149 - accuracy: 0.9002\n",
      "10000/10000 [==============================] - 0s 34us/sample - loss: 0.0147 - accuracy: 0.9025\n",
      "10000/10000 [==============================] - 0s 36us/sample - loss: 0.0146 - accuracy: 0.9039\n",
      "10000/10000 [==============================] - 0s 36us/sample - loss: 0.0145 - accuracy: 0.9051\n",
      "10000/10000 [==============================] - 0s 34us/sample - loss: 0.0144 - accuracy: 0.9071\n",
      "10000/10000 [==============================] - 0s 35us/sample - loss: 0.0143 - accuracy: 0.9074\n",
      "10000/10000 [==============================] - 0s 34us/sample - loss: 0.0143 - accuracy: 0.9086\n",
      "10000/10000 [==============================] - 0s 34us/sample - loss: 0.0143 - accuracy: 0.9089\n",
      "10000/10000 [==============================] - 0s 34us/sample - loss: 0.0143 - accuracy: 0.9093\n",
      "10000/10000 [==============================] - 0s 36us/sample - loss: 0.0144 - accuracy: 0.9097\n",
      "10000/10000 [==============================] - 0s 35us/sample - loss: 0.0145 - accuracy: 0.9089\n",
      "10000/10000 [==============================] - 0s 34us/sample - loss: 0.0146 - accuracy: 0.9081\n",
      "10000/10000 [==============================] - 0s 34us/sample - loss: 0.0147 - accuracy: 0.9070\n",
      "10000/10000 [==============================] - 0s 35us/sample - loss: 0.0149 - accuracy: 0.9061\n",
      "10000/10000 [==============================] - 0s 36us/sample - loss: 0.0151 - accuracy: 0.9052\n",
      "10000/10000 [==============================] - 0s 36us/sample - loss: 0.0154 - accuracy: 0.9040\n",
      "10000/10000 [==============================] - 0s 34us/sample - loss: 0.0157 - accuracy: 0.9020\n",
      "10000/10000 [==============================] - 0s 35us/sample - loss: 0.0160 - accuracy: 0.9001\n",
      "10000/10000 [==============================] - 0s 36us/sample - loss: 0.0163 - accuracy: 0.8978\n",
      "10000/10000 [==============================] - 0s 36us/sample - loss: 0.0167 - accuracy: 0.8959\n",
      "161th iteration\n",
      "10000/10000 [==============================] - 0s 36us/sample - loss: 0.0151 - accuracy: 0.8978\n",
      "10000/10000 [==============================] - 0s 34us/sample - loss: 0.0150 - accuracy: 0.8998\n",
      "10000/10000 [==============================] - 0s 35us/sample - loss: 0.0149 - accuracy: 0.9019\n",
      "10000/10000 [==============================] - 0s 36us/sample - loss: 0.0148 - accuracy: 0.9031\n",
      "10000/10000 [==============================] - 0s 34us/sample - loss: 0.0147 - accuracy: 0.9039\n",
      "10000/10000 [==============================] - 0s 34us/sample - loss: 0.0147 - accuracy: 0.9045\n",
      "10000/10000 [==============================] - 0s 35us/sample - loss: 0.0147 - accuracy: 0.9049\n",
      "10000/10000 [==============================] - 0s 35us/sample - loss: 0.0147 - accuracy: 0.9044\n",
      "10000/10000 [==============================] - 0s 36us/sample - loss: 0.0148 - accuracy: 0.9046\n",
      "10000/10000 [==============================] - 0s 35us/sample - loss: 0.0150 - accuracy: 0.9046\n",
      "10000/10000 [==============================] - 0s 34us/sample - loss: 0.0151 - accuracy: 0.9045\n",
      "10000/10000 [==============================] - 0s 36us/sample - loss: 0.0153 - accuracy: 0.9043\n",
      "10000/10000 [==============================] - 0s 36us/sample - loss: 0.0155 - accuracy: 0.9032\n",
      "10000/10000 [==============================] - 0s 36us/sample - loss: 0.0158 - accuracy: 0.9015\n",
      "10000/10000 [==============================] - 0s 37us/sample - loss: 0.0162 - accuracy: 0.9003\n",
      "10000/10000 [==============================] - 0s 34us/sample - loss: 0.0165 - accuracy: 0.8984\n",
      "10000/10000 [==============================] - 0s 35us/sample - loss: 0.0169 - accuracy: 0.8955\n",
      "10000/10000 [==============================] - 0s 36us/sample - loss: 0.0174 - accuracy: 0.8918\n",
      "10000/10000 [==============================] - 0s 34us/sample - loss: 0.0179 - accuracy: 0.8889\n",
      "10000/10000 [==============================] - 0s 37us/sample - loss: 0.0185 - accuracy: 0.8855\n",
      "10000/10000 [==============================] - 0s 36us/sample - loss: 0.0191 - accuracy: 0.8818\n",
      "162th iteration\n",
      "10000/10000 [==============================] - 0s 35us/sample - loss: 0.0151 - accuracy: 0.9033\n",
      "10000/10000 [==============================] - 0s 34us/sample - loss: 0.0153 - accuracy: 0.9023\n",
      "10000/10000 [==============================] - 0s 34us/sample - loss: 0.0155 - accuracy: 0.9018\n",
      "10000/10000 [==============================] - 0s 34us/sample - loss: 0.0158 - accuracy: 0.9015\n",
      "10000/10000 [==============================] - 0s 34us/sample - loss: 0.0162 - accuracy: 0.9000\n",
      "10000/10000 [==============================] - 0s 35us/sample - loss: 0.0167 - accuracy: 0.8994\n",
      "10000/10000 [==============================] - 0s 36us/sample - loss: 0.0172 - accuracy: 0.8980\n",
      "10000/10000 [==============================] - 0s 34us/sample - loss: 0.0179 - accuracy: 0.8953\n",
      "10000/10000 [==============================] - 0s 34us/sample - loss: 0.0186 - accuracy: 0.8932\n",
      "10000/10000 [==============================] - 0s 34us/sample - loss: 0.0195 - accuracy: 0.8897\n",
      "10000/10000 [==============================] - 0s 36us/sample - loss: 0.0204 - accuracy: 0.8864\n",
      "10000/10000 [==============================] - 0s 36us/sample - loss: 0.0215 - accuracy: 0.8824\n",
      "10000/10000 [==============================] - 0s 35us/sample - loss: 0.0227 - accuracy: 0.8784\n",
      "10000/10000 [==============================] - 0s 36us/sample - loss: 0.0240 - accuracy: 0.8719\n",
      "10000/10000 [==============================] - 0s 36us/sample - loss: 0.0253 - accuracy: 0.8656\n",
      "10000/10000 [==============================] - 0s 36us/sample - loss: 0.0268 - accuracy: 0.8603\n",
      "10000/10000 [==============================] - 0s 36us/sample - loss: 0.0284 - accuracy: 0.8495\n",
      "10000/10000 [==============================] - 0s 36us/sample - loss: 0.0301 - accuracy: 0.8382\n",
      "10000/10000 [==============================] - 0s 34us/sample - loss: 0.0318 - accuracy: 0.8270\n",
      "10000/10000 [==============================] - 0s 38us/sample - loss: 0.0336 - accuracy: 0.8159\n",
      "10000/10000 [==============================] - 0s 38us/sample - loss: 0.0355 - accuracy: 0.8016\n",
      "163th iteration\n",
      "10000/10000 [==============================] - 0s 36us/sample - loss: 0.0159 - accuracy: 0.8943\n",
      "10000/10000 [==============================] - 0s 35us/sample - loss: 0.0160 - accuracy: 0.8947\n",
      "10000/10000 [==============================] - 0s 34us/sample - loss: 0.0162 - accuracy: 0.8946\n",
      "10000/10000 [==============================] - 0s 35us/sample - loss: 0.0164 - accuracy: 0.8939\n",
      "10000/10000 [==============================] - 0s 35us/sample - loss: 0.0167 - accuracy: 0.8934\n",
      "10000/10000 [==============================] - 0s 36us/sample - loss: 0.0171 - accuracy: 0.8924\n",
      "10000/10000 [==============================] - 0s 34us/sample - loss: 0.0176 - accuracy: 0.8906\n",
      "10000/10000 [==============================] - 0s 34us/sample - loss: 0.0181 - accuracy: 0.8883\n",
      "10000/10000 [==============================] - 0s 36us/sample - loss: 0.0188 - accuracy: 0.8878\n",
      "10000/10000 [==============================] - 0s 34us/sample - loss: 0.0196 - accuracy: 0.8852\n",
      "10000/10000 [==============================] - 0s 34us/sample - loss: 0.0205 - accuracy: 0.8830\n",
      "10000/10000 [==============================] - 0s 34us/sample - loss: 0.0215 - accuracy: 0.8806\n",
      "10000/10000 [==============================] - 0s 34us/sample - loss: 0.0226 - accuracy: 0.8762\n",
      "10000/10000 [==============================] - 0s 34us/sample - loss: 0.0238 - accuracy: 0.8701\n",
      "10000/10000 [==============================] - 0s 37us/sample - loss: 0.0252 - accuracy: 0.8629\n",
      "10000/10000 [==============================] - 0s 36us/sample - loss: 0.0266 - accuracy: 0.8543\n",
      "10000/10000 [==============================] - 0s 37us/sample - loss: 0.0282 - accuracy: 0.8481\n",
      "10000/10000 [==============================] - 0s 36us/sample - loss: 0.0299 - accuracy: 0.8376\n",
      "10000/10000 [==============================] - 0s 34us/sample - loss: 0.0317 - accuracy: 0.8268\n",
      "10000/10000 [==============================] - 0s 34us/sample - loss: 0.0335 - accuracy: 0.8146\n",
      "10000/10000 [==============================] - 0s 35us/sample - loss: 0.0355 - accuracy: 0.8016\n",
      "164th iteration\n",
      "10000/10000 [==============================] - 0s 36us/sample - loss: 0.0151 - accuracy: 0.8978\n",
      "10000/10000 [==============================] - 0s 35us/sample - loss: 0.0149 - accuracy: 0.8996\n",
      "10000/10000 [==============================] - 0s 38us/sample - loss: 0.0147 - accuracy: 0.9015\n",
      "10000/10000 [==============================] - 0s 38us/sample - loss: 0.0145 - accuracy: 0.9034\n",
      "10000/10000 [==============================] - 0s 35us/sample - loss: 0.0144 - accuracy: 0.9044\n",
      "10000/10000 [==============================] - 0s 39us/sample - loss: 0.0143 - accuracy: 0.9045\n",
      "10000/10000 [==============================] - 0s 37us/sample - loss: 0.0142 - accuracy: 0.9057\n",
      "10000/10000 [==============================] - 0s 36us/sample - loss: 0.0141 - accuracy: 0.9064\n",
      "10000/10000 [==============================] - 0s 38us/sample - loss: 0.0141 - accuracy: 0.9068\n",
      "10000/10000 [==============================] - 0s 39us/sample - loss: 0.0141 - accuracy: 0.9073\n",
      "10000/10000 [==============================] - 0s 40us/sample - loss: 0.0142 - accuracy: 0.9064\n",
      "10000/10000 [==============================] - 0s 35us/sample - loss: 0.0142 - accuracy: 0.9076\n",
      "10000/10000 [==============================] - 0s 34us/sample - loss: 0.0143 - accuracy: 0.9073\n",
      "10000/10000 [==============================] - 0s 36us/sample - loss: 0.0144 - accuracy: 0.9066\n",
      "10000/10000 [==============================] - 0s 39us/sample - loss: 0.0146 - accuracy: 0.9056\n",
      "10000/10000 [==============================] - 0s 40us/sample - loss: 0.0148 - accuracy: 0.9035\n",
      "10000/10000 [==============================] - 0s 39us/sample - loss: 0.0150 - accuracy: 0.9019\n",
      "10000/10000 [==============================] - 1s 61us/sample - loss: 0.0152 - accuracy: 0.9007\n",
      "10000/10000 [==============================] - 0s 40us/sample - loss: 0.0155 - accuracy: 0.8999\n",
      "10000/10000 [==============================] - 0s 40us/sample - loss: 0.0158 - accuracy: 0.8984\n",
      "10000/10000 [==============================] - 0s 39us/sample - loss: 0.0161 - accuracy: 0.8961\n",
      "165th iteration\n",
      "10000/10000 [==============================] - 0s 37us/sample - loss: 0.0151 - accuracy: 0.8978\n",
      "10000/10000 [==============================] - 0s 39us/sample - loss: 0.0149 - accuracy: 0.9000\n",
      "10000/10000 [==============================] - 0s 37us/sample - loss: 0.0147 - accuracy: 0.9017\n",
      "10000/10000 [==============================] - 0s 36us/sample - loss: 0.0146 - accuracy: 0.9035\n",
      "10000/10000 [==============================] - 0s 37us/sample - loss: 0.0144 - accuracy: 0.9048\n",
      "10000/10000 [==============================] - 0s 37us/sample - loss: 0.0143 - accuracy: 0.9053\n",
      "10000/10000 [==============================] - 0s 36us/sample - loss: 0.0143 - accuracy: 0.9059\n",
      "10000/10000 [==============================] - 0s 36us/sample - loss: 0.0143 - accuracy: 0.9051\n",
      "10000/10000 [==============================] - 0s 38us/sample - loss: 0.0143 - accuracy: 0.9043\n",
      "10000/10000 [==============================] - 0s 38us/sample - loss: 0.0143 - accuracy: 0.9047\n",
      "10000/10000 [==============================] - 0s 37us/sample - loss: 0.0144 - accuracy: 0.9049\n",
      "10000/10000 [==============================] - 0s 36us/sample - loss: 0.0145 - accuracy: 0.9050\n",
      "10000/10000 [==============================] - 0s 36us/sample - loss: 0.0146 - accuracy: 0.9038\n",
      "10000/10000 [==============================] - 0s 36us/sample - loss: 0.0148 - accuracy: 0.9029\n",
      "10000/10000 [==============================] - 0s 38us/sample - loss: 0.0150 - accuracy: 0.9000\n",
      "10000/10000 [==============================] - 0s 36us/sample - loss: 0.0152 - accuracy: 0.8981\n",
      "10000/10000 [==============================] - 0s 36us/sample - loss: 0.0154 - accuracy: 0.8964\n",
      "10000/10000 [==============================] - 0s 35us/sample - loss: 0.0157 - accuracy: 0.8952\n",
      "10000/10000 [==============================] - 0s 36us/sample - loss: 0.0160 - accuracy: 0.8931\n",
      "10000/10000 [==============================] - 0s 35us/sample - loss: 0.0164 - accuracy: 0.8900\n",
      "10000/10000 [==============================] - 1s 58us/sample - loss: 0.0168 - accuracy: 0.8870\n",
      "166th iteration\n",
      "10000/10000 [==============================] - 0s 36us/sample - loss: 0.0168 - accuracy: 0.8870\n",
      "10000/10000 [==============================] - 0s 36us/sample - loss: 0.0168 - accuracy: 0.8875\n",
      "10000/10000 [==============================] - 0s 36us/sample - loss: 0.0169 - accuracy: 0.8882\n",
      "10000/10000 [==============================] - 0s 37us/sample - loss: 0.0171 - accuracy: 0.8883\n",
      "10000/10000 [==============================] - 0s 38us/sample - loss: 0.0174 - accuracy: 0.8883\n",
      "10000/10000 [==============================] - 0s 36us/sample - loss: 0.0177 - accuracy: 0.8874\n",
      "10000/10000 [==============================] - 0s 36us/sample - loss: 0.0181 - accuracy: 0.8870\n",
      "10000/10000 [==============================] - 0s 36us/sample - loss: 0.0186 - accuracy: 0.8869\n",
      "10000/10000 [==============================] - 0s 36us/sample - loss: 0.0193 - accuracy: 0.8856\n",
      "10000/10000 [==============================] - 0s 37us/sample - loss: 0.0200 - accuracy: 0.8846\n",
      "10000/10000 [==============================] - 0s 40us/sample - loss: 0.0209 - accuracy: 0.8822\n",
      "10000/10000 [==============================] - 0s 36us/sample - loss: 0.0218 - accuracy: 0.8786\n",
      "10000/10000 [==============================] - 0s 36us/sample - loss: 0.0229 - accuracy: 0.8735\n",
      "10000/10000 [==============================] - 0s 36us/sample - loss: 0.0241 - accuracy: 0.8681\n",
      "10000/10000 [==============================] - 0s 36us/sample - loss: 0.0255 - accuracy: 0.8628\n",
      "10000/10000 [==============================] - 0s 36us/sample - loss: 0.0269 - accuracy: 0.8549\n",
      "10000/10000 [==============================] - 0s 36us/sample - loss: 0.0285 - accuracy: 0.8469\n",
      "10000/10000 [==============================] - 0s 36us/sample - loss: 0.0301 - accuracy: 0.8371\n",
      "10000/10000 [==============================] - 0s 36us/sample - loss: 0.0318 - accuracy: 0.8258\n",
      "10000/10000 [==============================] - 0s 38us/sample - loss: 0.0336 - accuracy: 0.8155\n",
      "10000/10000 [==============================] - 0s 36us/sample - loss: 0.0355 - accuracy: 0.8016\n",
      "167th iteration\n",
      "10000/10000 [==============================] - 0s 36us/sample - loss: 0.0156 - accuracy: 0.8985\n",
      "10000/10000 [==============================] - 0s 35us/sample - loss: 0.0156 - accuracy: 0.8975\n",
      "10000/10000 [==============================] - 0s 36us/sample - loss: 0.0156 - accuracy: 0.8972\n",
      "10000/10000 [==============================] - 0s 37us/sample - loss: 0.0157 - accuracy: 0.8974\n",
      "10000/10000 [==============================] - 0s 37us/sample - loss: 0.0159 - accuracy: 0.8966\n",
      "10000/10000 [==============================] - 0s 36us/sample - loss: 0.0160 - accuracy: 0.8975\n",
      "10000/10000 [==============================] - 0s 36us/sample - loss: 0.0163 - accuracy: 0.8968\n",
      "10000/10000 [==============================] - 0s 36us/sample - loss: 0.0165 - accuracy: 0.8965\n",
      "10000/10000 [==============================] - 0s 36us/sample - loss: 0.0169 - accuracy: 0.8948\n",
      "10000/10000 [==============================] - 0s 37us/sample - loss: 0.0173 - accuracy: 0.8943\n",
      "10000/10000 [==============================] - 0s 36us/sample - loss: 0.0178 - accuracy: 0.8924\n",
      "10000/10000 [==============================] - 0s 36us/sample - loss: 0.0183 - accuracy: 0.8921\n",
      "10000/10000 [==============================] - 0s 36us/sample - loss: 0.0189 - accuracy: 0.8910\n",
      "10000/10000 [==============================] - 0s 37us/sample - loss: 0.0196 - accuracy: 0.8894\n",
      "10000/10000 [==============================] - 0s 36us/sample - loss: 0.0203 - accuracy: 0.8864\n",
      "10000/10000 [==============================] - 0s 39us/sample - loss: 0.0211 - accuracy: 0.8827\n",
      "10000/10000 [==============================] - 0s 41us/sample - loss: 0.0220 - accuracy: 0.8792\n",
      "10000/10000 [==============================] - 0s 39us/sample - loss: 0.0230 - accuracy: 0.8748\n",
      "10000/10000 [==============================] - 0s 43us/sample - loss: 0.0240 - accuracy: 0.8705\n",
      "10000/10000 [==============================] - 0s 42us/sample - loss: 0.0251 - accuracy: 0.8671\n",
      "10000/10000 [==============================] - 0s 39us/sample - loss: 0.0263 - accuracy: 0.8606\n",
      "168th iteration\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000/10000 [==============================] - 0s 40us/sample - loss: 0.0151 - accuracy: 0.8978\n",
      "10000/10000 [==============================] - 0s 39us/sample - loss: 0.0150 - accuracy: 0.8986\n",
      "10000/10000 [==============================] - 0s 40us/sample - loss: 0.0149 - accuracy: 0.9000\n",
      "10000/10000 [==============================] - 0s 39us/sample - loss: 0.0149 - accuracy: 0.9011\n",
      "10000/10000 [==============================] - 0s 42us/sample - loss: 0.0149 - accuracy: 0.9021\n",
      "10000/10000 [==============================] - 0s 39us/sample - loss: 0.0149 - accuracy: 0.9027\n",
      "10000/10000 [==============================] - 0s 37us/sample - loss: 0.0149 - accuracy: 0.9019\n",
      "10000/10000 [==============================] - 0s 37us/sample - loss: 0.0150 - accuracy: 0.9016\n",
      "10000/10000 [==============================] - 0s 37us/sample - loss: 0.0151 - accuracy: 0.9017\n",
      "10000/10000 [==============================] - 0s 38us/sample - loss: 0.0152 - accuracy: 0.9007\n",
      "10000/10000 [==============================] - 0s 41us/sample - loss: 0.0154 - accuracy: 0.9002\n",
      "10000/10000 [==============================] - 0s 40us/sample - loss: 0.0156 - accuracy: 0.9002\n",
      "10000/10000 [==============================] - 0s 43us/sample - loss: 0.0158 - accuracy: 0.9001\n",
      "10000/10000 [==============================] - 0s 39us/sample - loss: 0.0161 - accuracy: 0.8987\n",
      "10000/10000 [==============================] - 0s 40us/sample - loss: 0.0164 - accuracy: 0.8975\n",
      "10000/10000 [==============================] - 0s 38us/sample - loss: 0.0167 - accuracy: 0.8968\n",
      "10000/10000 [==============================] - 0s 39us/sample - loss: 0.0171 - accuracy: 0.8950\n",
      "10000/10000 [==============================] - 0s 39us/sample - loss: 0.0175 - accuracy: 0.8933\n",
      "10000/10000 [==============================] - 0s 37us/sample - loss: 0.0180 - accuracy: 0.8910\n",
      "10000/10000 [==============================] - 0s 37us/sample - loss: 0.0184 - accuracy: 0.8876\n",
      "10000/10000 [==============================] - 0s 37us/sample - loss: 0.0190 - accuracy: 0.8848\n",
      "169th iteration\n",
      "10000/10000 [==============================] - 0s 36us/sample - loss: 0.0180 - accuracy: 0.8781\n",
      "10000/10000 [==============================] - 0s 39us/sample - loss: 0.0176 - accuracy: 0.8805\n",
      "10000/10000 [==============================] - 0s 39us/sample - loss: 0.0172 - accuracy: 0.8837\n",
      "10000/10000 [==============================] - 0s 42us/sample - loss: 0.0168 - accuracy: 0.8866\n",
      "10000/10000 [==============================] - 0s 38us/sample - loss: 0.0165 - accuracy: 0.8886\n",
      "10000/10000 [==============================] - 0s 39us/sample - loss: 0.0161 - accuracy: 0.8905\n",
      "10000/10000 [==============================] - 0s 37us/sample - loss: 0.0158 - accuracy: 0.8934\n",
      "10000/10000 [==============================] - 0s 38us/sample - loss: 0.0156 - accuracy: 0.8958\n",
      "10000/10000 [==============================] - 0s 42us/sample - loss: 0.0153 - accuracy: 0.8969\n",
      "10000/10000 [==============================] - 0s 41us/sample - loss: 0.0151 - accuracy: 0.8985\n",
      "10000/10000 [==============================] - 0s 39us/sample - loss: 0.0150 - accuracy: 0.9003\n",
      "10000/10000 [==============================] - 0s 41us/sample - loss: 0.0149 - accuracy: 0.9006\n",
      "10000/10000 [==============================] - 0s 39us/sample - loss: 0.0148 - accuracy: 0.9017\n",
      "10000/10000 [==============================] - 0s 37us/sample - loss: 0.0147 - accuracy: 0.9025\n",
      "10000/10000 [==============================] - 0s 37us/sample - loss: 0.0147 - accuracy: 0.9024\n",
      "10000/10000 [==============================] - 0s 39us/sample - loss: 0.0148 - accuracy: 0.9024\n",
      "10000/10000 [==============================] - 0s 37us/sample - loss: 0.0149 - accuracy: 0.9017\n",
      "10000/10000 [==============================] - 0s 38us/sample - loss: 0.0150 - accuracy: 0.9006\n",
      "10000/10000 [==============================] - 0s 39us/sample - loss: 0.0151 - accuracy: 0.9001\n",
      "10000/10000 [==============================] - 0s 37us/sample - loss: 0.0153 - accuracy: 0.8991\n",
      "10000/10000 [==============================] - 0s 37us/sample - loss: 0.0156 - accuracy: 0.8985\n",
      "170th iteration\n",
      "10000/10000 [==============================] - 0s 37us/sample - loss: 0.0155 - accuracy: 0.8978\n",
      "10000/10000 [==============================] - 0s 37us/sample - loss: 0.0153 - accuracy: 0.9002\n",
      "10000/10000 [==============================] - 0s 37us/sample - loss: 0.0151 - accuracy: 0.9015\n",
      "10000/10000 [==============================] - 0s 38us/sample - loss: 0.0149 - accuracy: 0.9029\n",
      "10000/10000 [==============================] - 0s 37us/sample - loss: 0.0148 - accuracy: 0.9033\n",
      "10000/10000 [==============================] - 0s 37us/sample - loss: 0.0147 - accuracy: 0.9040\n",
      "10000/10000 [==============================] - 0s 37us/sample - loss: 0.0146 - accuracy: 0.9055\n",
      "10000/10000 [==============================] - 0s 38us/sample - loss: 0.0146 - accuracy: 0.9052\n",
      "10000/10000 [==============================] - 0s 40us/sample - loss: 0.0145 - accuracy: 0.9053\n",
      "10000/10000 [==============================] - 0s 40us/sample - loss: 0.0146 - accuracy: 0.9051\n",
      "10000/10000 [==============================] - 0s 38us/sample - loss: 0.0146 - accuracy: 0.9045\n",
      "10000/10000 [==============================] - 0s 37us/sample - loss: 0.0147 - accuracy: 0.9040\n",
      "10000/10000 [==============================] - 0s 37us/sample - loss: 0.0148 - accuracy: 0.9026\n",
      "10000/10000 [==============================] - 0s 38us/sample - loss: 0.0149 - accuracy: 0.9025\n",
      "10000/10000 [==============================] - 0s 37us/sample - loss: 0.0151 - accuracy: 0.9004\n",
      "10000/10000 [==============================] - 0s 37us/sample - loss: 0.0153 - accuracy: 0.8997\n",
      "10000/10000 [==============================] - 0s 39us/sample - loss: 0.0155 - accuracy: 0.8967\n",
      "10000/10000 [==============================] - 0s 38us/sample - loss: 0.0158 - accuracy: 0.8939\n",
      "10000/10000 [==============================] - 0s 41us/sample - loss: 0.0161 - accuracy: 0.8912\n",
      "10000/10000 [==============================] - 0s 40us/sample - loss: 0.0164 - accuracy: 0.8881\n",
      "10000/10000 [==============================] - 0s 40us/sample - loss: 0.0168 - accuracy: 0.8853\n",
      "171th iteration\n",
      "10000/10000 [==============================] - 1s 63us/sample - loss: 0.0151 - accuracy: 0.8978\n",
      "10000/10000 [==============================] - 0s 40us/sample - loss: 0.0149 - accuracy: 0.8996\n",
      "10000/10000 [==============================] - 0s 42us/sample - loss: 0.0147 - accuracy: 0.9021\n",
      "10000/10000 [==============================] - 0s 40us/sample - loss: 0.0145 - accuracy: 0.9036\n",
      "10000/10000 [==============================] - 0s 40us/sample - loss: 0.0144 - accuracy: 0.9050\n",
      "10000/10000 [==============================] - 0s 41us/sample - loss: 0.0143 - accuracy: 0.9054\n",
      "10000/10000 [==============================] - 0s 40us/sample - loss: 0.0142 - accuracy: 0.9059\n",
      "10000/10000 [==============================] - 0s 37us/sample - loss: 0.0141 - accuracy: 0.9054\n",
      "10000/10000 [==============================] - 0s 38us/sample - loss: 0.0141 - accuracy: 0.9069\n",
      "10000/10000 [==============================] - 0s 39us/sample - loss: 0.0141 - accuracy: 0.9077\n",
      "10000/10000 [==============================] - 0s 38us/sample - loss: 0.0142 - accuracy: 0.9076\n",
      "10000/10000 [==============================] - 0s 38us/sample - loss: 0.0142 - accuracy: 0.9073\n",
      "10000/10000 [==============================] - 0s 39us/sample - loss: 0.0143 - accuracy: 0.9061\n",
      "10000/10000 [==============================] - 0s 37us/sample - loss: 0.0145 - accuracy: 0.9039\n",
      "10000/10000 [==============================] - 0s 38us/sample - loss: 0.0146 - accuracy: 0.9027\n",
      "10000/10000 [==============================] - 0s 38us/sample - loss: 0.0148 - accuracy: 0.9012\n",
      "10000/10000 [==============================] - 0s 37us/sample - loss: 0.0150 - accuracy: 0.9000\n",
      "10000/10000 [==============================] - 0s 40us/sample - loss: 0.0152 - accuracy: 0.8987\n",
      "10000/10000 [==============================] - 0s 38us/sample - loss: 0.0155 - accuracy: 0.8968\n",
      "10000/10000 [==============================] - 0s 39us/sample - loss: 0.0158 - accuracy: 0.8952\n",
      "10000/10000 [==============================] - 0s 39us/sample - loss: 0.0161 - accuracy: 0.8926\n",
      "172th iteration\n",
      "10000/10000 [==============================] - 0s 37us/sample - loss: 0.0168 - accuracy: 0.8853\n",
      "10000/10000 [==============================] - 0s 39us/sample - loss: 0.0163 - accuracy: 0.8882\n",
      "10000/10000 [==============================] - 0s 38us/sample - loss: 0.0159 - accuracy: 0.8920\n",
      "10000/10000 [==============================] - 1s 60us/sample - loss: 0.0156 - accuracy: 0.8950\n",
      "10000/10000 [==============================] - 0s 37us/sample - loss: 0.0153 - accuracy: 0.8980\n",
      "10000/10000 [==============================] - 0s 37us/sample - loss: 0.0150 - accuracy: 0.9002\n",
      "10000/10000 [==============================] - 0s 37us/sample - loss: 0.0148 - accuracy: 0.9025\n",
      "10000/10000 [==============================] - 0s 38us/sample - loss: 0.0146 - accuracy: 0.9051\n",
      "10000/10000 [==============================] - 0s 38us/sample - loss: 0.0144 - accuracy: 0.9053\n",
      "10000/10000 [==============================] - 0s 38us/sample - loss: 0.0143 - accuracy: 0.9049\n",
      "10000/10000 [==============================] - 0s 37us/sample - loss: 0.0142 - accuracy: 0.9059\n",
      "10000/10000 [==============================] - 0s 37us/sample - loss: 0.0142 - accuracy: 0.9060\n",
      "10000/10000 [==============================] - 0s 37us/sample - loss: 0.0142 - accuracy: 0.9061\n",
      "10000/10000 [==============================] - 0s 37us/sample - loss: 0.0142 - accuracy: 0.9065\n",
      "10000/10000 [==============================] - 0s 38us/sample - loss: 0.0143 - accuracy: 0.9065\n",
      "10000/10000 [==============================] - 0s 37us/sample - loss: 0.0144 - accuracy: 0.9058\n",
      "10000/10000 [==============================] - 0s 37us/sample - loss: 0.0146 - accuracy: 0.9036\n",
      "10000/10000 [==============================] - 0s 37us/sample - loss: 0.0148 - accuracy: 0.9021\n",
      "10000/10000 [==============================] - 0s 37us/sample - loss: 0.0150 - accuracy: 0.9004\n",
      "10000/10000 [==============================] - 0s 39us/sample - loss: 0.0153 - accuracy: 0.8994\n",
      "10000/10000 [==============================] - 0s 37us/sample - loss: 0.0156 - accuracy: 0.8985\n",
      "173th iteration\n",
      "10000/10000 [==============================] - 0s 37us/sample - loss: 0.0168 - accuracy: 0.8853\n",
      "10000/10000 [==============================] - 0s 38us/sample - loss: 0.0167 - accuracy: 0.8868\n",
      "10000/10000 [==============================] - 0s 40us/sample - loss: 0.0167 - accuracy: 0.8875\n",
      "10000/10000 [==============================] - 0s 40us/sample - loss: 0.0167 - accuracy: 0.8884\n",
      "10000/10000 [==============================] - 0s 37us/sample - loss: 0.0167 - accuracy: 0.8890\n",
      "10000/10000 [==============================] - 0s 41us/sample - loss: 0.0168 - accuracy: 0.8887\n",
      "10000/10000 [==============================] - 0s 44us/sample - loss: 0.0169 - accuracy: 0.8883\n",
      "10000/10000 [==============================] - 0s 39us/sample - loss: 0.0171 - accuracy: 0.8887\n",
      "10000/10000 [==============================] - 0s 39us/sample - loss: 0.0174 - accuracy: 0.8888\n",
      "10000/10000 [==============================] - 0s 39us/sample - loss: 0.0177 - accuracy: 0.8886\n",
      "10000/10000 [==============================] - 0s 40us/sample - loss: 0.0181 - accuracy: 0.8872\n",
      "10000/10000 [==============================] - 0s 42us/sample - loss: 0.0185 - accuracy: 0.8862\n",
      "10000/10000 [==============================] - 0s 41us/sample - loss: 0.0191 - accuracy: 0.8855\n",
      "10000/10000 [==============================] - 0s 37us/sample - loss: 0.0197 - accuracy: 0.8839\n",
      "10000/10000 [==============================] - 0s 37us/sample - loss: 0.0203 - accuracy: 0.8821\n",
      "10000/10000 [==============================] - 0s 37us/sample - loss: 0.0211 - accuracy: 0.8801\n",
      "10000/10000 [==============================] - 0s 37us/sample - loss: 0.0220 - accuracy: 0.8777\n",
      "10000/10000 [==============================] - 0s 37us/sample - loss: 0.0229 - accuracy: 0.8751\n",
      "10000/10000 [==============================] - 0s 37us/sample - loss: 0.0240 - accuracy: 0.8714\n",
      "10000/10000 [==============================] - 0s 37us/sample - loss: 0.0251 - accuracy: 0.8674\n",
      "10000/10000 [==============================] - 0s 38us/sample - loss: 0.0263 - accuracy: 0.8606\n",
      "174th iteration\n",
      "10000/10000 [==============================] - 0s 37us/sample - loss: 0.0155 - accuracy: 0.8978\n",
      "10000/10000 [==============================] - 0s 37us/sample - loss: 0.0153 - accuracy: 0.9000\n",
      "10000/10000 [==============================] - 0s 37us/sample - loss: 0.0151 - accuracy: 0.9015\n",
      "10000/10000 [==============================] - 0s 38us/sample - loss: 0.0149 - accuracy: 0.9021\n",
      "10000/10000 [==============================] - 0s 43us/sample - loss: 0.0148 - accuracy: 0.9027\n",
      "10000/10000 [==============================] - 0s 39us/sample - loss: 0.0147 - accuracy: 0.9028\n",
      "10000/10000 [==============================] - 0s 40us/sample - loss: 0.0147 - accuracy: 0.9037\n",
      "10000/10000 [==============================] - 0s 42us/sample - loss: 0.0147 - accuracy: 0.9029\n",
      "10000/10000 [==============================] - 0s 40us/sample - loss: 0.0148 - accuracy: 0.9015\n",
      "10000/10000 [==============================] - 0s 42us/sample - loss: 0.0149 - accuracy: 0.9009\n",
      "10000/10000 [==============================] - 0s 38us/sample - loss: 0.0150 - accuracy: 0.9007\n",
      "10000/10000 [==============================] - 0s 38us/sample - loss: 0.0151 - accuracy: 0.8992\n",
      "10000/10000 [==============================] - 0s 37us/sample - loss: 0.0153 - accuracy: 0.8985\n",
      "10000/10000 [==============================] - 0s 37us/sample - loss: 0.0156 - accuracy: 0.8961\n",
      "10000/10000 [==============================] - 0s 37us/sample - loss: 0.0158 - accuracy: 0.8945\n",
      "10000/10000 [==============================] - 0s 37us/sample - loss: 0.0161 - accuracy: 0.8920\n",
      "10000/10000 [==============================] - 0s 37us/sample - loss: 0.0164 - accuracy: 0.8898\n",
      "10000/10000 [==============================] - 0s 37us/sample - loss: 0.0168 - accuracy: 0.8874\n",
      "10000/10000 [==============================] - 0s 37us/sample - loss: 0.0172 - accuracy: 0.8856\n",
      "10000/10000 [==============================] - 0s 37us/sample - loss: 0.0176 - accuracy: 0.8815\n",
      "10000/10000 [==============================] - 0s 39us/sample - loss: 0.0180 - accuracy: 0.8781\n",
      "175th iteration\n",
      "10000/10000 [==============================] - 0s 38us/sample - loss: 0.0180 - accuracy: 0.8781\n",
      "10000/10000 [==============================] - 0s 37us/sample - loss: 0.0178 - accuracy: 0.8787\n",
      "10000/10000 [==============================] - 0s 37us/sample - loss: 0.0177 - accuracy: 0.8805\n",
      "10000/10000 [==============================] - 0s 37us/sample - loss: 0.0176 - accuracy: 0.8817\n",
      "10000/10000 [==============================] - 0s 37us/sample - loss: 0.0175 - accuracy: 0.8819\n",
      "10000/10000 [==============================] - 0s 37us/sample - loss: 0.0175 - accuracy: 0.8821\n",
      "10000/10000 [==============================] - 0s 37us/sample - loss: 0.0176 - accuracy: 0.8832\n",
      "10000/10000 [==============================] - 0s 37us/sample - loss: 0.0176 - accuracy: 0.8837\n",
      "10000/10000 [==============================] - 0s 37us/sample - loss: 0.0178 - accuracy: 0.8839\n",
      "10000/10000 [==============================] - 0s 37us/sample - loss: 0.0180 - accuracy: 0.8840\n",
      "10000/10000 [==============================] - 0s 37us/sample - loss: 0.0183 - accuracy: 0.8843\n",
      "10000/10000 [==============================] - 0s 36us/sample - loss: 0.0187 - accuracy: 0.8840\n",
      "10000/10000 [==============================] - 0s 37us/sample - loss: 0.0192 - accuracy: 0.8831\n",
      "10000/10000 [==============================] - 0s 38us/sample - loss: 0.0197 - accuracy: 0.8824\n",
      "10000/10000 [==============================] - 0s 38us/sample - loss: 0.0204 - accuracy: 0.8812\n",
      "10000/10000 [==============================] - 0s 37us/sample - loss: 0.0211 - accuracy: 0.8798\n",
      "10000/10000 [==============================] - 0s 37us/sample - loss: 0.0220 - accuracy: 0.8784\n",
      "10000/10000 [==============================] - 0s 37us/sample - loss: 0.0229 - accuracy: 0.8745\n",
      "10000/10000 [==============================] - 0s 37us/sample - loss: 0.0239 - accuracy: 0.8709\n",
      "10000/10000 [==============================] - 0s 37us/sample - loss: 0.0251 - accuracy: 0.8675\n",
      "10000/10000 [==============================] - 0s 37us/sample - loss: 0.0263 - accuracy: 0.8606\n",
      "176th iteration\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000/10000 [==============================] - 0s 37us/sample - loss: 0.0151 - accuracy: 0.8978\n",
      "10000/10000 [==============================] - 0s 37us/sample - loss: 0.0150 - accuracy: 0.8991\n",
      "10000/10000 [==============================] - 0s 37us/sample - loss: 0.0150 - accuracy: 0.9012\n",
      "10000/10000 [==============================] - 0s 38us/sample - loss: 0.0150 - accuracy: 0.9020\n",
      "10000/10000 [==============================] - 0s 37us/sample - loss: 0.0150 - accuracy: 0.9027\n",
      "10000/10000 [==============================] - 0s 37us/sample - loss: 0.0150 - accuracy: 0.9028\n",
      "10000/10000 [==============================] - 0s 37us/sample - loss: 0.0151 - accuracy: 0.9025\n",
      "10000/10000 [==============================] - 0s 37us/sample - loss: 0.0152 - accuracy: 0.9034\n",
      "10000/10000 [==============================] - 0s 38us/sample - loss: 0.0153 - accuracy: 0.9026\n",
      "10000/10000 [==============================] - 0s 37us/sample - loss: 0.0155 - accuracy: 0.9023\n",
      "10000/10000 [==============================] - 0s 37us/sample - loss: 0.0158 - accuracy: 0.9016\n",
      "10000/10000 [==============================] - 0s 37us/sample - loss: 0.0160 - accuracy: 0.9015\n",
      "10000/10000 [==============================] - 0s 37us/sample - loss: 0.0164 - accuracy: 0.9010\n",
      "10000/10000 [==============================] - 0s 37us/sample - loss: 0.0167 - accuracy: 0.9001\n",
      "10000/10000 [==============================] - 0s 38us/sample - loss: 0.0171 - accuracy: 0.8994\n",
      "10000/10000 [==============================] - 0s 38us/sample - loss: 0.0176 - accuracy: 0.8966\n",
      "10000/10000 [==============================] - 0s 38us/sample - loss: 0.0181 - accuracy: 0.8941\n",
      "10000/10000 [==============================] - 0s 38us/sample - loss: 0.0186 - accuracy: 0.8913\n",
      "10000/10000 [==============================] - 0s 37us/sample - loss: 0.0192 - accuracy: 0.8877\n",
      "10000/10000 [==============================] - 0s 37us/sample - loss: 0.0199 - accuracy: 0.8843\n",
      "10000/10000 [==============================] - 0s 37us/sample - loss: 0.0206 - accuracy: 0.8813\n",
      "177th iteration\n",
      "10000/10000 [==============================] - 0s 37us/sample - loss: 0.0155 - accuracy: 0.8978\n",
      "10000/10000 [==============================] - 0s 38us/sample - loss: 0.0155 - accuracy: 0.8998\n",
      "10000/10000 [==============================] - 0s 37us/sample - loss: 0.0155 - accuracy: 0.9005\n",
      "10000/10000 [==============================] - 0s 37us/sample - loss: 0.0156 - accuracy: 0.9006\n",
      "10000/10000 [==============================] - 1s 59us/sample - loss: 0.0157 - accuracy: 0.8998\n",
      "10000/10000 [==============================] - 0s 38us/sample - loss: 0.0159 - accuracy: 0.8988\n",
      "10000/10000 [==============================] - 0s 38us/sample - loss: 0.0161 - accuracy: 0.8987\n",
      "10000/10000 [==============================] - 0s 38us/sample - loss: 0.0164 - accuracy: 0.8980\n",
      "10000/10000 [==============================] - 0s 37us/sample - loss: 0.0167 - accuracy: 0.8979\n",
      "10000/10000 [==============================] - 0s 37us/sample - loss: 0.0171 - accuracy: 0.8970\n",
      "10000/10000 [==============================] - 0s 37us/sample - loss: 0.0176 - accuracy: 0.8948\n",
      "10000/10000 [==============================] - 0s 37us/sample - loss: 0.0181 - accuracy: 0.8935\n",
      "10000/10000 [==============================] - 0s 37us/sample - loss: 0.0187 - accuracy: 0.8917\n",
      "10000/10000 [==============================] - 0s 37us/sample - loss: 0.0194 - accuracy: 0.8895\n",
      "10000/10000 [==============================] - 0s 38us/sample - loss: 0.0202 - accuracy: 0.8876\n",
      "10000/10000 [==============================] - 0s 37us/sample - loss: 0.0210 - accuracy: 0.8853\n",
      "10000/10000 [==============================] - 0s 37us/sample - loss: 0.0219 - accuracy: 0.8812\n",
      "10000/10000 [==============================] - 0s 37us/sample - loss: 0.0229 - accuracy: 0.8776\n",
      "10000/10000 [==============================] - 0s 37us/sample - loss: 0.0239 - accuracy: 0.8726\n",
      "10000/10000 [==============================] - 0s 38us/sample - loss: 0.0251 - accuracy: 0.8675\n",
      "10000/10000 [==============================] - 0s 37us/sample - loss: 0.0263 - accuracy: 0.8606\n",
      "178th iteration\n",
      "10000/10000 [==============================] - 0s 37us/sample - loss: 0.0161 - accuracy: 0.8926\n",
      "10000/10000 [==============================] - 0s 38us/sample - loss: 0.0162 - accuracy: 0.8918\n",
      "10000/10000 [==============================] - 0s 37us/sample - loss: 0.0163 - accuracy: 0.8921\n",
      "10000/10000 [==============================] - 0s 38us/sample - loss: 0.0165 - accuracy: 0.8919\n",
      "10000/10000 [==============================] - 0s 37us/sample - loss: 0.0168 - accuracy: 0.8919\n",
      "10000/10000 [==============================] - 0s 37us/sample - loss: 0.0171 - accuracy: 0.8909\n",
      "10000/10000 [==============================] - 0s 37us/sample - loss: 0.0176 - accuracy: 0.8887\n",
      "10000/10000 [==============================] - 1s 60us/sample - loss: 0.0181 - accuracy: 0.8871\n",
      "10000/10000 [==============================] - 0s 40us/sample - loss: 0.0187 - accuracy: 0.8860\n",
      "10000/10000 [==============================] - 0s 42us/sample - loss: 0.0194 - accuracy: 0.8842\n",
      "10000/10000 [==============================] - 0s 43us/sample - loss: 0.0203 - accuracy: 0.8809\n",
      "10000/10000 [==============================] - 0s 38us/sample - loss: 0.0213 - accuracy: 0.8773\n",
      "10000/10000 [==============================] - 0s 37us/sample - loss: 0.0224 - accuracy: 0.8720\n",
      "10000/10000 [==============================] - 0s 39us/sample - loss: 0.0236 - accuracy: 0.8679\n",
      "10000/10000 [==============================] - 0s 38us/sample - loss: 0.0250 - accuracy: 0.8625\n",
      "10000/10000 [==============================] - 0s 39us/sample - loss: 0.0265 - accuracy: 0.8555\n",
      "10000/10000 [==============================] - 0s 38us/sample - loss: 0.0281 - accuracy: 0.8461\n",
      "10000/10000 [==============================] - 0s 37us/sample - loss: 0.0298 - accuracy: 0.8360\n",
      "10000/10000 [==============================] - 0s 37us/sample - loss: 0.0316 - accuracy: 0.8262\n",
      "10000/10000 [==============================] - 0s 37us/sample - loss: 0.0335 - accuracy: 0.8148\n",
      "10000/10000 [==============================] - 0s 37us/sample - loss: 0.0355 - accuracy: 0.8016\n",
      "179th iteration\n",
      "10000/10000 [==============================] - 0s 37us/sample - loss: 0.0151 - accuracy: 0.8978\n",
      "10000/10000 [==============================] - 0s 38us/sample - loss: 0.0149 - accuracy: 0.9004\n",
      "10000/10000 [==============================] - 0s 39us/sample - loss: 0.0146 - accuracy: 0.9016\n",
      "10000/10000 [==============================] - 0s 38us/sample - loss: 0.0144 - accuracy: 0.9027\n",
      "10000/10000 [==============================] - 0s 38us/sample - loss: 0.0143 - accuracy: 0.9039\n",
      "10000/10000 [==============================] - 0s 37us/sample - loss: 0.0141 - accuracy: 0.9050\n",
      "10000/10000 [==============================] - 0s 39us/sample - loss: 0.0140 - accuracy: 0.9062\n",
      "10000/10000 [==============================] - 0s 39us/sample - loss: 0.0139 - accuracy: 0.9068\n",
      "10000/10000 [==============================] - 0s 37us/sample - loss: 0.0139 - accuracy: 0.9065\n",
      "10000/10000 [==============================] - 0s 37us/sample - loss: 0.0139 - accuracy: 0.9057\n",
      "10000/10000 [==============================] - 0s 37us/sample - loss: 0.0139 - accuracy: 0.9058\n",
      "10000/10000 [==============================] - 0s 37us/sample - loss: 0.0139 - accuracy: 0.9067\n",
      "10000/10000 [==============================] - 1s 59us/sample - loss: 0.0140 - accuracy: 0.9065\n",
      "10000/10000 [==============================] - 0s 37us/sample - loss: 0.0141 - accuracy: 0.9061\n",
      "10000/10000 [==============================] - 0s 41us/sample - loss: 0.0142 - accuracy: 0.9056\n",
      "10000/10000 [==============================] - 0s 39us/sample - loss: 0.0144 - accuracy: 0.9049\n",
      "10000/10000 [==============================] - 0s 38us/sample - loss: 0.0146 - accuracy: 0.9039\n",
      "10000/10000 [==============================] - 0s 40us/sample - loss: 0.0148 - accuracy: 0.9035\n",
      "10000/10000 [==============================] - 0s 42us/sample - loss: 0.0150 - accuracy: 0.9026\n",
      "10000/10000 [==============================] - 0s 40us/sample - loss: 0.0153 - accuracy: 0.9005\n",
      "10000/10000 [==============================] - 0s 37us/sample - loss: 0.0156 - accuracy: 0.8985\n",
      "180th iteration\n",
      "10000/10000 [==============================] - 0s 38us/sample - loss: 0.0151 - accuracy: 0.8978\n",
      "10000/10000 [==============================] - 0s 42us/sample - loss: 0.0149 - accuracy: 0.9003\n",
      "10000/10000 [==============================] - 0s 42us/sample - loss: 0.0146 - accuracy: 0.9022\n",
      "10000/10000 [==============================] - 0s 42us/sample - loss: 0.0145 - accuracy: 0.9042\n",
      "10000/10000 [==============================] - 0s 41us/sample - loss: 0.0143 - accuracy: 0.9060\n",
      "10000/10000 [==============================] - 0s 37us/sample - loss: 0.0142 - accuracy: 0.9077\n",
      "10000/10000 [==============================] - 0s 37us/sample - loss: 0.0141 - accuracy: 0.9085\n",
      "10000/10000 [==============================] - 0s 37us/sample - loss: 0.0140 - accuracy: 0.9084\n",
      "10000/10000 [==============================] - 0s 37us/sample - loss: 0.0140 - accuracy: 0.9078\n",
      "10000/10000 [==============================] - 0s 37us/sample - loss: 0.0139 - accuracy: 0.9086\n",
      "10000/10000 [==============================] - 0s 37us/sample - loss: 0.0139 - accuracy: 0.9098\n",
      "10000/10000 [==============================] - 0s 37us/sample - loss: 0.0140 - accuracy: 0.9097\n",
      "10000/10000 [==============================] - 0s 38us/sample - loss: 0.0140 - accuracy: 0.9086\n",
      "10000/10000 [==============================] - 0s 37us/sample - loss: 0.0141 - accuracy: 0.9077\n",
      "10000/10000 [==============================] - 0s 37us/sample - loss: 0.0142 - accuracy: 0.9074\n",
      "10000/10000 [==============================] - 0s 38us/sample - loss: 0.0144 - accuracy: 0.9068\n",
      "10000/10000 [==============================] - 0s 37us/sample - loss: 0.0145 - accuracy: 0.9060\n",
      "10000/10000 [==============================] - 0s 38us/sample - loss: 0.0147 - accuracy: 0.9045\n",
      "10000/10000 [==============================] - 0s 38us/sample - loss: 0.0150 - accuracy: 0.9025\n",
      "10000/10000 [==============================] - 0s 37us/sample - loss: 0.0152 - accuracy: 0.9005\n",
      "10000/10000 [==============================] - 0s 37us/sample - loss: 0.0155 - accuracy: 0.8978\n",
      "181th iteration\n",
      "10000/10000 [==============================] - 0s 37us/sample - loss: 0.0151 - accuracy: 0.8978\n",
      "10000/10000 [==============================] - 0s 37us/sample - loss: 0.0149 - accuracy: 0.8999\n",
      "10000/10000 [==============================] - 0s 37us/sample - loss: 0.0148 - accuracy: 0.9008\n",
      "10000/10000 [==============================] - 0s 37us/sample - loss: 0.0147 - accuracy: 0.9019\n",
      "10000/10000 [==============================] - 0s 38us/sample - loss: 0.0146 - accuracy: 0.9029\n",
      "10000/10000 [==============================] - 0s 37us/sample - loss: 0.0146 - accuracy: 0.9027\n",
      "10000/10000 [==============================] - 0s 38us/sample - loss: 0.0146 - accuracy: 0.9029\n",
      "10000/10000 [==============================] - 0s 37us/sample - loss: 0.0147 - accuracy: 0.9022\n",
      "10000/10000 [==============================] - 0s 37us/sample - loss: 0.0147 - accuracy: 0.9013\n",
      "10000/10000 [==============================] - 0s 38us/sample - loss: 0.0149 - accuracy: 0.9005\n",
      "10000/10000 [==============================] - 0s 37us/sample - loss: 0.0150 - accuracy: 0.8993\n",
      "10000/10000 [==============================] - 0s 37us/sample - loss: 0.0152 - accuracy: 0.8968\n",
      "10000/10000 [==============================] - 0s 37us/sample - loss: 0.0154 - accuracy: 0.8949\n",
      "10000/10000 [==============================] - 0s 37us/sample - loss: 0.0156 - accuracy: 0.8939\n",
      "10000/10000 [==============================] - 0s 37us/sample - loss: 0.0159 - accuracy: 0.8933\n",
      "10000/10000 [==============================] - 0s 38us/sample - loss: 0.0162 - accuracy: 0.8908\n",
      "10000/10000 [==============================] - 0s 37us/sample - loss: 0.0165 - accuracy: 0.8882\n",
      "10000/10000 [==============================] - 0s 39us/sample - loss: 0.0168 - accuracy: 0.8863\n",
      "10000/10000 [==============================] - 0s 39us/sample - loss: 0.0172 - accuracy: 0.8845\n",
      "10000/10000 [==============================] - 0s 37us/sample - loss: 0.0176 - accuracy: 0.8815\n",
      "10000/10000 [==============================] - 0s 38us/sample - loss: 0.0180 - accuracy: 0.8781\n",
      "182th iteration\n",
      "10000/10000 [==============================] - 0s 40us/sample - loss: 0.0151 - accuracy: 0.8978\n",
      "10000/10000 [==============================] - 0s 40us/sample - loss: 0.0151 - accuracy: 0.8990\n",
      "10000/10000 [==============================] - 0s 39us/sample - loss: 0.0150 - accuracy: 0.9008\n",
      "10000/10000 [==============================] - 0s 40us/sample - loss: 0.0151 - accuracy: 0.9002\n",
      "10000/10000 [==============================] - 0s 37us/sample - loss: 0.0151 - accuracy: 0.9011\n",
      "10000/10000 [==============================] - 0s 38us/sample - loss: 0.0152 - accuracy: 0.9019\n",
      "10000/10000 [==============================] - 0s 37us/sample - loss: 0.0153 - accuracy: 0.9025\n",
      "10000/10000 [==============================] - 0s 37us/sample - loss: 0.0155 - accuracy: 0.9019\n",
      "10000/10000 [==============================] - 0s 37us/sample - loss: 0.0157 - accuracy: 0.9011\n",
      "10000/10000 [==============================] - 0s 37us/sample - loss: 0.0160 - accuracy: 0.9006\n",
      "10000/10000 [==============================] - 0s 37us/sample - loss: 0.0163 - accuracy: 0.8989\n",
      "10000/10000 [==============================] - 0s 39us/sample - loss: 0.0167 - accuracy: 0.8983\n",
      "10000/10000 [==============================] - 0s 37us/sample - loss: 0.0171 - accuracy: 0.8968\n",
      "10000/10000 [==============================] - 0s 37us/sample - loss: 0.0176 - accuracy: 0.8954\n",
      "10000/10000 [==============================] - 0s 37us/sample - loss: 0.0181 - accuracy: 0.8929\n",
      "10000/10000 [==============================] - 0s 37us/sample - loss: 0.0187 - accuracy: 0.8909\n",
      "10000/10000 [==============================] - 0s 37us/sample - loss: 0.0193 - accuracy: 0.8875\n",
      "10000/10000 [==============================] - 0s 37us/sample - loss: 0.0200 - accuracy: 0.8855\n",
      "10000/10000 [==============================] - 0s 37us/sample - loss: 0.0207 - accuracy: 0.8802\n",
      "10000/10000 [==============================] - 0s 37us/sample - loss: 0.0216 - accuracy: 0.8756\n",
      "10000/10000 [==============================] - 0s 37us/sample - loss: 0.0224 - accuracy: 0.8711\n",
      "183th iteration\n",
      "10000/10000 [==============================] - 0s 37us/sample - loss: 0.0151 - accuracy: 0.8978\n",
      "10000/10000 [==============================] - 0s 37us/sample - loss: 0.0149 - accuracy: 0.8998\n",
      "10000/10000 [==============================] - 0s 37us/sample - loss: 0.0146 - accuracy: 0.9028\n",
      "10000/10000 [==============================] - 0s 37us/sample - loss: 0.0145 - accuracy: 0.9043\n",
      "10000/10000 [==============================] - 0s 37us/sample - loss: 0.0143 - accuracy: 0.9044\n",
      "10000/10000 [==============================] - 0s 37us/sample - loss: 0.0142 - accuracy: 0.9054\n",
      "10000/10000 [==============================] - 0s 37us/sample - loss: 0.0141 - accuracy: 0.9059\n",
      "10000/10000 [==============================] - 0s 37us/sample - loss: 0.0141 - accuracy: 0.9067\n",
      "10000/10000 [==============================] - 0s 37us/sample - loss: 0.0141 - accuracy: 0.9061\n",
      "10000/10000 [==============================] - 0s 37us/sample - loss: 0.0141 - accuracy: 0.9057\n",
      "10000/10000 [==============================] - 0s 37us/sample - loss: 0.0142 - accuracy: 0.9045\n",
      "10000/10000 [==============================] - 0s 40us/sample - loss: 0.0142 - accuracy: 0.9041\n",
      "10000/10000 [==============================] - 0s 43us/sample - loss: 0.0144 - accuracy: 0.9038\n",
      "10000/10000 [==============================] - 0s 39us/sample - loss: 0.0145 - accuracy: 0.9021\n",
      "10000/10000 [==============================] - 0s 42us/sample - loss: 0.0147 - accuracy: 0.9020\n",
      "10000/10000 [==============================] - 0s 38us/sample - loss: 0.0150 - accuracy: 0.9013\n",
      "10000/10000 [==============================] - 0s 38us/sample - loss: 0.0153 - accuracy: 0.8990\n",
      "10000/10000 [==============================] - 0s 37us/sample - loss: 0.0156 - accuracy: 0.8953\n",
      "10000/10000 [==============================] - 0s 37us/sample - loss: 0.0159 - accuracy: 0.8919\n",
      "10000/10000 [==============================] - 0s 37us/sample - loss: 0.0163 - accuracy: 0.8893\n",
      "10000/10000 [==============================] - 0s 37us/sample - loss: 0.0168 - accuracy: 0.8853\n",
      "184th iteration\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000/10000 [==============================] - 0s 37us/sample - loss: 0.0156 - accuracy: 0.8985\n",
      "10000/10000 [==============================] - 0s 38us/sample - loss: 0.0157 - accuracy: 0.8975\n",
      "10000/10000 [==============================] - 0s 39us/sample - loss: 0.0158 - accuracy: 0.8972\n",
      "10000/10000 [==============================] - 0s 42us/sample - loss: 0.0160 - accuracy: 0.8965\n",
      "10000/10000 [==============================] - 0s 40us/sample - loss: 0.0163 - accuracy: 0.8965\n",
      "10000/10000 [==============================] - 0s 39us/sample - loss: 0.0166 - accuracy: 0.8945\n",
      "10000/10000 [==============================] - 0s 38us/sample - loss: 0.0171 - accuracy: 0.8944\n",
      "10000/10000 [==============================] - 0s 39us/sample - loss: 0.0177 - accuracy: 0.8930\n",
      "10000/10000 [==============================] - 0s 39us/sample - loss: 0.0183 - accuracy: 0.8912\n",
      "10000/10000 [==============================] - 0s 37us/sample - loss: 0.0191 - accuracy: 0.8888\n",
      "10000/10000 [==============================] - 1s 60us/sample - loss: 0.0201 - accuracy: 0.8842\n",
      "10000/10000 [==============================] - 0s 37us/sample - loss: 0.0211 - accuracy: 0.8814\n",
      "10000/10000 [==============================] - 0s 37us/sample - loss: 0.0223 - accuracy: 0.8773\n",
      "10000/10000 [==============================] - 0s 37us/sample - loss: 0.0236 - accuracy: 0.8723\n",
      "10000/10000 [==============================] - 0s 37us/sample - loss: 0.0250 - accuracy: 0.8663\n",
      "10000/10000 [==============================] - 0s 38us/sample - loss: 0.0265 - accuracy: 0.8581\n",
      "10000/10000 [==============================] - 0s 37us/sample - loss: 0.0281 - accuracy: 0.8489\n",
      "10000/10000 [==============================] - 0s 38us/sample - loss: 0.0298 - accuracy: 0.8392\n",
      "10000/10000 [==============================] - 0s 40us/sample - loss: 0.0316 - accuracy: 0.8273\n",
      "10000/10000 [==============================] - 0s 38us/sample - loss: 0.0335 - accuracy: 0.8149\n",
      "10000/10000 [==============================] - 0s 38us/sample - loss: 0.0355 - accuracy: 0.8016\n",
      "185th iteration\n",
      "10000/10000 [==============================] - 0s 41us/sample - loss: 0.0168 - accuracy: 0.8853\n",
      "10000/10000 [==============================] - 0s 39us/sample - loss: 0.0167 - accuracy: 0.8862\n",
      "10000/10000 [==============================] - 0s 38us/sample - loss: 0.0168 - accuracy: 0.8874\n",
      "10000/10000 [==============================] - 0s 42us/sample - loss: 0.0169 - accuracy: 0.8880\n",
      "10000/10000 [==============================] - 0s 40us/sample - loss: 0.0170 - accuracy: 0.8873\n",
      "10000/10000 [==============================] - 0s 41us/sample - loss: 0.0173 - accuracy: 0.8870\n",
      "10000/10000 [==============================] - 0s 40us/sample - loss: 0.0176 - accuracy: 0.8863\n",
      "10000/10000 [==============================] - 0s 38us/sample - loss: 0.0181 - accuracy: 0.8847\n",
      "10000/10000 [==============================] - 0s 39us/sample - loss: 0.0186 - accuracy: 0.8839\n",
      "10000/10000 [==============================] - 0s 39us/sample - loss: 0.0193 - accuracy: 0.8826\n",
      "10000/10000 [==============================] - 0s 38us/sample - loss: 0.0201 - accuracy: 0.8800\n",
      "10000/10000 [==============================] - 0s 40us/sample - loss: 0.0210 - accuracy: 0.8782\n",
      "10000/10000 [==============================] - 0s 40us/sample - loss: 0.0221 - accuracy: 0.8744\n",
      "10000/10000 [==============================] - 0s 39us/sample - loss: 0.0233 - accuracy: 0.8692\n",
      "10000/10000 [==============================] - 0s 38us/sample - loss: 0.0247 - accuracy: 0.8635\n",
      "10000/10000 [==============================] - 0s 37us/sample - loss: 0.0262 - accuracy: 0.8576\n",
      "10000/10000 [==============================] - 0s 37us/sample - loss: 0.0278 - accuracy: 0.8499\n",
      "10000/10000 [==============================] - 0s 36us/sample - loss: 0.0296 - accuracy: 0.8394\n",
      "10000/10000 [==============================] - 0s 36us/sample - loss: 0.0315 - accuracy: 0.8290\n",
      "10000/10000 [==============================] - 0s 36us/sample - loss: 0.0334 - accuracy: 0.8156\n",
      "10000/10000 [==============================] - 0s 36us/sample - loss: 0.0355 - accuracy: 0.8016\n",
      "186th iteration\n",
      "10000/10000 [==============================] - 0s 36us/sample - loss: 0.0151 - accuracy: 0.8978\n",
      "10000/10000 [==============================] - 0s 36us/sample - loss: 0.0151 - accuracy: 0.8989\n",
      "10000/10000 [==============================] - 0s 36us/sample - loss: 0.0151 - accuracy: 0.9005\n",
      "10000/10000 [==============================] - 0s 36us/sample - loss: 0.0151 - accuracy: 0.9010\n",
      "10000/10000 [==============================] - 0s 36us/sample - loss: 0.0152 - accuracy: 0.9004\n",
      "10000/10000 [==============================] - 0s 36us/sample - loss: 0.0154 - accuracy: 0.9010\n",
      "10000/10000 [==============================] - 0s 36us/sample - loss: 0.0156 - accuracy: 0.9013\n",
      "10000/10000 [==============================] - 0s 36us/sample - loss: 0.0158 - accuracy: 0.9010\n",
      "10000/10000 [==============================] - 0s 36us/sample - loss: 0.0161 - accuracy: 0.9007\n",
      "10000/10000 [==============================] - 0s 36us/sample - loss: 0.0165 - accuracy: 0.8990\n",
      "10000/10000 [==============================] - 0s 36us/sample - loss: 0.0170 - accuracy: 0.8980\n",
      "10000/10000 [==============================] - 0s 36us/sample - loss: 0.0175 - accuracy: 0.8967\n",
      "10000/10000 [==============================] - 0s 36us/sample - loss: 0.0182 - accuracy: 0.8948\n",
      "10000/10000 [==============================] - 0s 36us/sample - loss: 0.0189 - accuracy: 0.8935\n",
      "10000/10000 [==============================] - 0s 36us/sample - loss: 0.0197 - accuracy: 0.8904\n",
      "10000/10000 [==============================] - 0s 36us/sample - loss: 0.0205 - accuracy: 0.8877\n",
      "10000/10000 [==============================] - 0s 36us/sample - loss: 0.0215 - accuracy: 0.8843\n",
      "10000/10000 [==============================] - 0s 35us/sample - loss: 0.0226 - accuracy: 0.8782\n",
      "10000/10000 [==============================] - 0s 37us/sample - loss: 0.0237 - accuracy: 0.8730\n",
      "10000/10000 [==============================] - 0s 39us/sample - loss: 0.0250 - accuracy: 0.8672\n",
      "10000/10000 [==============================] - 0s 38us/sample - loss: 0.0263 - accuracy: 0.8606\n",
      "187th iteration\n",
      "10000/10000 [==============================] - 0s 40us/sample - loss: 0.0180 - accuracy: 0.8781\n",
      "10000/10000 [==============================] - 0s 37us/sample - loss: 0.0178 - accuracy: 0.8790\n",
      "10000/10000 [==============================] - 0s 36us/sample - loss: 0.0177 - accuracy: 0.8813\n",
      "10000/10000 [==============================] - 0s 36us/sample - loss: 0.0177 - accuracy: 0.8819\n",
      "10000/10000 [==============================] - 0s 37us/sample - loss: 0.0177 - accuracy: 0.8822\n",
      "10000/10000 [==============================] - 0s 37us/sample - loss: 0.0178 - accuracy: 0.8826\n",
      "10000/10000 [==============================] - 0s 36us/sample - loss: 0.0180 - accuracy: 0.8824\n",
      "10000/10000 [==============================] - 0s 36us/sample - loss: 0.0184 - accuracy: 0.8817\n",
      "10000/10000 [==============================] - 0s 36us/sample - loss: 0.0188 - accuracy: 0.8808\n",
      "10000/10000 [==============================] - 0s 36us/sample - loss: 0.0194 - accuracy: 0.8788\n",
      "10000/10000 [==============================] - 0s 36us/sample - loss: 0.0201 - accuracy: 0.8778\n",
      "10000/10000 [==============================] - 0s 35us/sample - loss: 0.0210 - accuracy: 0.8770\n",
      "10000/10000 [==============================] - 0s 36us/sample - loss: 0.0221 - accuracy: 0.8740\n",
      "10000/10000 [==============================] - 0s 39us/sample - loss: 0.0233 - accuracy: 0.8698\n",
      "10000/10000 [==============================] - 0s 36us/sample - loss: 0.0246 - accuracy: 0.8643\n",
      "10000/10000 [==============================] - 0s 36us/sample - loss: 0.0261 - accuracy: 0.8580\n",
      "10000/10000 [==============================] - 0s 36us/sample - loss: 0.0278 - accuracy: 0.8518\n",
      "10000/10000 [==============================] - 0s 36us/sample - loss: 0.0296 - accuracy: 0.8414\n",
      "10000/10000 [==============================] - 0s 36us/sample - loss: 0.0314 - accuracy: 0.8293\n",
      "10000/10000 [==============================] - 0s 37us/sample - loss: 0.0334 - accuracy: 0.8166\n",
      "10000/10000 [==============================] - 0s 37us/sample - loss: 0.0355 - accuracy: 0.8016\n",
      "188th iteration\n",
      "10000/10000 [==============================] - 0s 37us/sample - loss: 0.0155 - accuracy: 0.8978\n",
      "10000/10000 [==============================] - 0s 36us/sample - loss: 0.0155 - accuracy: 0.8996\n",
      "10000/10000 [==============================] - 0s 36us/sample - loss: 0.0156 - accuracy: 0.9002\n",
      "10000/10000 [==============================] - 0s 36us/sample - loss: 0.0158 - accuracy: 0.9000\n",
      "10000/10000 [==============================] - 0s 36us/sample - loss: 0.0160 - accuracy: 0.8988\n",
      "10000/10000 [==============================] - 0s 43us/sample - loss: 0.0163 - accuracy: 0.8994\n",
      "10000/10000 [==============================] - 0s 38us/sample - loss: 0.0167 - accuracy: 0.8988\n",
      "10000/10000 [==============================] - 0s 37us/sample - loss: 0.0173 - accuracy: 0.8974\n",
      "10000/10000 [==============================] - 0s 37us/sample - loss: 0.0179 - accuracy: 0.8955\n",
      "10000/10000 [==============================] - 0s 37us/sample - loss: 0.0187 - accuracy: 0.8942\n",
      "10000/10000 [==============================] - 0s 37us/sample - loss: 0.0196 - accuracy: 0.8911\n",
      "10000/10000 [==============================] - 0s 37us/sample - loss: 0.0206 - accuracy: 0.8869\n",
      "10000/10000 [==============================] - 0s 37us/sample - loss: 0.0218 - accuracy: 0.8835\n",
      "10000/10000 [==============================] - 0s 38us/sample - loss: 0.0231 - accuracy: 0.8766\n",
      "10000/10000 [==============================] - 0s 37us/sample - loss: 0.0246 - accuracy: 0.8706\n",
      "10000/10000 [==============================] - 0s 38us/sample - loss: 0.0261 - accuracy: 0.8625\n",
      "10000/10000 [==============================] - 0s 37us/sample - loss: 0.0278 - accuracy: 0.8534\n",
      "10000/10000 [==============================] - 0s 37us/sample - loss: 0.0296 - accuracy: 0.8415\n",
      "10000/10000 [==============================] - 0s 37us/sample - loss: 0.0315 - accuracy: 0.8297\n",
      "10000/10000 [==============================] - 0s 39us/sample - loss: 0.0335 - accuracy: 0.8159\n",
      "10000/10000 [==============================] - 0s 37us/sample - loss: 0.0355 - accuracy: 0.8016\n",
      "189th iteration\n",
      "10000/10000 [==============================] - 0s 36us/sample - loss: 0.0151 - accuracy: 0.8978\n",
      "10000/10000 [==============================] - 0s 36us/sample - loss: 0.0151 - accuracy: 0.8983\n",
      "10000/10000 [==============================] - 0s 36us/sample - loss: 0.0152 - accuracy: 0.8994\n",
      "10000/10000 [==============================] - 0s 36us/sample - loss: 0.0154 - accuracy: 0.8997\n",
      "10000/10000 [==============================] - 0s 36us/sample - loss: 0.0156 - accuracy: 0.9000\n",
      "10000/10000 [==============================] - 0s 36us/sample - loss: 0.0160 - accuracy: 0.8984\n",
      "10000/10000 [==============================] - 0s 36us/sample - loss: 0.0164 - accuracy: 0.8974\n",
      "10000/10000 [==============================] - 0s 35us/sample - loss: 0.0169 - accuracy: 0.8954\n",
      "10000/10000 [==============================] - 0s 36us/sample - loss: 0.0176 - accuracy: 0.8940\n",
      "10000/10000 [==============================] - 0s 35us/sample - loss: 0.0184 - accuracy: 0.8907\n",
      "10000/10000 [==============================] - 0s 35us/sample - loss: 0.0193 - accuracy: 0.8876\n",
      "10000/10000 [==============================] - 1s 57us/sample - loss: 0.0203 - accuracy: 0.8843\n",
      "10000/10000 [==============================] - 0s 35us/sample - loss: 0.0215 - accuracy: 0.8794\n",
      "10000/10000 [==============================] - 0s 36us/sample - loss: 0.0228 - accuracy: 0.8727\n",
      "10000/10000 [==============================] - 0s 36us/sample - loss: 0.0243 - accuracy: 0.8660\n",
      "10000/10000 [==============================] - 0s 36us/sample - loss: 0.0258 - accuracy: 0.8585\n",
      "10000/10000 [==============================] - 0s 36us/sample - loss: 0.0276 - accuracy: 0.8498\n",
      "10000/10000 [==============================] - 0s 35us/sample - loss: 0.0294 - accuracy: 0.8394\n",
      "10000/10000 [==============================] - 0s 35us/sample - loss: 0.0313 - accuracy: 0.8298\n",
      "10000/10000 [==============================] - 0s 36us/sample - loss: 0.0334 - accuracy: 0.8161\n",
      "10000/10000 [==============================] - 0s 35us/sample - loss: 0.0355 - accuracy: 0.8016\n",
      "190th iteration\n"
     ]
    }
   ],
   "source": [
    "i = 0\n",
    "for agg_weights_list in agg_weights_list_per_pi_sorted:\n",
    "    j = 0\n",
    "    for agg_weights in agg_weights_list:\n",
    "        aggr_model = keras.models.clone_model(model1)\n",
    "        aggr_model.set_weights(agg_weights)\n",
    "        compile_model(aggr_model)\n",
    "        score = aggr_model.evaluate(x_test, y_test)\n",
    "        Z[i][j] = score[0]\n",
    "        j += 1\n",
    "        K.clear_session() #prevent memory leak https://github.com/keras-team/keras/issues/13118\n",
    "    i += 1\n",
    "    print(\"{}th iteration\".format(i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import cm\n",
    "from matplotlib.ticker import LinearLocator, FormatStrFormatter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig = plt.figure()\n",
    "ax = fig.gca(projection='3d')\n",
    "surf = ax.plot_surface(X, Y, Z, cmap=cm.Pastel1,\n",
    "                       linewidth=0, antialiased=False)\n",
    "ax.set_xlabel('$theta$')\n",
    "ax.set_ylabel('$distance$')\n",
    "ax.set_zlabel('$loss$')\n",
    "# Customize the z axis.\n",
    "# ax.set_xlim(0.051, 0.054)\n",
    "# ax.zaxis.set_major_locator(LinearLocator(10))\n",
    "# ax.zaxis.set_major_formatter(FormatStrFormatter('%.02f'))\n",
    "# ax.view_init(0, 180)\n",
    "# ax.view_init(0, 0)\n",
    "# Add a color bar which maps values to colors.\n",
    "fig.colorbar(surf, shrink=0.5, aspect=5)\n",
    "plt.savefig('fig1.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### When \\theta = 0.5 how was the aggregation benefit got affected?\n",
    "*aggregation benefit*: (the accuracy of the aggregated model) - (accuracy of the better local model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(190, 21)"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(190, 21)"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Z.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(190,)"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Z[:,theta_index].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "theta_index = 10\n",
    "B = np.zeros(Z.shape[0])\n",
    "B = Z[:,theta_index] - np.max(Z[:,0] - Z[:,1])\n",
    "B_y = np.array(sorted(dist_list))\n",
    "plt.plot(B_y, B)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "name": "personalized.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
